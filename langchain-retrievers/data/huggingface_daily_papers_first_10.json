[
  {
    "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and\n  Long-Form Answers",
    "summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
    "authors": [
      "Saad Obaid ul Islam",
      "Anne Lauscher",
      "Goran Glava\u0161"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Large language models",
      "factual question-answering",
      "controlled evaluation framework",
      "short-long form alignment",
      "position-dependent accuracy",
      "momentum effects",
      "mechanistic analysis",
      "factual consistency",
      "query complexity",
      "trustworthiness"
    ]
  },
  {
    "title": "Through the Perspective of LiDAR: A Feature-Enriched and\n  Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud\n  Segmentation",
    "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.   Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.",
    "authors": [
      "Fei Zhang",
      "Rob Chancia",
      "Josie Clapp",
      "Amirhossein Hassanzadeh",
      "Dimah Dera",
      "Richard MacKenzie",
      "Jan van Aardt"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "spherical projection",
      "feature enrichment",
      "ensemble learning",
      "pseudo-labels",
      "uncertainty maps",
      "back-projection",
      "three-tier visualization suite",
      "Mangrove3D",
      "ForestSemantic",
      "Semantic3D",
      "mean Intersection over Union (mIoU)"
    ]
  },
  {
    "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed\n  Societal Biases in LLMs",
    "summary": "Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia.",
    "authors": [
      "Nafiseh Nikeghbal",
      "Amir Hossein Kargaran",
      "Jana Diesner"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Large language models",
      "LLMs",
      "adversarial attacks",
      "normative behavior",
      "ethical behavior",
      "socio-demographic categories",
      "gender",
      "race",
      "religion",
      "nationality",
      "sex orientation",
      "bias metrics",
      "human judgments",
      "bias amplification"
    ]
  },
  {
    "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
    "summary": "Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.",
    "authors": [
      "Qinglin Zhu",
      "Yizhen Yao",
      "Runcong Zhao",
      "Yanzheng Xiang",
      "Amrutha Saseendran",
      "Chen Jin",
      "Philip Alexander Teare",
      "Bin Liang",
      "Yulan He",
      "Lin Gui"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "autoregressive models",
      "diffusion-inspired approaches",
      "LlaDA",
      "Dream",
      "Latent Refinement Decoding",
      "Latent Refinement",
      "Predictive Feedback Loop",
      "KL-divergence dynamics",
      "HumanEval",
      "MBPP",
      "GSM8K",
      "MATH500"
    ]
  },
  {
    "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum\n  Likelihood Estimation",
    "summary": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po .",
    "authors": [
      "Abdelhakim Benechehab",
      "Gabriel Singer",
      "Corentin L\u00e9ger",
      "Youssef Attia El Hili",
      "Giuseppe Paolo",
      "Albert Thomas",
      "Maurizio Filippone",
      "Bal\u00e1zs K\u00e9gl"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "generative models",
      "Maximum Likelihood Estimation",
      "Reinforcement Learning",
      "Policy Gradient methods",
      "Bilevel Optimization",
      "reward function",
      "policy gradient objective",
      "tabular classification",
      "model-based reinforcement learning"
    ]
  },
  {
    "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language\n  Models",
    "summary": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.",
    "authors": [
      "Haomin Wang",
      "Jinhui Yin",
      "Qi Wei",
      "Wenguang Zeng",
      "Lixin Gu",
      "Shenglong Ye",
      "Zhangwei Gao",
      "Yaohui Wang",
      "Yanting Zhang",
      "Yuanqi Li",
      "Yanwen Guo",
      "Wenhai Wang",
      "Kai Chen",
      "Yu Qiao",
      "Hongjie Zhang"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "multimodal large language models",
      "MLLMs",
      "SVG understanding",
      "SVG editing",
      "SVG generation",
      "SAgoge",
      "SArena",
      "SVG-specific special tokens",
      "subword-based embedding initialization",
      "two-stage training strategy"
    ]
  },
  {
    "title": "Stable Video Infinity: Infinite-Length Video Generation with Error\n  Recycling",
    "summary": "We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.",
    "authors": [
      "Wuyang Li",
      "Wentao Pan",
      "Po-Chien Luan",
      "Yang Gao",
      "Alexandre Alahi"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Error-Recycling Fine-Tuning",
      "Diffusion Transformer",
      "DiT",
      "flow matching",
      "one-step bidirectional integration",
      "replay memory",
      "temporal consistency",
      "scene transitions",
      "autoregressive learning",
      "error injection",
      "error accumulation"
    ]
  },
  {
    "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
    "summary": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.",
    "authors": [
      "Xin Gui",
      "King Zhu",
      "JinCheng Ren",
      "Qianben Chen",
      "Zekun Moore Wang",
      "Yizhi LI",
      "Xinpeng Liu",
      "Xiaowan Li",
      "Wenli Ren",
      "Linyu Miao",
      "Tianrui Qin",
      "Ziqi Shu",
      "He Zhu",
      "Xiangru Tang",
      "Dingfeng Shi",
      "Jiaheng Liu",
      "Yuchen Eleanor Jiang",
      "Minghao Liu",
      "Ge Zhang",
      "Wangchunshu Zhou"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "large language models",
      "agents",
      "Acadreason benchmark",
      "academic knowledge",
      "computer science",
      "economics",
      "law",
      "mathematics",
      "philosophy",
      "top-tier publications",
      "systematic evaluations",
      "GPT-5"
    ]
  },
  {
    "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
    "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs.",
    "authors": [
      "Siyuan Huang",
      "Xiaoye Qu",
      "Yafu Li",
      "Yun Luo",
      "Zefeng He",
      "Daizong Liu",
      "Yu Cheng"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Reinforcement Learning with Verifiable Rewards",
      "Large Vision-Language Models",
      "multimodal reasoning",
      "token perception",
      "Chain-of-Thought",
      "rollout trajectory",
      "visual dependency",
      "Visually-Perceptive Policy Optimization",
      "policy gradient algorithm",
      "perceptually pivotal tokens"
    ]
  },
  {
    "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
    "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
    "authors": [
      "Yixiao Wang",
      "Mingxiao Huo",
      "Zhixuan Liang",
      "Yushi Du",
      "Lingfeng Sun",
      "Haotian Lin",
      "Jinghuan Shang",
      "Chensheng Peng",
      "Mohit Bansal",
      "Mingyu Ding",
      "Masayoshi Tomizuka"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "pretrained vision foundation models",
      "VFMs",
      "unified representation",
      "policy",
      "routing network",
      "Patchwise Expert Routing",
      "Curriculum Top-K Annealing",
      "parameter-efficient fine-tuning",
      "expert utilization",
      "adaptive robot-domain knowledge integration"
    ]
  },
  {
    "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
    "summary": "Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
    "authors": [
      "Adnan El Assadi",
      "Isaac Chung",
      "Roman Solomatin",
      "Niklas Muennighoff",
      "Kenneth Enevoldsen"
    ],
    "date": "2025-10-11",
    "ai_keywords": [
      "embedding models",
      "HUME",
      "Human Evaluation Framework for Text Embeddings",
      "MTEB",
      "reranking",
      "classification",
      "clustering",
      "semantic textual similarity",
      "low-resource languages",
      "dataset issues",
      "model development",
      "benchmarks"
    ]
  },
  {
    "title": "Don't Just Fine-tune the Agent, Tune the Environment",
    "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce Environment Tuning, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. Environment Tuning orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.",
    "authors": [
      "Siyuan Lu",
      "Zechuan Wang",
      "Hongxuan Zhang",
      "Qintong Wu",
      "Leilei Gan",
      "Chenyi Zhuang",
      "Jinjie Gu",
      "Tao Lin"
    ],
    "date": "2025-10-11",
    "ai_keywords": [
      "Environment Tuning",
      "structured curriculum",
      "environment augmentation",
      "progress rewards",
      "reinforcement learning",
      "supervised fine-tuning",
      "LLM agents",
      "Berkeley Function-Calling Leaderboard",
      "in-distribution performance",
      "out-of-distribution generalization"
    ]
  },
  {
    "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
    "summary": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
    "authors": [
      "Haoyu Zhao",
      "Cheng Zeng",
      "Linghao Zhuang",
      "Yaxi Zhao",
      "Shengke Xue",
      "Hao Wang",
      "Xingyue Zhao",
      "Zhongyu Li",
      "Kehan Li",
      "Siteng Huang",
      "Mingxiu Chen",
      "Xin Li",
      "Deli Zhao",
      "Hua Zou"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "3D Gaussian Splatting",
      "mesh primitives",
      "Multi-modal Large Language Model",
      "physically plausible",
      "articulated assets",
      "zero-shot sim-to-real transfer",
      "robotic manipulation"
    ]
  },
  {
    "title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable\n  High-Accuracy Authorship Attribution",
    "summary": "In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing a critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as a single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000 Node.js back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and a modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As a result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: https://github.com/LLM-NodeJS-dataset.",
    "authors": [
      "Norbert Tihanyi",
      "Bilel Cherif",
      "Richard A. Dubniczky",
      "Mohamed Amine Ferrag",
      "Tam\u00e1s Bisztray"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "Large Language Models",
      "LLM-NodeJS",
      "Node.js",
      "JavaScript",
      "machine learning classifiers",
      "Transformer encoders",
      "CodeT5-JSA",
      "CodeT5",
      "BERT",
      "CodeBERT",
      "Longformer",
      "authorship attribution",
      "model fingerprinting",
      "dataflow",
      "structure",
      "code transformations"
    ]
  },
  {
    "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
    "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.",
    "authors": [
      "Zhengbo Zhang",
      "Zhiheng Lyu",
      "Junhao Gong",
      "Hongzhu Yi",
      "Xinming Wang",
      "Yuxuan Zhou",
      "Jiabing Yang",
      "Ping Nie",
      "Yan Huang",
      "Wenhu Chen"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "LLMs",
      "BrowserAgent",
      "Playwright",
      "Supervised Fine-Tuning",
      "Rejection Fine-Tuning",
      "explicit memory mechanism",
      "multi-hop QA",
      "HotpotQA",
      "2Wiki",
      "Bamboogle"
    ]
  },
  {
    "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
    "summary": "While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.",
    "authors": [
      "Yue Huang",
      "Hang Hua",
      "Yujun Zhou",
      "Pengcheng Jing",
      "Manish Nagireddy",
      "Inkit Padhi",
      "Greta Dolcetti",
      "Zhangchen Xu",
      "Subhajit Chaudhury",
      "Ambrish Rawat",
      "Liubov Nedoshivina",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Xiangliang Zhang"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "AuraGen",
      "Safiron",
      "cross-planner adapter",
      "compact guardian model",
      "automated reward model",
      "Pre-Exec Bench",
      "risk detection",
      "risk categorization",
      "explanation",
      "cross-planner generalization"
    ]
  },
  {
    "title": "Are Large Reasoning Models Interruptible?",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.",
    "authors": [
      "Tsung-Han Wu",
      "Mihran Miroyan",
      "David M. Chan",
      "Trevor Darrell",
      "Narges Norouzi",
      "Joseph E. Gonzalez"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Large Reasoning Models",
      "frozen world assumption",
      "interruptions",
      "dynamic context",
      "long-form reasoning",
      "reasoning leakage",
      "panic",
      "self-doubt"
    ]
  },
  {
    "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
    "summary": "High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at https://github.com/cxcscmu/RePro.",
    "authors": [
      "Zichun Yu",
      "Chenyan Xiong"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "reinforcement learning",
      "rephraser",
      "quality reward",
      "faithfulness rewards",
      "pretraining data",
      "DCLM-RefinedWeb",
      "downstream tasks",
      "ReWire",
      "data efficiency",
      "critical information preservation"
    ]
  },
  {
    "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.",
    "authors": [
      "Jinghao Zhang",
      "Naishan Zheng",
      "Ruilin Li",
      "Dongzhou Cheng",
      "Zheming Liang",
      "Feng Zhao",
      "Jiaqi Wang"
    ],
    "date": "2025-10-11",
    "ai_keywords": [
      "Reinforcement Learning with Verifiable Rewards",
      "RLVR",
      "Large Language Models",
      "LLMs",
      "Process Reward Models",
      "PRMs",
      "reward shaping",
      "process tokens",
      "logit space",
      "flow rewards",
      "latent space",
      "flow fields",
      "off-policy data",
      "on-policy rejection sampling",
      "velocity deviations",
      "hidden states",
      "context comprehension",
      "language benchmarks",
      "multimodal reasoning benchmarks"
    ]
  },
  {
    "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
    "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. Based on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.",
    "authors": [
      "Hoigi Seo",
      "Dong Un Kang",
      "Hyunjin Cho",
      "Joohoon Lee",
      "Se Young Chun"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "vision-language models",
      "vision encoder",
      "object hallucination",
      "visual tokens",
      "epistemic uncertainty",
      "adversarial perturbations",
      "self-attention",
      "visual encoding"
    ]
  },
  {
    "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
    "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.",
    "authors": [
      "Yuhang Li",
      "Chenchen Zhang",
      "Ruilin Lv",
      "Ao Liu",
      "Ken Deng",
      "Yuanxing Zhang",
      "Jiaheng Liu",
      "Wiggin Zhou",
      "Bo Zhou"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "front-end development",
      "reinforcement learning",
      "agentic",
      "vision-grounded",
      "multimodal LLM",
      "MLLM",
      "visual critic",
      "vision-grounded feedback",
      "Forced Optimization",
      "behavioral collapse",
      "self-edit cycle",
      "vision-grounded front-end code generation"
    ]
  },
  {
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "summary": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce GIR-Bench, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at https://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
    "authors": [
      "Hongxiang Li",
      "Yaowei Li",
      "Bin Lin",
      "Yuwei Niu",
      "Yuhang Yang",
      "Xiaoshuang Huang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Long Chen"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "unified multimodal models",
      "large language models",
      "image understanding",
      "image generation",
      "multimodal intelligence",
      "reasoning-centric benchmark",
      "understanding-generation consistency",
      "text-to-image generation",
      "multi-step reasoning",
      "evaluation pipelines",
      "MLLM-as-a-Judge paradigm"
    ]
  },
  {
    "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
    "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.",
    "authors": [
      "Caorui Li",
      "Yu Chen",
      "Yiyan Ji",
      "Jin Xu",
      "Zhenyu Cui",
      "Shihao Li",
      "Yuanxing Zhang",
      "Jiafu Tang",
      "Zhenghao Song",
      "Dingling Zhang",
      "Ying He",
      "Haoxiang Liu",
      "Yuxuan Wang",
      "Qiufeng Wang",
      "Zhenhe Wu",
      "Jiehui Luo",
      "Zhiyu Pan",
      "Weihao Xie",
      "Chenchen Zhang",
      "Zhaohui Wang",
      "Jiayi Tian",
      "Yanghai Wang",
      "Zhe Cao",
      "Minxin Dai",
      "Ke Wang",
      "Runzhe Wen",
      "Yinghao Ma",
      "Yaning Pan",
      "Sungkyun Chang",
      "Termeh Taheri",
      "Haiwen Xia",
      "Christos Plachouras",
      "Emmanouil Benetos",
      "Yizhi Li",
      "Ge Zhang",
      "Jian Yang",
      "Tianhao Peng",
      "Zili Wang",
      "Minghao Liu",
      "Junran Peng",
      "Zhaoxiang Zhang",
      "Jiaheng Liu"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "multimodal large language models",
      "video understanding",
      "audio-visual understanding",
      "modality complementarity",
      "logical consistency",
      "question-answer pairs",
      "reasoning traces",
      "temporal reasoning",
      "spatial localization",
      "counting",
      "causal inference",
      "summarization"
    ]
  },
  {
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "summary": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
    "authors": [
      "Ganlin Yang",
      "Tianyi Zhang",
      "Haoran Hao",
      "Weiyun Wang",
      "Yibin Liu",
      "Dehui Wang",
      "Guanzhou Chen",
      "Zijian Cai",
      "Junting Chen",
      "Weijie Su",
      "Wengang Zhou",
      "Yu Qiao",
      "Jifeng Dai",
      "Jiangmiao Pang",
      "Gen Luo",
      "Wenhai Wang",
      "Yao Mu",
      "Zhi Hou"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Vision-Language Models",
      "Vision-Language-Action models",
      "embodied reasoning",
      "Vlaser",
      "Vlaser-6M dataset",
      "spatial reasoning",
      "embodied grounding",
      "embodied QA",
      "task planning",
      "supervised VLA fine-tuning",
      "domain shift",
      "WidowX benchmark",
      "Google Robot benchmark"
    ]
  },
  {
    "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
    "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.",
    "authors": [
      "Haoran Feng",
      "Dizhe Zhang",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "DiT",
      "hybrid training",
      "perspective data",
      "panoramic data",
      "geometric fidelity",
      "photorealism",
      "pre-VAE",
      "post-VAE",
      "cross-domain knowledge",
      "perspective image guidance",
      "panoramic refinement",
      "perceptual quality",
      "diversity",
      "circular padding",
      "yaw loss",
      "cube loss",
      "text-to-panorama",
      "inpainting",
      "outpainting"
    ]
  },
  {
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
    "summary": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
    "authors": [
      "Jianhao Yuan",
      "Fabio Pizzati",
      "Francesco Pinto",
      "Lars Kunze",
      "Ivan Laptev",
      "Paul Newman",
      "Philip Torr",
      "Daniele De Martini"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "video diffusion models",
      "intuitive physics",
      "LikePhys",
      "denoising objective",
      "ELBO-based likelihood",
      "Plausibility Preference Error",
      "benchmark",
      "human preference",
      "model design",
      "inference settings",
      "domain-specific capacity variations"
    ]
  },
  {
    "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
    "summary": "Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.",
    "authors": [
      "Xinlong Chen",
      "Yue Ding",
      "Weihong Lin",
      "Jingyun Hua",
      "Linli Yao",
      "Yang Shi",
      "Bozhou Li",
      "Yuanxing Zhang",
      "Qiang Liu",
      "Pengfei Wan",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "audiovisual video captioning",
      "temporal orchestration",
      "AVoCaDO",
      "AVoCaDO SFT",
      "AVoCaDO GRPO",
      "reward functions",
      "temporal coherence",
      "dialogue accuracy",
      "caption length",
      "collapse",
      "VDC",
      "DREAM-1K"
    ]
  },
  {
    "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
    "summary": "Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.",
    "authors": [
      "Yinan Chen",
      "Jiangning Zhang",
      "Teng Hu",
      "Yuxiang Zeng",
      "Zhucun Xue",
      "Qingdong He",
      "Chengjie Wang",
      "Yong Liu",
      "Xiaobin Hu",
      "Shuicheng Yan"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "instruction-guided video editing",
      "benchmark suite",
      "source diversity",
      "task coverage",
      "evaluation metrics",
      "large language models",
      "video quality",
      "instruction compliance",
      "video fidelity",
      "multimodal assessments"
    ]
  },
  {
    "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "summary": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
    "authors": [
      "Yuxuan Xue",
      "Xianghui Xie",
      "Margaret Kostyrko",
      "Gerard Pons-Moll"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "vision-language models",
      "image generation models",
      "diffusion-based generative pipeline",
      "SMPL body-shape parameters"
    ]
  },
  {
    "title": "Diffusion Transformers with Representation Autoencoders",
    "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
    "authors": [
      "Boyang Zheng",
      "Nanye Ma",
      "Shengbang Tong",
      "Saining Xie"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Latent generative modeling",
      "Diffusion Transformers (DiT)",
      "VAE encoder",
      "Representation Autoencoders (RAEs)",
      "pretrained representation encoders",
      "DINO",
      "SigLIP",
      "MAE",
      "high-dimensional latent spaces",
      "transformer-based architecture",
      "diffusion transformers",
      "image generation",
      "ImageNet",
      "FID"
    ]
  },
  {
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
    "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "authors": [
      "Chengqi Duan",
      "Kaiyue Sun",
      "Rongyao Fang",
      "Manyuan Zhang",
      "Yan Feng",
      "Ying Luo",
      "Yufang Liu",
      "Ke Wang",
      "Peng Pei",
      "Xunliang Cai",
      "Hongsheng Li",
      "Yi Ma",
      "Xihui Liu"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "Large Language Models",
      "Vision Language Models",
      "mathematical reasoning",
      "visual assistance",
      "multimodal unified models",
      "Chain-of-Thought",
      "Math-VR",
      "image-to-code converter",
      "visual reasoning",
      "mathematical problems"
    ]
  },
  {
    "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
    "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline.",
    "authors": [
      "Soroush Mehraban",
      "Andrea Iaboni",
      "Babak Taati"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "transformer-based models",
      "3D Human Mesh Recovery",
      "Error-Constrained Layer Merging",
      "Mask-guided Token Merging",
      "Mean Per Joint Position Error",
      "diffusion-based decoder",
      "temporal context",
      "pose priors",
      "motion capture datasets"
    ]
  },
  {
    "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
    "summary": "When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities.",
    "authors": [
      "Xi Fang",
      "Weijie Xu",
      "Yuchong Zhang",
      "Stephanie Eckman",
      "Scott Nickleach",
      "Chandan K. Reddy"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "large language models",
      "emotional intelligence",
      "user memory",
      "emotional scenarios",
      "user profiles",
      "systematic biases",
      "demographic factors",
      "emotion understanding",
      "supportive recommendations",
      "social hierarchies"
    ]
  },
  {
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.",
    "authors": [
      "Wei Huang",
      "Yi Ge",
      "Shuai Yang",
      "Yicheng Xiao",
      "Huizi Mao",
      "Yujun Lin",
      "Hanrong Ye",
      "Sifei Liu",
      "Ka Chun Cheung",
      "Hongxu Yin",
      "Yao Lu",
      "Xiaojuan Qi",
      "Song Han",
      "Yukang Chen"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "NVFP4 quantization",
      "Low-Rank Adaptation (LoRA)",
      "Adaptive Quantization Noise (AQN)",
      "reinforcement learning",
      "large language models (LLMs)",
      "rollout phase",
      "policy entropy",
      "exploration",
      "reward growth",
      "GSM8K",
      "MATH 500"
    ]
  },
  {
    "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
    "summary": "General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available.",
    "authors": [
      "Changjiang Gao",
      "Zixian Huang",
      "Jingyang Gong",
      "Shujian Huang",
      "Lei Li",
      "Fei Yuan"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "instruct models",
      "layer-selective tuning",
      "parallel data",
      "Qwen3-XPlus models",
      "spBLEU",
      "xComet",
      "multilingual tasks",
      "reasoning datasets"
    ]
  },
  {
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "summary": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL",
    "authors": [
      "Zhaochen Yu",
      "Ling Yang",
      "Jiaru Zou",
      "Shuicheng Yan",
      "Mengdi Wang"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "agentic RL",
      "SFT initialization",
      "real end-to-end tool-use trajectories",
      "high-diversity datasets",
      "model-aware datasets",
      "exploration-friendly techniques",
      "clip higher",
      "overlong reward shaping",
      "policy entropy",
      "deliberative strategy",
      "agentic SFT dataset",
      "RL dataset",
      "AIME2024/AIME2025",
      "GPQA-Diamond",
      "LiveCodeBench-v6"
    ]
  },
  {
    "title": "Skill-Targeted Adaptive Training",
    "summary": "Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.",
    "authors": [
      "Yinghui He",
      "Abhishek Panigrahi",
      "Yong Lin",
      "Sanjeev Arora"
    ],
    "date": "2025-10-11",
    "ai_keywords": [
      "vanilla supervised fine-tuning",
      "SFT",
      "STAT",
      "metacognition",
      "large language model",
      "LLM",
      "task dataset",
      "skills",
      "Missing-Skill-Profile",
      "STAT-Sel",
      "STAT-Syn",
      "Llama",
      "Qwen",
      "MATH",
      "AIME24/25",
      "AMC23",
      "RL",
      "GRPO"
    ]
  },
  {
    "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
    "summary": "In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5times fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000times larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.",
    "authors": [
      "Gang Liu",
      "Jie Chen",
      "Yihan Zhu",
      "Michael Sun",
      "Tengfei Luo",
      "Nitesh V Chawla",
      "Meng Jiang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "in-context learning",
      "diffusion models",
      "denoising Transformer",
      "molecular tokenizer",
      "Node Pair Encoding",
      "molecular design",
      "pretraining",
      "foundation model"
    ]
  },
  {
    "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
    "summary": "Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.",
    "authors": [
      "Chenyu Wang",
      "Paria Rashidinejad",
      "DiJia Su",
      "Song Jiang",
      "Sid Wang",
      "Siyan Zhao",
      "Cai Zhou",
      "Shannon Zejiang Shen",
      "Feiyu Chen",
      "Tommi Jaakkola",
      "Yuandong Tian",
      "Bo Liu"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "diffusion large language models",
      "dLLMs",
      "autoregressive models",
      "reinforcement learning",
      "RL",
      "log-likelihood",
      "policy gradient methods",
      "evidence lower bound",
      "ELBO",
      "Sandwiched Policy Gradient",
      "SPG",
      "GSM8K",
      "MATH500",
      "Countdown",
      "Sudoku"
    ]
  },
  {
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "summary": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.",
    "authors": [
      "Junpeng Liu",
      "Yuzhong Zhao",
      "Bowen Cao",
      "Jiayu Ding",
      "Yilin Jia",
      "Tengchao Lv",
      "Yupan Huang",
      "Shaohan Huang",
      "Nan Yang",
      "Li Dong",
      "Lei Cui",
      "Tao Ge",
      "Xun Wang",
      "Huitian Jiao",
      "Sun Mao",
      "FNU Kartik",
      "Si-Qing Chen",
      "Wai Lam",
      "Furu Wei"
    ],
    "date": "2025-10-13",
    "ai_keywords": [
      "DocReward",
      "Bradley-Terry loss",
      "document reward model",
      "document generation",
      "extrinsic evaluation"
    ]
  },
  {
    "title": "Making Mathematical Reasoning Adaptive",
    "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
    "authors": [
      "Zhejian Lai",
      "Xiang Geng",
      "Zhijun Wang",
      "Yang Bai",
      "Jiahuan Li",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Shujian Huang"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "AdaR",
      "adaptive reasoning",
      "spurious reasoning",
      "logically equivalent queries",
      "RLVR",
      "data synthesis",
      "mathematical reasoning",
      "data efficiency"
    ]
  },
  {
    "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
    "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired \"where to look\" and \"why it matters\" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.",
    "authors": [
      "Sheng Wang",
      "Ruiming Wu",
      "Charles Herndon",
      "Yihang Liu",
      "Shunsuke Koga",
      "Jeanne Shen",
      "Zhi Huang"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "whole-slide image",
      "AI Session Recorder",
      "viewer logs",
      "behavioral commands",
      "bounding boxes",
      "Pathology-CoT dataset",
      "Pathologist-o3",
      "regions of interest",
      "behavior-guided reasoning",
      "gastrointestinal lymph-node metastasis detection",
      "OpenAI o3 model"
    ]
  },
  {
    "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
    "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.",
    "authors": [
      "Yan Wang",
      "Keyi Wang",
      "Shanshan Yang",
      "Jaisal Patel",
      "Jeff Zhao",
      "Fengran Mo",
      "Xueqing Peng",
      "Lingfei Qian",
      "Jimin Huang",
      "Guojun Xiong",
      "Xiao-Yang Liu",
      "Jian-Yun Nie"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "LLMs",
      "FinAuditing",
      "taxonomy-aligned",
      "structure-aware",
      "multi-document benchmark",
      "FinSM",
      "FinRE",
      "FinMR",
      "semantic consistency",
      "relational consistency",
      "numerical consistency",
      "retrieval",
      "classification",
      "reasoning metrics",
      "zero-shot experiments",
      "US-GAAP",
      "XBRL filings"
    ]
  },
  {
    "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
    "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.",
    "authors": [
      "Yu Li",
      "Menghan Xia",
      "Gongye Liu",
      "Jianhong Bai",
      "Xintao Wang",
      "Conglang Zhang",
      "Yuxuan Lin",
      "Ruihang Chu",
      "Pengfei Wan",
      "Yujiu Yang"
    ],
    "date": "2025-10-12",
    "ai_keywords": [
      "Text-to-Video models",
      "viewpoint planning",
      "4D scenes",
      "adaptive learning branch",
      "viewpoint extraction",
      "hybrid-condition guided camera extrinsic denoising",
      "camera extrinsic diffusion branch"
    ]
  },
  {
    "title": "Self-Improving LLM Agents at Test-Time",
    "summary": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
    "authors": [
      "Emre Can Acikgoz",
      "Cheng Qian",
      "Heng Ji",
      "Dilek Hakkani-T\u00fcr",
      "Gokhan Tur"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "language model fine-tuning",
      "self-awareness",
      "self-data augmentation",
      "test-time fine-tuning",
      "Test-Time Self-Improvement (TT-SI)",
      "Test-Time Distillation (TT-D)",
      "self-improvement algorithms",
      "self-evolution"
    ]
  },
  {
    "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
    "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
    "authors": [
      "Chen Huang",
      "Wei Lu",
      "Wenxuan Zhang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Reasoning Models",
      "chain-of-thought",
      "response length",
      "model entropy",
      "thinking phase",
      "final answer phase",
      "Phase Entropy Aware Reward",
      "PEAR",
      "out-of-distribution robustness"
    ]
  },
  {
    "title": "How to Teach Large Multimodal Models New Skills",
    "summary": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL",
    "authors": [
      "Zhen Zhu",
      "Yiming Gong",
      "Yao Xiao",
      "Yaoyao Liu",
      "Derek Hoiem"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "multimodal models",
      "sequential fine-tuning",
      "self-attention projection layers",
      "MLP Gate&Up",
      "Down projection"
    ]
  },
  {
    "title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix\n  Factorization",
    "summary": "Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy",
    "authors": [
      "Lawrence Liu",
      "Alexander Liu",
      "Mengdi Wang",
      "Tuo Zhao",
      "Lin F. Yang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "LLMs",
      "semi-structured pruning",
      "2:4 sparsity",
      "ARMOR",
      "matrix-factORization",
      "block diagonal matrices",
      "block coordinate descent",
      "proxy loss",
      "Llama",
      "Qwen",
      "model compression",
      "task accuracy"
    ]
  },
  {
    "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic\n  Embodied Capabilities",
    "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent to perceive, comprehend, and interact with the physical world. While multimodal large language models (MLLMs) show promise as embodied agents, a thorough and systematic evaluation of their embodied capabilities remains underexplored, as existing benchmarks primarily focus on specific domains such as planning or spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive and fine-grained benchmark that evaluates MLLMs on atomic embodied capabilities. BEAR comprises 4,469 interleaved image-video-text entries across 14 domains in 6 categories, including tasks from low-level pointing, trajectory understanding, spatial reasoning, to high-level planning. Extensive evaluation results of 20 representative MLLMs reveal their persistent limitations across all domains of embodied capabilities. To tackle the shortfall, we propose BEAR-Agent, a multimodal conversable agent that integrates pretrained vision models to strengthen MLLM perception, 3D understanding, and planning capabilities. It substantially enhances MLLM performance across diverse embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that improving MLLM embodied capabilities can benefit embodied tasks in simulated environments. Project website: https://bear-official66.github.io/",
    "authors": [
      "Yu Qi",
      "Haibo Zhao",
      "Ziyu Guo",
      "Siyuan Ma",
      "Ziyan Chen",
      "Yaokun Han",
      "Renrui Zhang",
      "Zitiantao Lin",
      "Shiji Xin",
      "Yijian Huang",
      "Kai Cheng",
      "Peiheng Wang",
      "Jiazheng Liu",
      "Jiayi Zhang",
      "Yizhe Zhu",
      "Wenqing Wang",
      "Yiran Qin",
      "Xupeng Zhu",
      "Haojie Huang",
      "Lawson L. S. Wong"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "multimodal large language models",
      "embodied capabilities",
      "BEAR",
      "image-video-text entries",
      "trajectory understanding",
      "spatial reasoning",
      "planning capabilities",
      "BEAR-Agent",
      "pretrained vision models",
      "3D understanding"
    ]
  },
  {
    "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization",
    "summary": "Personalizing diffusion models allows users to generate new images that incorporate a given subject, allowing more control than a text prompt. These models often suffer somewhat when they end up just recreating the subject image, and ignoring the text prompt. We observe that one popular method for personalization, the IP-Adapter automatically generates masks that we definitively segment the subject from the background during inference. We propose to use this automatically generated mask on a second pass to mask the image tokens, thus restricting them to the subject, not the background, allowing the text prompt to attend to the rest of the image. For text prompts describing locations and places, this produces images that accurately depict the subject while definitively matching the prompt. We compare our method to a few other test time personalization methods, and find our method displays high prompt and source image alignment.",
    "authors": [
      "James Baker"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "diffusion models",
      "IP-Adapter",
      "masks",
      "image tokens",
      "prompt and source image alignment"
    ]
  },
  {
    "title": "LightReasoner: Can Small Language Models Teach Large Language Models\n  Reasoning?",
    "summary": "Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner",
    "authors": [
      "Jingyuan Wang",
      "Yankai Chen",
      "Zhonghang Li",
      "Chao Huang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large language models",
      "small language models",
      "supervised fine-tuning",
      "behavioral divergence",
      "LightReasoner",
      "critical reasoning moments",
      "supervision examples",
      "expert-amateur contrast",
      "fine-tuning stage",
      "mathematical benchmarks"
    ]
  },
  {
    "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell\n  Biology",
    "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
    "authors": [
      "Sajib Acharjee Dip",
      "Adrika Zafor",
      "Bikash Kumar Paul",
      "Uddip Acharjee Shuvo",
      "Muhit Islam Emon",
      "Xuan Wang",
      "Liqing Zhang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "large language models",
      "LLM4Cell",
      "single-cell biology",
      "natural-language reasoning",
      "generative annotation",
      "multimodal data integration",
      "RNA",
      "ATAC",
      "multi-omic",
      "spatial modalities",
      "text-bridge",
      "trajectory modeling",
      "perturbation modeling",
      "drug-response prediction",
      "benchmark suitability",
      "data diversity",
      "ethical constraints",
      "scalability constraints",
      "biological grounding",
      "multi-omics alignment",
      "fairness",
      "privacy",
      "explainability",
      "interpretability",
      "standardization",
      "trustworthy model development"
    ]
  },
  {
    "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
    "summary": "Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.",
    "authors": [
      "Egor Cherepanov",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "recurrent models",
      "transformer models",
      "long-term dependencies",
      "context windows",
      "memory extensions",
      "ELMUR",
      "External Layer Memory",
      "bidirectional cross-attention",
      "Least Recently Used",
      "LRU memory module",
      "replacement",
      "convex blending",
      "T-Maze task",
      "POPGym",
      "MIKASA-Robo",
      "sparse-reward manipulation tasks",
      "visual observations"
    ]
  },
  {
    "title": "Formalizing Style in Personal Narratives",
    "summary": "Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states.",
    "authors": [
      "Gustave Cortal",
      "Alain Finkel"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "functional linguistics",
      "computer science",
      "psychological observations",
      "language models",
      "linguistic features",
      "processes",
      "participants",
      "circumstances",
      "verbal processes",
      "mental processes",
      "post-traumatic stress disorder"
    ]
  },
  {
    "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance\n  for Self-supervised Monocular Depth Estimation",
    "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, a novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach introduces a coarse-to-fine progressive learning framework: 1) Firstly, we aggregate multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance. A proxy task comparing close-distant image patches is designed to enforce depth-aware feature alignment using text prompts; 2) Next, building on the coarse features, we integrate camera pose information and pixel-wise language alignment to refine depth predictions. This module seamlessly integrates with existing self-supervised MDE pipelines (e.g., Monodepth2, ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth estimation. By aggregating CLIP's semantic context and DINO's spatial details through language guidance, our method effectively addresses feature granularity mismatches. Extensive experiments on the KITTI benchmark demonstrate that our method significantly outperforms SOTA methods across all metrics, which also indeed benefits downstream tasks like BEV perception. Code is available at https://github.com/Zhangwenyao1/Hybrid-depth.",
    "authors": [
      "Wenyao Zhang",
      "Hongsi Liu",
      "Bohan Li",
      "Jiawei He",
      "Zekun Qi",
      "Yunnan Wang",
      "Shengyang Zhao",
      "Xinqiang Yu",
      "Wenjun Zeng",
      "Xin Jin"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "self-supervised monocular depth estimation",
      "foundation models",
      "CLIP",
      "DINO",
      "coarse-to-fine progressive learning",
      "contrastive language guidance",
      "proxy task",
      "depth-aware feature alignment",
      "camera pose information",
      "pixel-wise language alignment",
      "KITTI benchmark",
      "BEV perception"
    ]
  },
  {
    "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
    "summary": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.",
    "authors": [
      "Wenjie Du",
      "Li Jiang",
      "Keda Tao",
      "Xue Liu",
      "Huan Wang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "chain-of-thought generation",
      "KV cache",
      "token-dropping",
      "head-reallocating",
      "attention heads",
      "functional heterogeneity",
      "RLKV",
      "reinforcement learning",
      "cache usage",
      "reasoning quality",
      "KV compression"
    ]
  },
  {
    "title": "KORMo: Korean Open Reasoning Model for Everyone",
    "summary": "This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research.",
    "authors": [
      "Minjun Kim",
      "Hyeonseok Lim",
      "Hangyeol Yoo",
      "Inho Won",
      "Seungwoo Song",
      "Minkyung Cho",
      "Junhun Yuk",
      "Changsu Choi",
      "Dongjae Shin",
      "Huige Lee",
      "Hoyun Song",
      "Alice Oh",
      "Kyungtae Lim"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "large language model",
      "LLM",
      "KORMo-10B",
      "synthetic data",
      "Korean-English corpus",
      "pretraining",
      "model collapse",
      "bilingual instruction tuning",
      "fully open models",
      "FOMs",
      "low-resource settings"
    ]
  },
  {
    "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out\n  of Distribution Generalization",
    "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.",
    "authors": [
      "Mohammad Mahdi Samiei Paqaleh",
      "Arash Marioriyad",
      "Arman Tahmasebi-Zadeh",
      "Mohamadreza Fereydooni",
      "Mahdi Ghaznavai",
      "Mahdieh Soleymani Baghshah"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Complexity OoD",
      "generalization",
      "out of distribution",
      "Kolmogorov complexity",
      "operational proxies",
      "System1",
      "System2",
      "solution traces",
      "inductive biases",
      "spurious shortcuts",
      "semantic robustness",
      "catastrophic forgetting",
      "step-wise calibration"
    ]
  },
  {
    "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
    "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-n attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms.",
    "authors": [
      "Mikhail Terekhov",
      "Alexander Panfilov",
      "Daniil Dzenhaliou",
      "Caglar Gulcehre",
      "Maksym Andriushchenko",
      "Ameya Prabhu",
      "Jonas Geiping"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "LLM",
      "AI control protocols",
      "security problem",
      "backdoor insertion",
      "LLM monitors",
      "adaptive attacks",
      "prompt injections",
      "Defer-to-Resample protocol",
      "best-of-n attack"
    ]
  },
  {
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
    "summary": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
    "authors": [
      "Suwhan Choi",
      "Jaeyoon Jung",
      "Haebin Seong",
      "Minchan Kim",
      "Minyeong Kim",
      "Yongjun Cho",
      "Yoonshik Kim",
      "Yubeen Park",
      "Youngjae Yu",
      "Yunsung Lee"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "embodied AI",
      "desktop environments",
      "sensorimotor interactions",
      "OWA Toolkit",
      "Generalist-IDM",
      "timestamp-based event prediction",
      "zero-shot generalization",
      "internet-scale pseudo-labeling",
      "VAPT",
      "LIBERO manipulation",
      "CANVAS navigation",
      "desktop pretraining"
    ]
  },
  {
    "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
    "summary": "Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present , a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
    "authors": [
      "Lorenzo Bianchi",
      "Giacomo Pacini",
      "Fabio Carrara",
      "Nicola Messina",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "zero-shot captioners",
      "common-space vision-language representations",
      "text-aligned image feature",
      "patch-centric paradigm",
      "region-level supervision",
      "atomic captioning units",
      "latent captioners",
      "dense visual features",
      "DINO",
      "region-based captioning tasks",
      "zero-shot dense captioning",
      "region-set captioning",
      "trace captioning task",
      "patch-wise semantic representations"
    ]
  },
  {
    "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger\n  Unimodal Models",
    "summary": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/",
    "authors": [
      "Sharut Gupta",
      "Shobhita Sundaram",
      "Chenyu Wang",
      "Stefanie Jegelka",
      "Phillip Isola"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "multimodal learners",
      "visual question answering",
      "paired datasets",
      "unpaired multimodal data",
      "modality-agnostic training",
      "cross-modal structure",
      "unimodal training",
      "downstream performance"
    ]
  },
  {
    "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
    "summary": "Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that maximise diversity in model responses. Our method, Diversifying Sample Condensation (DISCO), selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. DISCO shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.",
    "authors": [
      "Alexander Rubinstein",
      "Benjamin Raible",
      "Martin Gubri",
      "Seong Joon Oh"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Diversifying Sample Condensation (DISCO)",
      "model disagreements",
      "greedy selection",
      "information-theoretically optimal rule",
      "MMLU",
      "Hellaswag",
      "Winogrande",
      "ARC"
    ]
  },
  {
    "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall",
    "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.",
    "authors": [
      "Jiayu Yang",
      "Yuxuan Fan",
      "Songning Lai",
      "Shengen Wu",
      "Jiaqi Tang",
      "Chun Kang",
      "Zhijiang Guo",
      "Yutao Yue"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Language Models",
      "knowledge editing",
      "multi-hop factual recall",
      "causal analysis",
      "query neurons",
      "value neurons",
      "transformer layers",
      "Attribution-Controlled Knowledge Editing",
      "GPT-J",
      "Qwen3-8B",
      "semantic interpretability"
    ]
  },
  {
    "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation",
    "summary": "As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate a single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces a next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-clean-token-prediction in the embedding space. We employ a probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images.",
    "authors": [
      "Yao Teng",
      "Fuyun Wang",
      "Xian Liu",
      "Zhekai Chen",
      "Han Shi",
      "Yu Wang",
      "Zhenguo Li",
      "Weiyang Liu",
      "Difan Zou",
      "Xihui Liu"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "autoregressive text-to-image models",
      "Speculative Jacobi-Denoising Decoding",
      "denoising process",
      "Jacobi iterations",
      "next-clean-token prediction",
      "token embeddings",
      "low-cost fine-tuning",
      "Gaussian noise",
      "next-clean-token-prediction",
      "probabilistic criterion",
      "visual quality"
    ]
  },
  {
    "title": "AutoPR: Let's Automate Your Academic Promotion!",
    "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.",
    "authors": [
      "Qiguang Chen",
      "Zheng Yan",
      "Mingda Yang",
      "Libo Qin",
      "Yixin Yuan",
      "Hanjing Li",
      "Jinhao Liu",
      "Yiyan Ji",
      "Dengyun Peng",
      "Jiannan Guan",
      "Mengkang Hu",
      "Yantao Du",
      "Wanxiang Che"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "multimodal benchmark",
      "content extraction",
      "collaborative synthesis",
      "platform-specific adaptation",
      "LLM pipelines"
    ]
  },
  {
    "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
    "summary": "Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.",
    "authors": [
      "Shuang Chen",
      "Yue Guo",
      "Yimeng Ye",
      "Shijue Huang",
      "Wenbo Hu",
      "Haoxi Li",
      "Manyuan Zhang",
      "Jiayu Chen",
      "Song Guo",
      "Nanyun Peng"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "multimodal large reasoning models",
      "MLRMs",
      "adaptive reasoning",
      "high window-entropy",
      "HWE",
      "Adaptive Cold-Start",
      "Adaptive Entropy Policy Optimization",
      "AEPO",
      "entropy reward",
      "dynamic KL control"
    ]
  },
  {
    "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a mutual welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and mutual welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .",
    "authors": [
      "Siqi Zhu",
      "David Zhang",
      "Pedro Cisneros-Velarde",
      "Jiaxuan You"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Large Language Models",
      "Game-Theoretic Alignment",
      "GTAlign",
      "strategic game",
      "payoff matrices",
      "mutual welfare reward",
      "inference technique",
      "reasoning efficiency",
      "answer quality",
      "mutual welfare"
    ]
  },
  {
    "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval",
    "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries spanning 23 domains, with positive documents carefully verified by human experts. Compared to prior benchmarks, MRMR introduces three key advancements. First, it challenges retrieval systems across diverse areas of expertise, enabling fine-grained model comparison across domains. Second, queries are reasoning-intensive, with images requiring deeper interpretation such as diagnosing microscopic slides. We further introduce Contradiction Retrieval, a novel task requiring models to identify conflicting concepts. Finally, queries and documents are constructed as image-text interleaved sequences. Unlike earlier benchmarks restricted to single images or unimodal documents, MRMR offers a realistic setting with multi-image queries and mixed-modality corpus documents. We conduct an extensive evaluation of 4 categories of multimodal retrieval systems and 14 frontier models on MRMR. The text embedding model Qwen3-Embedding with LLM-generated image captions achieves the highest performance, highlighting substantial room for improving multimodal retrieval models. Although latest multimodal models such as Ops-MM-Embedding perform competitively on expert-domain queries, they fall short on reasoning-intensive tasks. We believe that MRMR paves the way for advancing multimodal retrieval in more realistic and challenging scenarios.",
    "authors": [
      "Siyue Zhang",
      "Yuan Gao",
      "Xiao Zhou",
      "Yilun Zhao",
      "Tingyu Song",
      "Arman Cohan",
      "Anh Tuan Luu",
      "Chen Zhao"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "multimodal retrieval",
      "expert-level",
      "multidisciplinary",
      "reasoning-intensive",
      "Contradiction Retrieval",
      "image-text interleaved",
      "multimodal models",
      "text embedding",
      "LLM-generated image captions",
      "Ops-MM-Embedding"
    ]
  },
  {
    "title": "Understanding DeepResearch via Reports",
    "summary": "DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: https://github.com/HKUDS/DeepResearch-Eval.",
    "authors": [
      "Tianyu Fan",
      "Xinyao Niu",
      "Yuxiang Zheng",
      "Fengji Zhang",
      "Chengen Huang",
      "Bei Chen",
      "Junyang Lin",
      "Chao Huang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "DeepResearch",
      "LLM",
      "research reports",
      "LLM-as-a-Judge",
      "expert concordance",
      "standardized benchmark",
      "real-world categories",
      "intelligent research partners"
    ]
  },
  {
    "title": "StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics",
    "summary": "Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce StatEval, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.",
    "authors": [
      "Yuchen Lu",
      "Run Yang",
      "Yichen Zhang",
      "Shuguang Yu",
      "Runpeng Dai",
      "Ziwei Wang",
      "Jiayi Xiang",
      "Wenxin E",
      "Siran Gao",
      "Xinyao Ruan",
      "Yirui Huang",
      "Chenjing Xi",
      "Haibo Hu",
      "Yueming Fu",
      "Qinglan Yu",
      "Xiaobing Wei",
      "Jiani Gu",
      "Rui Sun",
      "Jiaxuan Jia",
      "Fan Zhou"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "mathematical reasoning",
      "logical reasoning",
      "statistics",
      "benchmark",
      "foundational problems",
      "research-level proof tasks",
      "multi-agent pipeline",
      "human-in-the-loop validation",
      "evaluation framework",
      "computational tasks",
      "proof-based tasks",
      "reasoning ability",
      "GPT5-mini",
      "statistical intelligence"
    ]
  },
  {
    "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks",
    "summary": "Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agent's planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution.",
    "authors": [
      "Shuzheng Si",
      "Haozhe Zhao",
      "Kangyang Luo",
      "Gang Chen",
      "Fanchao Qi",
      "Minjia Zhang",
      "Baobao Chang",
      "Maosong Sun"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "plan-and-execute framework",
      "EAGLET",
      "global planner",
      "homologous consensus filtering",
      "fine-tuning",
      "rule-based reinforcement learning",
      "executor capability gain reward",
      "long-horizon tasks"
    ]
  },
  {
    "title": "Mitigating Overthinking through Reasoning Shaping",
    "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes.",
    "authors": [
      "Feifan Song",
      "Shaohang Wei",
      "Bofei Gao",
      "Yejie Wang",
      "Wen Luo",
      "Wei Li",
      "Linli Yao",
      "Weimin Xiong",
      "Liang Chen",
      "Tianyu Liu",
      "Houfeng Wang"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Reinforcement Learning from Verifier Reward (RLVR)",
      "overthinking",
      "token consumption",
      "model performance",
      "token-level supervision",
      "Group Relative Segment Penalization (GRSP)",
      "step-level method",
      "reasoning segments",
      "length-aware weighting mechanism",
      "segment clusters",
      "RL training",
      "model sizes"
    ]
  },
  {
    "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels",
    "summary": "Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100times fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.",
    "authors": [
      "Zhepeng Cen",
      "Haolin Chen",
      "Shiyu Wang",
      "Zuxin Liu",
      "Zhiwei Liu",
      "Ding Zhao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang",
      "Weiran Yao"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Large Language Models",
      "imitation learning",
      "reinforcement learning",
      "Webscale-RL pipeline",
      "Webscale-RL dataset",
      "continual pretraining",
      "data refinement",
      "benchmarks"
    ]
  },
  {
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
    "authors": [
      "Runyang You",
      "Yongqi Li",
      "Meng Liu",
      "Wenjie Wang",
      "Liqiang Nie",
      "Wenjie Li"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Parallel test-time scaling",
      "large language models",
      "token-based chains-of-thought",
      "latent reasoning",
      "continuous vector spaces",
      "Monte Carlo Dropout",
      "Additive Gaussian Noise",
      "Latent Reward Model",
      "step-wise contrastive objective",
      "scalable inference"
    ]
  },
  {
    "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
    "summary": "The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.",
    "authors": [
      "Zixin Zhang",
      "Kanghao Chen",
      "Xingwang Lin",
      "Lutao Jiang",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Litao Guo",
      "Yinchuan Li",
      "Ying-Cong Chen"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "MLLMs",
      "embodied AI",
      "Vision-Language-Action",
      "VLA",
      "Visual Question Answering",
      "VQA",
      "Tool Recognition",
      "Tool Understanding",
      "Tool Creation"
    ]
  },
  {
    "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
    "summary": "Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.",
    "authors": [
      "Yumin Choi",
      "Dongki Kim",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Large Language Models",
      "multimodal expansions",
      "prompt optimization",
      "Multimodal Prompt Optimizer",
      "alignment-preserving updates",
      "Bayesian-based selection strategy"
    ]
  },
  {
    "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
    "summary": "Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.",
    "authors": [
      "Kang Liao",
      "Size Wu",
      "Zhonghua Wu",
      "Linyi Jin",
      "Chao Wang",
      "Yikai Wang",
      "Fei Wang",
      "Wei Li",
      "Chen Change Loy"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "language regression",
      "diffusion-based generation",
      "camera-centric",
      "multimodal model",
      "spatial awareness",
      "vision-language",
      "camera as language",
      "geometric context",
      "Puffin-4M",
      "global camera parameters",
      "pixel-wise camera maps",
      "spatial generation",
      "instruction tuning",
      "spatial imagination",
      "world exploration",
      "photography guidance"
    ]
  },
  {
    "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
    "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
    "authors": [
      "Yi Lu",
      "Jianing Wang",
      "Linsen Guo",
      "Wei He",
      "Hongyin Tang",
      "Tao Gui",
      "Xuanjing Huang",
      "Xuezhi Cao",
      "Wei Wang",
      "Xunliang Cai"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Chain-of-Thought",
      "CoT",
      "Large Reasoning Models",
      "LRMs",
      "long-horizon reasoning",
      "query composition",
      "reinforcement learning with verified rewards",
      "RLVR",
      "AIME2024"
    ]
  },
  {
    "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
    "summary": "Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.",
    "authors": [
      "Minkyoung Cho",
      "Ruben Ohana",
      "Christian Jacobsen",
      "Adityan Jothi",
      "Min-Hung Chen",
      "Z. Morley Mao",
      "Ethem Can"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "diffusion models",
      "TC-LoRA",
      "Temporally Modulated Conditional LoRA",
      "hypernetwork",
      "LoRA adapters",
      "generative fidelity",
      "spatial conditions",
      "dynamic conditioning",
      "static conditioning",
      "activation-based methods"
    ]
  },
  {
    "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
    "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.",
    "authors": [
      "Ci-Siang Lin",
      "Min-Hung Chen",
      "I-Jieh Liu",
      "Chien-Yi Wang",
      "Sifei Liu",
      "Yu-Chiang Frank Wang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Referring Video Object Segmentation (RVOS)",
      "Temporal Prompt Generation and Selection (Tenet)",
      "foundation segmentation models",
      "temporal prompts",
      "Prompt Preference Learning",
      "object detectors",
      "trackers"
    ]
  },
  {
    "title": "Instant4D: 4D Gaussian Splatting in Minutes",
    "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.",
    "authors": [
      "Zhanpeng Luo",
      "Haoxi Ran",
      "Li Lu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "deep visual SLAM",
      "4D representation",
      "grid pruning",
      "4D Gaussian representation"
    ]
  },
  {
    "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
    "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.",
    "authors": [
      "Terry Yue Zhuo",
      "Xiaolong Jin",
      "Hange Liu",
      "Juyong Jiang",
      "Tianyang Liu",
      "Chen Gong",
      "Bhupesh Bishnoi",
      "Vaisakhi Mishra",
      "Marek Suppa",
      "Noah Ziems",
      "Saiteja Utpala",
      "Ming Xu",
      "Guangyu Song",
      "Kaixin Li",
      "Yuhan Cao",
      "Bo Liu",
      "Zheng Liu",
      "Sabina Abdurakhmanova",
      "Wenhao Yu",
      "Mengzhao Jia",
      "Jihan Yao",
      "Kenneth Hamilton",
      "Kumar Shridhar",
      "Minh Chien Vu",
      "Dingmin Wang",
      "Jiawei Liu",
      "Zijian Wang",
      "Qian Liu",
      "Binyuan Hui",
      "Meg Risdal",
      "Ahsen Khaliq",
      "Atin Sood",
      "Zhenchang Xing",
      "Wasi Uddin Ahmad",
      "John Grundy",
      "David Lo",
      "Banghua Zhu",
      "Xiaoning Du",
      "Torsten Scholak",
      "Leandro von Werra"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Chatbot Arena",
      "BigCodeArena",
      "LLM-generated code",
      "code execution",
      "human evaluation",
      "BigCodeReward",
      "AutoCodeArena",
      "Elo rating benchmark",
      "code understanding",
      "code generation"
    ]
  },
  {
    "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
    "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ",
    "authors": [
      "Chi Yan",
      "Dan Xu"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "3D occupancy prediction",
      "Gaussian representation",
      "dense representation",
      "Progressive Gaussian Transformer Framework",
      "progressive online densification",
      "feed-forward strategy",
      "anisotropy-aware sampling",
      "spatio-temporal fusion",
      "mIoU improvement"
    ]
  },
  {
    "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
    "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a \"Formulation Brain\" for high-level reasoning to pace and guide a separate \"Articulation Brain\" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction.",
    "authors": [
      "Donghang Wu",
      "Haoyang Zhang",
      "Jun Chen",
      "Xiangyu",
      "Zhang",
      "Hexin Liu",
      "Eng Siong Chng",
      "Fei Tian",
      "Xuerui Yang",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Gang Yu"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "Chain-of-Thought (CoT) reasoning",
      "Mind-Paced Speaking (MPS)",
      "dual-brain approach",
      "Formulation Brain",
      "Articulation Brain",
      "real-time reasoning",
      "Spoken-MQA",
      "URO-Bench"
    ]
  },
  {
    "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
    "summary": "With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km .",
    "authors": [
      "Peiwen Sun",
      "Shiqiang Lang",
      "Dongming Wu",
      "Yi Ding",
      "Kaituo Feng",
      "Huadai Liu",
      "Zhen Ye",
      "Rui Liu",
      "Yun-Hui Liu",
      "Jianan Wang",
      "Xiangyu Yue"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "structured spatial reasoning",
      "scale-aware modeling",
      "progressive training",
      "spatial QA pairs",
      "spatial reasoning model",
      "scale-aware experts",
      "progressive rewards",
      "SpaceVista-1M",
      "SpaceVista-7B",
      "SpaceVista-Bench"
    ]
  },
  {
    "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
    "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
    "authors": [
      "Ruyi Xu",
      "Guangxuan Xiao",
      "Yukang Chen",
      "Liuning He",
      "Kelly Peng",
      "Yao Lu",
      "Song Han"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "vision-language models",
      "VLMs",
      "real-time assistants",
      "autonomous agents",
      "video streams",
      "quadratic computational costs",
      "sliding window methods",
      "attention sinks",
      "vision tokens",
      "text tokens",
      "supervised fine-tuning",
      "SFT",
      "inference-time attention pattern",
      "Inf-Streams-Eval",
      "win rate",
      "NVIDIA H100",
      "VQA abilities",
      "LongVideoBench",
      "OVOBench Realtime"
    ]
  },
  {
    "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
    "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.",
    "authors": [
      "Gaurav Sahu",
      "Hugo Larochelle",
      "Laurent Charlin",
      "Christopher Pal"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "AI-assisted peer review",
      "reviewer personas",
      "structured evaluation criteria",
      "gpt-oss-120b",
      "accuracy",
      "LLM judge",
      "methodological novelty",
      "theoretical contributions",
      "hybrid peer-review systems"
    ]
  },
  {
    "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
    "summary": "Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.",
    "authors": [
      "Hyunmin Cho",
      "Donghoon Ahn",
      "Susung Hong",
      "Jee Eun Kim",
      "Seungryong Kim",
      "Kyong Hwan Jin"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "diffusion models",
      "image generation",
      "semantic inconsistencies",
      "hallucinations",
      "inference-time guidance",
      "trajectory signals",
      "intermediate sample",
      "projection basis",
      "tangential components",
      "first-order Taylor expansion",
      "sampling trajectory",
      "higher-probability regions",
      "plug-and-play",
      "architecture-agnostic"
    ]
  },
  {
    "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
    "summary": "Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.",
    "authors": [
      "Xiao Yu",
      "Baolin Peng",
      "Michel Galley",
      "Hao Cheng",
      "Qianhui Wu",
      "Janardhan Kulkarni",
      "Suman Nath",
      "Zhou Yu",
      "Jianfeng Gao"
    ],
    "date": "2025-10-10",
    "ai_keywords": [
      "vicarious trial and error",
      "Dyna-Mind",
      "Reasoning with Simulations",
      "ReSim",
      "Dyna-GRPO",
      "online reinforcement learning",
      "simulation ability",
      "reasoning",
      "planning",
      "Sokoban",
      "ALFWorld",
      "AndroidWorld"
    ]
  },
  {
    "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as Likelihood Estimation with Negative Samples (LENS). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR.",
    "authors": [
      "Yunzhen Feng",
      "Parag Jain",
      "Anthony Hartshorn",
      "Yaqi Duan",
      "Julia Kempe"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Reinforcement learning with verifiable rewards",
      "RLVR",
      "Group Relative Policy Optimization",
      "GRPO",
      "maximum-likelihood",
      "MLE",
      "policy gradient",
      "value function",
      "confidence-weighted penalty",
      "LENS",
      "MATH benchmark",
      "Llama-3.1-8B",
      "Qwen-2.5-3B"
    ]
  },
  {
    "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
    "summary": "Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.",
    "authors": [
      "Yi-Cheng Lin",
      "Yu-Hsuan Li Liang",
      "Hsuan Su",
      "Tzu-Quan Lin",
      "Shang-Tse Chen",
      "Yun-Nung Chen",
      "Hung-yi Lee"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "ASR",
      "domain shift",
      "pseudo-labeling",
      "parameter-space correction",
      "fine-tuning",
      "correction vector",
      "Word Error Rate (WER)",
      "AfriSpeech-200",
      "Whisper tiny model"
    ]
  },
  {
    "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward\n  Modeling and LLM Alignment",
    "summary": "Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.",
    "authors": [
      "Tianci Liu",
      "Ran Xu",
      "Tony Yu",
      "Ilgee Hong",
      "Carl Yang",
      "Tuo Zhao",
      "Haoyu Wang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "reinforcement learning from human feedback",
      "reward models",
      "rubrics-as-rewards",
      "structured natural language criteria",
      "Contrastive Rubric Generation",
      "hard rules",
      "principles",
      "preference-label consistency",
      "rejection sampling",
      "Rubric-RM",
      "instruction-following",
      "biomedical benchmarks",
      "LLM alignment"
    ]
  },
  {
    "title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement\n  Learning",
    "summary": "The use of target networks is a popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains a compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as a bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing a novel update rule that computes the target using the MINimum estimate between the Target and Online network, giving rise to our method, MINTO. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into a wide range of value-based and actor-critic algorithms with a negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness.",
    "authors": [
      "Ahmed Hendawy",
      "Henrik Metternich",
      "Th\u00e9o Vincent",
      "Mahdi Kallel",
      "Jan Peters",
      "Carlo D'Eramo"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "target networks",
      "value functions",
      "deep Reinforcement Learning",
      "online network",
      "bootstrapped target",
      "MINTO",
      "value-based algorithms",
      "actor-critic algorithms",
      "overestimation bias",
      "online RL",
      "offline RL",
      "discrete action spaces",
      "continuous action spaces"
    ]
  },
  {
    "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
    "summary": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.",
    "authors": [
      "Bin Xia",
      "Bohao Peng",
      "Yuechen Zhang",
      "Junjia Huang",
      "Jiyang Liu",
      "Jingyao Li",
      "Haoru Tan",
      "Sitong Wu",
      "Chengyao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "instruction-based editing",
      "subject-driven generation",
      "multimodal instruction-based editing",
      "feature mixing",
      "index encoding",
      "position encoding",
      "joint training",
      "VLM"
    ]
  },
  {
    "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction",
    "summary": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.",
    "authors": [
      "Lujie Yang",
      "Xiaoyu Huang",
      "Zhen Wu",
      "Angjoo Kanazawa",
      "Pieter Abbeel",
      "Carmelo Sferrazza",
      "C. Karen Liu",
      "Rocky Duan",
      "Guanya Shi"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "reinforcement learning (RL)",
      "retargeting",
      "embodiment gap",
      "foot-skating",
      "penetration",
      "human-object interactions",
      "human-environment interactions",
      "interaction mesh",
      "Laplacian deformation",
      "kinematic constraints",
      "data augmentation",
      "proprioceptive RL policies",
      "long-horizon skills",
      "parkour",
      "loco-manipulation",
      "Unitree G1",
      "reward terms",
      "domain randomization"
    ]
  },
  {
    "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
    "summary": "We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.",
    "authors": [
      "Soroush Mehraban",
      "Vida Adeli",
      "Jacob Rommann",
      "Babak Taati",
      "Kyryl Truskovskyi"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "diffusion models",
      "video style transfer",
      "style adapters",
      "self-attention layers",
      "conditioning modules",
      "synthetic training clips",
      "camera motion",
      "Context-Style Classifier-Free Guidance",
      "CS-CFG",
      "temporally coherent",
      "style-faithful",
      "content-preserving"
    ]
  },
  {
    "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
    "summary": "Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.",
    "authors": [
      "Jiahao Wang",
      "Zhenpei Yang",
      "Yijing Bai",
      "Yingwei Li",
      "Yuliang Zou",
      "Bo Sun",
      "Abhijit Kundu",
      "Jose Lezama",
      "Luna Yue Huang",
      "Zehao Zhu",
      "Jyh-Jing Hwang",
      "Dragomir Anguelov",
      "Mingxing Tan",
      "Chiyu Max Jiang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "generative models",
      "autonomous vehicles",
      "video generation models",
      "end-to-end driving models",
      "statistical measures",
      "E2E planners",
      "distribution gaps",
      "synthetic data",
      "Operational Design Domains"
    ]
  },
  {
    "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
    "summary": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3Dleftrightarrow5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.",
    "authors": [
      "Fabian Paischer",
      "Gianluca Galletti",
      "William Hornsby",
      "Paul Setinek",
      "Lorenzo Zanisi",
      "Naomi Carey",
      "Stanislas Pamela",
      "Johannes Brandstetter"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "gyrokinetic equation",
      "reduced-order models",
      "neural surrogate",
      "Vision Transformers",
      "cross-attention",
      "integration modules",
      "channelwise mode separation",
      "heat flux prediction",
      "turbulent energy cascade",
      "nonlinear gyrokinetics",
      "scalable neural surrogates"
    ]
  },
  {
    "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation",
    "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.",
    "authors": [
      "Andreas Engelhardt",
      "Mark Boss",
      "Vikram Voletti",
      "Chun-Han Yao",
      "Hendrik P. A. Lensch",
      "Varun Jampani"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "video diffusion models",
      "physically based rendering (PBR)",
      "spatially varying PBR parameters",
      "surface normals",
      "explicit camera control",
      "neural prior",
      "relighting",
      "novel view synthesis",
      "object-centric datasets",
      "AR/VR",
      "movies",
      "games",
      "visual media"
    ]
  },
  {
    "title": "GCPO: When Contrast Fails, Go Gold",
    "summary": "Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.",
    "authors": [
      "Hao Wu",
      "Wei Liu"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Group Relative Policy Optimization (GRPO)",
      "Group Contrastive Policy Optimization (GCPO)",
      "reinforcement learning",
      "large language models",
      "rollout responses",
      "reference answers",
      "training efficiency",
      "generalization"
    ]
  },
  {
    "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models",
    "summary": "Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3",
    "authors": [
      "Yuntao Gui",
      "James Cheng"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Large Language Models (LLMs)",
      "search embeddings",
      "chain-of-thought capabilities",
      "supervised learning",
      "reinforcement learning (RL)",
      "RL environment",
      "embedding representations",
      "reasoning",
      "information retrieval"
    ]
  },
  {
    "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction",
    "summary": "This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.",
    "authors": [
      "Kaisi Guan",
      "Xihua Wang",
      "Zhengfeng Lai",
      "Xin Cheng",
      "Peng Zhang",
      "XiaoJiang Liu",
      "Ruihua Song",
      "Meng Cao"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Hierarchical Visual-Grounded Captioning",
      "HVGC",
      "BridgeDiT",
      "dual-tower diffusion transformer",
      "Dual CrossAttention",
      "DCA",
      "semantic synchronization",
      "temporal synchronization"
    ]
  },
  {
    "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
    "summary": "Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as pi_0 and Diffusion Policy improves OOD success rates by over 54\\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.",
    "authors": [
      "Zizhao Tong",
      "Di Chen",
      "Sicheng Hu",
      "Hongwei Fan",
      "Liliang Chen",
      "Guanghui Ren",
      "Hao Tang",
      "Hao Dong",
      "Ling Shao"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "shortcut learning",
      "out-of-distribution (OOD) generalization",
      "generative data augmentation",
      "data composition",
      "Information Fidelity",
      "feature-space geometry",
      "Decoherence Point",
      "Multi-View Video Augmentation (MVAug)",
      "causally disentangled",
      "policy architectures",
      "Diffusion Policy"
    ]
  },
  {
    "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for\n  Long-Horizon Tasks",
    "summary": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.",
    "authors": [
      "Cheng Yang",
      "Xuemeng Yang",
      "Licheng Wen",
      "Daocheng Fu",
      "Jianbiao Mei",
      "Rong Wu",
      "Pinlong Cai",
      "Yufan Shen",
      "Nianchen Deng",
      "Botian Shi",
      "Yu Qiao",
      "Haifeng Li"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Language Models",
      "LLM agents",
      "experience-driven",
      "self-evolving system",
      "hierarchical Memory Module",
      "long-horizon tasks",
      "trajectory reflection",
      "structured experience",
      "continuous learning",
      "self-evolution",
      "TAC benchmark",
      "Gemini-2.5 Flash model",
      "zero-shot improvement"
    ]
  },
  {
    "title": "Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens",
    "summary": "The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.",
    "authors": [
      "Shaohua Zhang",
      "Yuan Lin",
      "Hang Li"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "large language models",
      "function tokens",
      "memory retrieval",
      "memory consolidation",
      "next token prediction",
      "bipartite graph analysis",
      "content tokens"
    ]
  },
  {
    "title": "Beyond Outliers: A Study of Optimizers Under Quantization",
    "summary": "As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers.",
    "authors": [
      "Georgios Vlassis",
      "Saleh Ashkboos",
      "Alexandra Volkova",
      "Torsten Hoefler",
      "Dan Alistarh"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "optimizers",
      "model quantization",
      "post-training quantization",
      "quantization-aware training",
      "hyperparameter landscape",
      "max-to-mean ratio",
      "Kurtosis",
      "Shampoo",
      "scaling laws",
      "parameter efficiency"
    ]
  },
  {
    "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections",
    "summary": "We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body images with minimal occlusions, or well-calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce a data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in a single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is a pose-correlated feature aggregation module (PCFA), that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. We also introduce a perceiver-based multi-reference shape predictor, removing the need for pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task. Project Page: https://zcai0612.github.io/UP2You",
    "authors": [
      "Zeyu Cai",
      "Ziyang Li",
      "Xiaoben Li",
      "Boqian Li",
      "Zeyu Wang",
      "Zhenyu Zhang",
      "Yuliang Xiu"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "data rectifier",
      "pose-correlated feature aggregation",
      "perceiver-based multi-reference shape predictor",
      "Chamfer",
      "P2S",
      "PSNR",
      "LPIPS",
      "3D reconstruction",
      "3D virtual try-on"
    ]
  },
  {
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "summary": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.",
    "authors": [
      "Xiangyuan Xue",
      "Yifan Zhou",
      "Guibin Zhang",
      "Zaibin Zhang",
      "Yijiang Li",
      "Chen Zhang",
      "Zhenfei Yin",
      "Philip Torr",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "self-evolution",
      "large language model (LLM)",
      "reinforcement learning (RL)",
      "intrinsic reward signals",
      "Co-Evolving Multi-Agent Systems (CoMAS)",
      "LLM-as-a-judge",
      "decentralized",
      "scalable co-evolution"
    ]
  },
  {
    "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
    "summary": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
    "authors": [
      "Youjin Wang",
      "Yangjingyi Chen",
      "Jiahao Yan",
      "Jiaxuan Lu",
      "Xiao Sun"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "recurrent neural networks",
      "gradient vanishing",
      "gradient explosion",
      "Transformers",
      "quadratic complexity",
      "selective state-space models",
      "Mamba",
      "memory decay",
      "information-theoretic analysis",
      "horizontal-vertical memory fidelity",
      "cross-layer attention",
      "cross-token attention",
      "PG19",
      "Passkey Retrieval",
      "ultra-long sequence modeling"
    ]
  },
  {
    "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement\n  Learning",
    "summary": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.",
    "authors": [
      "Yoonjeon Kim",
      "Doohyuk Jang",
      "Eunho Yang"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "meta-awareness",
      "reasoning models",
      "meta-prediction",
      "rollouts",
      "self-alignment",
      "zero-variance prompts",
      "GRPO",
      "AIME25",
      "GPQA-Diamond"
    ]
  },
  {
    "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning",
    "summary": "The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.",
    "authors": [
      "Cheng Yang",
      "Jiaxuan Lu",
      "Haiyuan Wan",
      "Junchi Yu",
      "Feiwei Qin"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "multi-agent system",
      "evidence-based reasoning",
      "mechanistic grounding",
      "multi-channel recall",
      "constraint-aware agentic debate",
      "rationale aggregation",
      "explainable AI"
    ]
  },
  {
    "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
    "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance.",
    "authors": [
      "Chong Mou",
      "Qichao Sun",
      "Yanze Wu",
      "Pengze Zhang",
      "Xinghui Li",
      "Fulong Ye",
      "Songtao Zhao",
      "Qian He"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "diffusion models",
      "instruction-driven editing",
      "video editing",
      "image editing",
      "unified modeling",
      "modality-specific features"
    ]
  },
  {
    "title": "Towards Scalable and Consistent 3D Editing",
    "summary": "3D editing - the task of locally modifying the geometry or appearance of a 3D asset - has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: https://www.lv-lab.org/3DEditFormer/",
    "authors": [
      "Ruihao Xia",
      "Yang Tang",
      "Pan Zhou"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "3D editing",
      "3DEditVerse",
      "3DEditFormer",
      "3D-structure-preserving conditional transformer",
      "dual-guidance attention",
      "time-adaptive gating",
      "edit locality",
      "multi-view consistency",
      "semantic alignment",
      "image-to-3D generation"
    ]
  },
  {
    "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
    "summary": "Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.",
    "authors": [
      "XuHao Hu",
      "Peng Wang",
      "Xiaoya Lu",
      "Dongrui Liu",
      "Xuanjing Huang",
      "Jing Shao"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "LLMs",
      "finetuning",
      "emergent misalignment",
      "dishonesty",
      "deception",
      "high-stakes scenarios",
      "downstream tasks",
      "human-AI interaction",
      "biased users"
    ]
  },
  {
    "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
    "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).",
    "authors": [
      "Soyeong Jeong",
      "Taehee Jung",
      "Sung Ju Hwang",
      "Joo-Kyung Kim",
      "Dongyeop Kang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Long-Context Language Models",
      "thought templates",
      "reusable thought caches",
      "multi-hop reasoning",
      "evidence combination",
      "natural-language feedback",
      "retrieval-based",
      "retrieval-free",
      "ToTAL"
    ]
  },
  {
    "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation",
    "summary": "On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/.",
    "authors": [
      "Guanghao Li",
      "Kerui Ren",
      "Linning Xu",
      "Zhewen Zheng",
      "Changjian Jiang",
      "Xin Gao",
      "Bo Dai",
      "Jian Pu",
      "Mulin Yu",
      "Jiangmiao Pang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "3D reconstruction",
      "monocular image sequences",
      "real-to-sim",
      "AR/VR",
      "robotics",
      "per-scene optimization",
      "feed-forward foundation models",
      "SLAM-based pipelines",
      "3D foundation models",
      "pose estimation",
      "point prediction",
      "Gaussian decoder",
      "multi-scale features",
      "structured 3D Gaussians",
      "hierarchical Gaussian representation",
      "LoD-aware rendering strategy",
      "rendering fidelity",
      "redundancy",
      "interactive performance",
      "reconstruction quality"
    ]
  },
  {
    "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
    "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.",
    "authors": [
      "Jingyu Zhang",
      "Haozhu Wang",
      "Eric Michael Smith",
      "Sid Wang",
      "Amr Sharaf",
      "Mahesh Pasupuleti",
      "Benjamin Van Durme",
      "Daniel Khashabi",
      "Jason Weston",
      "Hongyuan Zhan"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "LLMs",
      "adversarial attacks",
      "safeguard models",
      "multi-agent reinforcement learning",
      "conversation agent",
      "feedback agent",
      "Dynamic Improvement Reward",
      "DIR",
      "WildJailbreak",
      "OR-Bench",
      "Pareto front"
    ]
  },
  {
    "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window",
    "summary": "While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.",
    "authors": [
      "Qiaoyu Tang",
      "Hao Xiang",
      "Le Yu",
      "Bowen Yu",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun",
      "WenJuan Zhang",
      "Pengbo Wang",
      "Shixuan Liu",
      "Zhenru Zhang",
      "Jianhong Tu",
      "Hongyu Lin",
      "Junyang Lin"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "reinforcement learning",
      "multi-turn agents",
      "long-horizon interactions",
      "high-difficulty training tasks",
      "dynamic context window",
      "reverse construction method",
      "question-answer pairs",
      "sliding window mechanisms",
      "dynamic context management",
      "BrowseComp-en",
      "BrowseComp-zh",
      "XBench-DeepSearch",
      "GAIA"
    ]
  },
  {
    "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
    "summary": "Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.",
    "authors": [
      "Leitian Tao",
      "Ilia Kulikov",
      "Swarnadeep Saha",
      "Tianlu Wang",
      "Jing Xu",
      "Yixuan Li",
      "Jason E Weston",
      "Ping Yu"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "verifiable rewards",
      "deterministic checkers",
      "binary feedback",
      "reward models",
      "continuous feedback",
      "reinforcement learning",
      "HERO",
      "Hybrid Ensemble Reward Optimization",
      "stratified normalization",
      "variance-aware weighting",
      "mathematical reasoning benchmarks"
    ]
  },
  {
    "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model",
    "summary": "Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a \"reality gap\" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
    "authors": [
      "Xueyi Liu",
      "He Wang",
      "Li Yi"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "joint-wise dynamics model",
      "reality gap",
      "data-efficient",
      "generalizable",
      "dynamic profile",
      "autonomous data collection",
      "teleoperation application"
    ]
  },
  {
    "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
    "summary": "While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.",
    "authors": [
      "Yihong Luo",
      "Tianyang Hu",
      "Jing Tang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Group Relative Preference Optimization",
      "GRPO",
      "diffusion models",
      "stochastic policy",
      "deterministic ODEs",
      "SDE-based samplers",
      "Gaussian noise",
      "Direct Group Preference Optimization",
      "DGPO",
      "policy-gradient framework",
      "group-level preferences",
      "in-domain reward metrics",
      "out-of-domain reward metrics"
    ]
  },
  {
    "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video\n  Super-Resolution",
    "summary": "Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.",
    "authors": [
      "Shian Du",
      "Menghan Xia",
      "Chang Liu",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Xiangyang Ji"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "cascaded video super-resolution",
      "foundation models",
      "text-to-video tasks",
      "generative conditions",
      "multi-modal video generation",
      "unified generative video super-resolution",
      "hybrid-modal conditions",
      "latent video diffusion model",
      "condition injection strategies",
      "training schemes",
      "data mixture techniques",
      "multi-modal guided generation"
    ]
  },
  {
    "title": "A^2Search: Ambiguity-Aware Question Answering with Reinforcement\n  Learning",
    "summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A^2Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed AnsF1 reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A^2Search achieves new state-of-the-art performance. With only a single rollout, A^2Search-7B yields an average AnsF1@1 score of 48.4% across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B (46.2%). Extensive analyses further show that A^2Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
    "authors": [
      "Fengji Zhang",
      "Xinyao Niu",
      "Chengyang Ying",
      "Guancheng Lin",
      "Zhongkai Hao",
      "Zhou Fan",
      "Chengen Huang",
      "Jacky Keung",
      "Bei Chen",
      "Junyang Lin"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Language Models",
      "Reinforcement Learning",
      "open-domain question answering",
      "ambiguity",
      "standard QA benchmarks",
      "trajectory sampling",
      "evidence verification",
      "AnsF1 reward",
      "multi-hop benchmarks",
      "ReSearch-32B"
    ]
  },
  {
    "title": "Recycling Pretrained Checkpoints: Orthogonal Growth of\n  Mixture-of-Experts for Efficient Large Language Model Pre-Training",
    "summary": "The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this \"sunk\" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining.",
    "authors": [
      "Ruizhe Wang",
      "Yucheng Ding",
      "Xiao Liu",
      "Yaoxiang Wang",
      "Peng Cheng",
      "Baining Guo",
      "Zhengjun Zha",
      "Yeyun Gong"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Language Models",
      "pretrained checkpoints",
      "orthogonal growth method",
      "interpositional layer copying",
      "expert duplication",
      "injected noise",
      "scaling experiments",
      "Mixture-of-Experts model"
    ]
  },
  {
    "title": "Training-Free Group Relative Policy Optimization",
    "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
    "authors": [
      "Yuzheng Cai",
      "Siqi Cai",
      "Yuchen Shi",
      "Zihan Xu",
      "Lichao Chen",
      "Yulei Qin",
      "Xiaoyu Tan",
      "Gang Li",
      "Zongyi Li",
      "Haojia Lin",
      "Yong Mao",
      "Ke Li",
      "Xing Sun"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Language Model (LLM)",
      "agentic reinforcement learning",
      "Supervised Fine-Tuning (SFT)",
      "Reinforcement Learning (RL)",
      "Group Relative Policy Optimization (GRPO)",
      "token prior",
      "Training-Free GRPO",
      "group relative semantic advantage",
      "multi-epoch learning",
      "minimal ground-truth data",
      "DeepSeek-V3.1-Terminus",
      "mathematical reasoning",
      "web searching tasks"
    ]
  },
  {
    "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \\textit{reasoning sparks}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of reasoning sparks is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a 60.17% average accuracy on five math benchmarks, an improvement of 2.66% over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
    "authors": [
      "Guanhua Huang",
      "Tingqiang Xu",
      "Mingze Wang",
      "Qi Yi",
      "Xue Gong",
      "Siheng Li",
      "Ruibin Xiong",
      "Kejiao Li",
      "Yuhao Jiang",
      "Bo Zhou"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Reinforcement Learning with Verifiable Rewards",
      "RLVR",
      "policy entropy",
      "exploration dynamics",
      "reasoning sparks",
      "Low-probability Regularization",
      "Lp-Reg",
      "heuristic proxy distribution",
      "KL divergence",
      "on-policy training",
      "math benchmarks"
    ]
  },
  {
    "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
    "summary": "We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds by applying specially designed activations to the outputs of models. Our approach demonstrates broad effectiveness across different domains: 1) for large language models(LLMs), boosting the AIME 2025 score for Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning agents, improving performance by more than 30% over strong baselines such as SAC on the challenging HumanoidBench; 3) for image classification, enhancing ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a computational overhead of less than 7%. Our work validates output activation as a powerful tool for entropy control, opening a new direction for designing simpler and more robust algorithms.",
    "authors": [
      "Zilin Kang",
      "Chonghua Liao",
      "Tingqiang Xu",
      "Huazhe Xu"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "ERA",
      "sampling entropy",
      "activations",
      "large language models",
      "LLMs",
      "AIME 2025",
      "Qwen2.5-Math-7B",
      "continuous control",
      "reinforcement learning",
      "SAC",
      "HumanoidBench",
      "image classification",
      "ImageNet",
      "ResNet-50",
      "entropy control"
    ]
  },
  {
    "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
    "summary": "Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.",
    "authors": [
      "Cong Wei",
      "Quande Liu",
      "Zixuan Ye",
      "Qiulin Wang",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Wenhu Chen"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Multimodal Large Language Model",
      "Multimodal DiT",
      "dual-stream design",
      "text/image-to-video generation",
      "in-context video generation",
      "in-context video editing",
      "task composition",
      "style transfer",
      "visual-prompt-based video generation"
    ]
  },
  {
    "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with\n  Holistic Platform and Adaptive Hybrid Policy Optimization",
    "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.",
    "authors": [
      "Xiangyu Zhao",
      "Junming Lin",
      "Tianhao Liang",
      "Yifan Zhou",
      "Wenhao Chai",
      "Yuzhe Gu",
      "Weiyun Wang",
      "Kai Chen",
      "Gen Luo",
      "Wenwei Zhang",
      "Junchi Yan",
      "Hua Yang",
      "Haodong Duan",
      "Xue Yang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "long-chain reflective reasoning",
      "MM-HELIX",
      "Step-Elicited Response Generation",
      "Adaptive Hybrid Policy Optimization",
      "offline supervision",
      "online optimization",
      "Qwen2.5-VL-7B"
    ]
  },
  {
    "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time\n  Consistency",
    "summary": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only 1sim4 steps, accelerating diffusion sampling by 15timessim50times. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
    "authors": [
      "Kaiwen Zheng",
      "Yuji Wang",
      "Qianli Ma",
      "Huayu Chen",
      "Jintao Zhang",
      "Yogesh Balaji",
      "Jianfei Chen",
      "Ming-Yu Liu",
      "Jun Zhu",
      "Qinsheng Zhang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "continuous-time consistency model",
      "sCM",
      "Jacobian-vector product",
      "FlashAttention-2",
      "score-regularized continuous-time consistency model",
      "rCM",
      "score distillation",
      "mode-seeking",
      "mode-covering",
      "diffusion distillation",
      "DMD2",
      "diffusion sampling"
    ]
  },
  {
    "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
    "summary": "Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.",
    "authors": [
      "Tianshi Zheng",
      "Kelvin Kiu-Wai Tam",
      "Newt Hue-Nam K. Nguyen",
      "Baixuan Xu",
      "Zhaowei Wang",
      "Jiayang Cheng",
      "Hong Ting Tsang",
      "Weiqi Wang",
      "Jiaxin Bai",
      "Tianqing Fang",
      "Yangqiu Song",
      "Ginny Y. Wong",
      "Simon See"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "large language models",
      "scientific law discovery",
      "NewtonBench",
      "metaphysical shifts",
      "interactive model discovery",
      "observational noise",
      "code interpreter",
      "exploration",
      "exploitation"
    ]
  },
  {
    "title": "Agent Learning via Early Experience",
    "summary": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
    "authors": [
      "Kai Zhang",
      "Xiangchao Chen",
      "Bo Liu",
      "Tianci Xue",
      "Zeyi Liao",
      "Zhihan Liu",
      "Xiyao Wang",
      "Yuting Ning",
      "Zhaorun Chen",
      "Xiaohan Fu",
      "Jian Xie",
      "Yuxuan Sun",
      "Boyu Gou",
      "Qi Qi",
      "Zihang Meng",
      "Jianwei Yang",
      "Ning Zhang",
      "Xian Li",
      "Ashish Shah",
      "Dat Huynh",
      "Hengduo Li",
      "Zi Yang",
      "Sara Cao",
      "Lawrence Jang",
      "Shuyan Zhou",
      "Jiacheng Zhu",
      "Huan Sun",
      "Jason Weston",
      "Yu Su",
      "Yifan Wu"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "reinforcement learning",
      "early experience",
      "implicit world modeling",
      "self-reflection",
      "out-of-domain generalization"
    ]
  },
  {
    "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning",
    "summary": "We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.",
    "authors": [
      "Minghong Cai",
      "Qiulin Wang",
      "Zongli Ye",
      "Wenze Liu",
      "Quande Liu",
      "Weicai Ye",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Xiangyu Yue"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "arbitrary spatio-temporal video completion",
      "latent video diffusion models",
      "causal VAEs",
      "VideoCanvas",
      "In-Context Conditioning (ICC)",
      "zero-padding",
      "Temporal RoPE Interpolation",
      "VideoCanvasBench",
      "intra-scene fidelity",
      "inter-scene creativity"
    ]
  },
  {
    "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large\n  Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
    "authors": [
      "Andong Deng",
      "Taojiannan Yang",
      "Shoubin Yu",
      "Lincoln Spencer",
      "Mohit Bansal",
      "Chen Chen",
      "Serena Yeung-Levy",
      "Xiaohan Wang"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Large Multimodal Models",
      "LMMs",
      "video reasoning",
      "scientific domain",
      "video benchmarks",
      "perception",
      "recognition",
      "reasoning tasks",
      "multiple-choice questions",
      "spatiotemporal perception",
      "logical reasoning",
      "cognitive abilities",
      "reasoning complexity",
      "visual grounding",
      "multimodal AI co-scientists"
    ]
  },
  {
    "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
    "summary": "Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preference-tunable inference: operators can dial the performance/cost trade-off at test time without retraining. Framed as a contextual bandit over prompt features and a user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks.",
    "authors": [
      "Wang Wei",
      "Tiankai Yang",
      "Hongjie Chen",
      "Yue Zhao",
      "Franck Dernoncourt",
      "Ryan A. Rossi",
      "Hoda Eldardiry"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "large language models",
      "adaptive routing",
      "online decision problem",
      "partial-feedback restriction",
      "contextual bandit",
      "prompt features",
      "user preference vector",
      "performance/cost trade-off"
    ]
  },
  {
    "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation",
    "summary": "Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.",
    "authors": [
      "Xiuwei Xu",
      "Angyuan Ma",
      "Hankun Li",
      "Bingyao Yu",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "spatial generalization",
      "visuomotor policy",
      "imitation learning",
      "data generation",
      "pointcloud",
      "real-to-real",
      "annotation mechanism",
      "group-wise augmentation",
      "camera-aware processing",
      "mobile manipulation"
    ]
  },
  {
    "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
    "summary": "Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
    "authors": [
      "Liwei Kang",
      "Yue Deng",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Wee Sun Lee",
      "Lidong Bing"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "reasoning models",
      "reflective behaviors",
      "confirmatory reflections",
      "supervised fine-tuning",
      "early-stopping method",
      "token efficiency",
      "reasoning tokens",
      "candidate answers"
    ]
  },
  {
    "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
    "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.",
    "authors": [
      "Xiangyu Peng",
      "Cab Qin",
      "Zeyuan Chen",
      "Ran Xu",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "multimodal retrieval-augmented generation",
      "large language models",
      "knowledge bases",
      "UniDoc-Bench",
      "PDF pages",
      "multimodal QA pairs",
      "factual retrieval",
      "comparison",
      "summarization",
      "logical reasoning",
      "text-only",
      "image-only",
      "multimodal text-image fusion",
      "multimodal joint retrieval",
      "candidate pools",
      "prompts",
      "evaluation metrics",
      "visual context",
      "textual evidence",
      "failure modes",
      "robust MM-RAG pipelines"
    ]
  },
  {
    "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
    "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
    "authors": [
      "Zecheng Tang",
      "Baibei Ji",
      "Quantong Qiu",
      "Haitian Wang",
      "Xiaobo Liang",
      "Juntao Li",
      "Min Zhang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "reward model",
      "large language model",
      "long-context",
      "short-context",
      "Pairwise Comparison",
      "Best-of-N",
      "context-response consistency",
      "multi-stage training",
      "long-context RMs",
      "LongRMs"
    ]
  },
  {
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
    "authors": [
      "Shangqing Tu",
      "Yaxuan Li",
      "Yushi Bai",
      "Lei Hou",
      "Juanzi Li"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Chain-of-Thought",
      "DeepPrune",
      "dynamic pruning",
      "judge model",
      "focal loss",
      "oversampling techniques",
      "AUROC",
      "online greedy clustering",
      "token reduction",
      "parallel reasoning",
      "large language models"
    ]
  },
  {
    "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
    "summary": "Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
    "authors": [
      "Changyao Tian",
      "Hao Li",
      "Gen Luo",
      "Xizhou Zhu",
      "Weijie Su",
      "Hanming Deng",
      "Jinguo Zhu",
      "Jie Shao",
      "Ziran Zhu",
      "Yunpeng Liu",
      "Lewei Lu",
      "Wenhai Wang",
      "Hongsheng Li",
      "Jifeng Dai"
    ],
    "date": "2025-10-09",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "compositional training",
      "continuous multimodal pre-training",
      "end-to-end training",
      "meta-architecture",
      "scaling property",
      "visual encoders",
      "LLMs",
      "NaViL"
    ]
  },
  {
    "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of\n  Physics Implausibility",
    "summary": "Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe that combines a balanced training dataset with a trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, a benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer a unified framework for probing and improving physical plausibility in multimodal models, shedding light on a challenging and underexplored aspect of visual-temporal understanding.",
    "authors": [
      "Saman Motamed",
      "Minghao Chen",
      "Luc Van Gool",
      "Iro Laina"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Video-Language Models",
      "VLMs",
      "TRAVL",
      "trajectory-aware attention",
      "motion encoding",
      "discrimination",
      "ImplausiBench",
      "physical plausibility",
      "visual-temporal understanding",
      "LLM-as-judge"
    ]
  },
  {
    "title": "Reinforcement Mid-Training",
    "summary": "The development of state-of-the-art large language models is commonly understood as a two-stage process involving pre-training and post-training. We point out the need for an additional intermediate stage called reinforcement mid-training with potential for strong performance gains. In this paper, we formally define the problem and identify three key challenges: (1) inefficient training due to excessive reasoning steps, (2) disregard of the imbalanced token entropy distribution, and (3) underutilization of token information. To address these challenges, we propose RMT, a framework for efficient, adaptive, and unified reinforcement mid-training with various innovative components. In particular, we first introduce a dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking. Next, we design a curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens. Finally, we present a dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information. Extensive experiments demonstrate the superiority of RMT over state-of-the-art methods, achieving up to +64.91% performance improvement with only 21% of the reasoning length in language modeling. We also show that checkpoints obtained after reinforcement mid-training can benefit the subsequent post-training, yielding up to +18.76% improvement in the mathematical domain.",
    "authors": [
      "Yijun Tian",
      "Shaoyu Chen",
      "Zhichao Xu",
      "Yawei Wang",
      "Jinhe Bi",
      "Peng Han",
      "Wei Wang"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "reinforcement mid-training",
      "dynamic token budget mechanism",
      "curriculum-based adaptive sampling",
      "dual training strategy",
      "reinforcement learning",
      "next-token prediction",
      "language modeling",
      "mathematical domain"
    ]
  },
  {
    "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
    "summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications. The dataset and the baselines code are available in this github page https://github.com/AkashGhosh/M3Retrieve.",
    "authors": [
      "Arkadeep Acharya",
      "Akash Ghosh",
      "Pradeepika Verma",
      "Kitsuchart Pasupa",
      "Sriparna Saha",
      "Priti Singh"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "RetrievalAugmented Generation",
      "multimodal retrieval models",
      "question answering",
      "cross-modal retrieval",
      "multimodal summarization",
      "M3Retrieve",
      "Multimodal Medical Retrieval Benchmark"
    ]
  },
  {
    "title": "Code Agent can be an End-to-end System Hacker: Benchmarking Real-world\n  Threats of Computer-use Agent",
    "summary": "Computer-use agent (CUA) frameworks, powered by large language models (LLMs) or multimodal LLMs (MLLMs), are rapidly maturing as assistants that can perceive context, reason, and act directly within software environments. Among their most critical applications is operating system (OS) control. As CUAs in the OS domain become increasingly embedded in daily operations, it is imperative to examine their real-world security implications, specifically whether CUAs can be misused to perform realistic, security-relevant attacks. Existing works exhibit four major limitations: Missing attacker-knowledge model on tactics, techniques, and procedures (TTP), Incomplete coverage for end-to-end kill chains, unrealistic environment without multi-host and encrypted user credentials, and unreliable judgment dependent on LLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark aligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises 140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks, and 26 end-to-end kill chains, systematically evaluates CUAs under a realistic enterprise OS security threat in a multi-host environment sandbox by hard-coded evaluation. We evaluate the existing five mainstream CUAs, including ReAct, AutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The results demonstrate that current frontier CUAs do not adequately cover OS security-centric threats. These capabilities of CUAs reduce dependence on custom malware and deep domain expertise, enabling even inexperienced attackers to mount complex enterprise intrusions, which raises social concern about the responsibility and security of CUAs.",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Bin Hu",
      "Hung-Chun Chiu",
      "Siyuan Ma",
      "Yizhe Zhang",
      "Xusheng Xiao",
      "Yinzhi Cao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "large language models",
      "multimodal LLMs",
      "computer-use agents",
      "operating system control",
      "security implications",
      "tactics",
      "techniques",
      "procedures",
      "MITRE ATT&CK Enterprise Matrix",
      "end-to-end kill chains",
      "multi-host environment",
      "enterprise OS security",
      "ReAct",
      "AutoGPT",
      "Gemini CLI",
      "Cursor CLI",
      "Cursor IDE"
    ]
  },
  {
    "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning\n  with Puzzles",
    "summary": "This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.",
    "authors": [
      "Yitao Long",
      "Yuru Jiang",
      "Hongjun Liu",
      "Yilun Zhao",
      "Jingchen Sun",
      "Yiqiu Shen",
      "Chen Zhao",
      "Arman Cohan",
      "Dennis Shasha"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "PuzzlePlex",
      "foundation models",
      "reasoning capabilities",
      "planning capabilities",
      "scalability",
      "deterministic games",
      "stochastic games",
      "single-player scenarios",
      "two-player scenarios",
      "game-playing strategies",
      "fine-grained metrics",
      "instruction-based settings",
      "code-based execution"
    ]
  },
  {
    "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial\n  Long-Form Question Answering",
    "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.",
    "authors": [
      "Yitao Long",
      "Tiansheng Hu",
      "Yilun Zhao",
      "Arman Cohan",
      "Chen Zhao"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "hallucination",
      "attribution",
      "FinLFQA",
      "supporting evidence",
      "intermediate numerical reasoning",
      "domain-specific financial knowledge",
      "end-to-end generation",
      "post-hoc approaches",
      "iterative refinement"
    ]
  },
  {
    "title": "A Single Character can Make or Break Your LLM Evals",
    "summary": "Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.",
    "authors": [
      "Jingtong Su",
      "Jianyu Zhang",
      "Karen Ullrich",
      "L\u00e9on Bottou",
      "Mark Ibrahim"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Large Language Model (LLM)",
      "MMLU",
      "attention head scores",
      "delimiter",
      "robustness",
      "prompt"
    ]
  },
  {
    "title": "Glocal Information Bottleneck for Time Series Imputation",
    "summary": "Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
    "authors": [
      "Jie Yang",
      "Kexin Zhang",
      "Guibin Zhang",
      "Philip S. Yu",
      "Kaize Ding"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Time Series Imputation",
      "Glocal Information Bottleneck",
      "Global Alignment loss",
      "mutual information approximation",
      "latent representations",
      "overfitting",
      "local noise",
      "generalization"
    ]
  },
  {
    "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization\n  Modeling",
    "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional non-reflective datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose CALM (Corrective Adaptation with Lightweight Modification), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop STORM (Smart Thinking Optimization Reasoning Model), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
    "authors": [
      "Zhengyang Tang",
      "Zihan Ye",
      "Chenyu Huang",
      "Xuhan Huang",
      "Chengpeng Li",
      "Sihang Li",
      "Guanhua Chen",
      "Ming Yan",
      "Zizhuo Wang",
      "Hongyuan Zha",
      "Dayiheng Liu",
      "Benyou Wang"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "Large Reasoning Models",
      "domain adaptation",
      "fine-tuning",
      "non-reflective datasets",
      "CALM",
      "corrective adaptation",
      "lightweight modification",
      "reasoning flaws",
      "corrective hints",
      "reasoning trajectories",
      "soft adaptation",
      "supervised fine-tuning",
      "reinforcement learning",
      "STORM",
      "Smart Thinking Optimization Reasoning Model"
    ]
  },
  {
    "title": "Pushing on Multilingual Reasoning Models with Language-Mixed\n  Chain-of-Thought",
    "summary": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
    "authors": [
      "Guijin Son",
      "Donghun Yang",
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Hyunwoo Ko",
      "Chanuk Lim",
      "Srikant Panda",
      "Minhyuk Kim",
      "Nikunj Drolia",
      "Dasol Choi",
      "Kyong-Ha Lee",
      "Youngjae Yu"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "chain-of-thought reasoning",
      "Language-Mixed CoT",
      "monolingual CoT",
      "Yi-Sang",
      "Qwen3-32B",
      "KO-REAson-35B",
      "cross-lingual",
      "mult-modal"
    ]
  },
  {
    "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces",
    "summary": "The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
    "authors": [
      "Minju Gwak",
      "Guijin Son",
      "Jaehyung Kim"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Uniform Information Density (UID)",
      "large language model (LLM)",
      "entropy-based stepwise information density",
      "local uniformity scores",
      "global uniformity scores",
      "reasoning quality",
      "information density spikes",
      "information bursts"
    ]
  },
  {
    "title": "The Markovian Thinker",
    "summary": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
    "authors": [
      "Milad Aghajohari",
      "Kamran Chitsaz",
      "Amirhossein Kazemnejad",
      "Sarath Chandar",
      "Alessandro Sordoni",
      "Aaron Courville",
      "Siva Reddy"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Reinforcement learning",
      "reasoning LLMs",
      "LongCoT",
      "Markovian Thinking",
      "attention-based policies",
      "Delethink",
      "fixed-size chunks",
      "R1-Distill",
      "test-time scaling",
      "RL initialization",
      "reasoning models"
    ]
  },
  {
    "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
    "summary": "Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
    "authors": [
      "Yansong Ning",
      "Rui Liu",
      "Jun Wang",
      "Kai Chen",
      "Wei Li",
      "Jun Fang",
      "Kan Zheng",
      "Naiqiang Tan",
      "Hao Liu"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "reinforcement learning",
      "agentic reinforcement learning",
      "hierarchical reward modeling",
      "trajectory level verifier",
      "turn level verifier",
      "reply augmented reinforcement learning",
      "travel planning agent",
      "sandbox environment",
      "spatiotemporal feasibility",
      "itinerary detail consistency",
      "LLMs",
      "Qwen3 32B",
      "OpenAI o1",
      "o3",
      "DeepSeek R1"
    ]
  },
  {
    "title": "Native Hybrid Attention for Efficient Sequence Modeling",
    "summary": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single softmax attention operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.",
    "authors": [
      "Jusen Du",
      "Jiaxi Hu",
      "Tao Zhang",
      "Weigao Sun",
      "Yu Cheng"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Native Hybrid Attention",
      "NHA",
      "linear attention",
      "full attention",
      "intra-layer hybridization",
      "inter-layer hybridization",
      "key-value slots",
      "linear RNN",
      "sliding window",
      "softmax attention",
      "recall-intensive tasks",
      "commonsense reasoning tasks",
      "pretrained LLMs"
    ]
  },
  {
    "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
    "summary": "Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.",
    "authors": [
      "Ziyuan Huang",
      "DanDan Zheng",
      "Cheng Zou",
      "Rui Liu",
      "Xiaolong Wang",
      "Kaixiang Ji",
      "Weilong Chai",
      "Jianxin Sun",
      "Libin Wang",
      "Yongjie Lv",
      "Taozhi Huang",
      "Jiajia Liu",
      "Qingpei Guo",
      "Ming Yang",
      "Jingdong Chen",
      "Jun Zhou"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "visual tokenization",
      "autoregressive paradigm",
      "discrete latent spaces",
      "large language models",
      "quantization errors",
      "continuous latent space",
      "three-stage sequential architecture",
      "low-level encoding",
      "semantic expansion",
      "visual reconstruction",
      "Ming-UniVision",
      "task-specific visual representations",
      "next-token prediction",
      "iterative understanding",
      "generation and editing"
    ]
  },
  {
    "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "summary": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.",
    "authors": [
      "Hongzhi Zang",
      "Mingjie Wei",
      "Si Xu",
      "Yongji Wu",
      "Zhen Guo",
      "Yuanqing Wang",
      "Hao Lin",
      "Liangzhi Shi",
      "Yuqing Xie",
      "Zhexuan Xu",
      "Zhihao Liu",
      "Kang Chen",
      "Wenhao Tang",
      "Quanlu Zhang",
      "Weinan Zhang",
      "Chao Yu",
      "Yu Wang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "vision-language-action models",
      "supervised fine-tuning",
      "reinforcement learning",
      "RLinf-VLA",
      "resource allocation",
      "GPU-parallelized simulators",
      "hybrid fine-grained pipeline allocation",
      "OpenVLA",
      "OpenVLA-OFT",
      "PPO",
      "GRPO",
      "ManiSkill",
      "LIBERO",
      "Franka robot"
    ]
  },
  {
    "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
    "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
    "authors": [
      "Yongyi Su",
      "Haojie Zhang",
      "Shijie Li",
      "Nanqing Liu",
      "Jingyi Liao",
      "Junyi Pan",
      "Yuan Liu",
      "Xiaofen Xing",
      "Chong Sun",
      "Chen Li",
      "Nancy F. Chen",
      "Shuicheng Yan",
      "Xulei Yang",
      "Xun Xu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Multimodal large language models",
      "Patch-as-Decodable Token",
      "PaDT",
      "Visual Reference Tokens",
      "VRTs",
      "visual patch embeddings",
      "lightweight decoder",
      "detection",
      "segmentation",
      "grounding predictions",
      "supervised fine-tuning",
      "per-token cross-entropy loss"
    ]
  },
  {
    "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
    "summary": "Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.",
    "authors": [
      "Sheriff Issaka",
      "Keyi Wang",
      "Yinka Ajibola",
      "Oluwatumininu Samuel-Ipaye",
      "Zhaoyi Zhang",
      "Nicte Aguillon Jimenez",
      "Evans Kofi Agyei",
      "Abraham Lin",
      "Rohan Ramachandran",
      "Sadick Abdul Mumin",
      "Faith Nchifor",
      "Mohammed Shuraim",
      "Lieqi Liu",
      "Erick Rosas Gonzalez",
      "Sylvester Kpei",
      "Jemimah Osei",
      "Carlene Ajeneza",
      "Persis Boateng",
      "Prisca Adwoa Dufie Yeboah",
      "Saadia Gabriel"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "multi-modal speech and text dataset",
      "fine-tuning",
      "ChrF++",
      "COMET",
      "BLEU",
      "Google Translate"
    ]
  },
  {
    "title": "NorMuon: Making Muon more efficient and scalable",
    "summary": "The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.",
    "authors": [
      "Zichong Li",
      "Liming Liu",
      "Chen Liang",
      "Weizhu Chen",
      "Tuo Zhao"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Muon",
      "orthogonalization",
      "optimization geometry",
      "conditioning",
      "neuron-level adaptive learning rates",
      "second-order momentum statistics",
      "row-wise normalization",
      "FSDP2",
      "training efficiency",
      "memory footprint"
    ]
  },
  {
    "title": "Heptapod: Language Modeling on Visual Signals",
    "summary": "We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.",
    "authors": [
      "Yongxin Zhu",
      "Jiawei Chen",
      "Yuanzhe Chen",
      "Zhuo Chen",
      "Dongya Jia",
      "Jian Cong",
      "Xiaobin Zhuang",
      "Yuping Wang",
      "Yuxuan Wang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "causal attention",
      "CFG",
      "semantic tokenizers",
      "next 2D distribution prediction",
      "causal Transformer",
      "visual tokenizer",
      "masked autoencoding",
      "FID"
    ]
  },
  {
    "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
    "summary": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
    "authors": [
      "Mingyu Liu",
      "Jiuhe Shu",
      "Hui Chen",
      "Zeju Li",
      "Canyu Zhao",
      "Jiange Yang",
      "Shenyuan Gao",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Diffusion Transformer",
      "DiT",
      "VLA-based models",
      "LIBERO",
      "latent interpolation",
      "latent action",
      "StaMo",
      "policy co-training",
      "human egocentric video"
    ]
  },
  {
    "title": "Online Generic Event Boundary Detection",
    "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets.",
    "authors": [
      "Hyungrok Jung",
      "Daneul Kim",
      "Seunggyun Lim",
      "Jeany Son",
      "Jonghyun Choi"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Online Generic Event Boundary Detection",
      "On-GEBD",
      "Event Segmentation Theory",
      "Consistent Event Anticipator",
      "Online Boundary Discriminator",
      "prediction error",
      "statistical tests",
      "Kinetics-GEBD",
      "TAPOS datasets"
    ]
  },
  {
    "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
    "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench.",
    "authors": [
      "Chenfei Liao",
      "Wensong Wang",
      "Zichen Wen",
      "Xu Zheng",
      "Yiyu Wang",
      "Haocong He",
      "Yuanhuiyi Lyu",
      "Lutao Jiang",
      "Xin Zou",
      "Yuqian Fu",
      "Bin Ren",
      "Linfeng Zhang",
      "Xuming Hu"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "visual token compression",
      "image downsampling",
      "VTC-Bench",
      "data filtering mechanism"
    ]
  },
  {
    "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
    "summary": "Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing unique_references studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
    "authors": [
      "Rajvee Sheth",
      "Samridhi Raj Sinha",
      "Mahavir Patil",
      "Himanshu Beniwal",
      "Mayank Singh"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "code-switching",
      "multilingual NLP",
      "large language models",
      "mixed-language inputs",
      "CSW datasets",
      "evaluation biases",
      "CSW-aware LLM research",
      "architecture",
      "training strategy",
      "evaluation methodology",
      "inclusive datasets",
      "fair evaluation",
      "linguistically grounded models"
    ]
  },
  {
    "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
    "summary": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO (G^2RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our G^2RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.",
    "authors": [
      "Yujie Zhou",
      "Pengyang Ling",
      "Jiazi Bu",
      "Yibin Wang",
      "Yuhang Zang",
      "Jiaqi Wang",
      "Li Niu",
      "Guangtao Zhai"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "online reinforcement learning",
      "diffusion models",
      "flow models",
      "Stochastic Differential Equations",
      "SDE",
      "denoising process",
      "Singular Stochastic Sampling",
      "Multi-Granularity Advantage Integration",
      "reward signals",
      "reward assessment",
      "sampling directions",
      "in-domain evaluations",
      "out-of-domain evaluations",
      "GRPO"
    ]
  },
  {
    "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
    "summary": "The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.",
    "authors": [
      "Xunyi Jiang",
      "Dingyi Chang",
      "Julian McAuley",
      "Xin Xu"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "factuality benchmarks",
      "fact retrieval pipeline",
      "benchmark aging",
      "LLM factuality evaluation"
    ]
  },
  {
    "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
    "authors": [
      "Junhan Zhu",
      "Hesong Wang",
      "Mingluo Su",
      "Zefang Wang",
      "Huan Wang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "diffusion models",
      "one-shot pruning",
      "OBS-Diff",
      "Optimal Brain Surgeon",
      "unstructured",
      "N:M semi-structured",
      "structured",
      "MHA heads",
      "FFN neurons",
      "timestep-aware Hessian",
      "logarithmic-decrease weighting scheme",
      "group-wise sequential pruning"
    ]
  },
  {
    "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
    "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
    "authors": [
      "Zhanke Zhou",
      "Chentao Cao",
      "Xiao Feng",
      "Xuan Li",
      "Zongze Li",
      "Xiangyu Lu",
      "Jiangchao Yao",
      "Weikai Huang",
      "Linrui Xu",
      "Tian Cheng",
      "Guanyu Jiang",
      "Yiming Zheng",
      "Brando Miranda",
      "Tongliang Liu",
      "Sanmi Koyejo",
      "Masashi Sugiyama",
      "Bo Han"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "self-evolving",
      "agentic reasoning system",
      "foundation model",
      "model-intrinsic capacity",
      "test-time iteration",
      "computation tool",
      "retrieval tool",
      "shared state map",
      "iterative refinement",
      "AIME",
      "Qwen2.5-14B-Instruct",
      "Llama-3.3-70B-Instruct",
      "tool-use analysis"
    ]
  },
  {
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
    "summary": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
    "authors": [
      "Zezhong Qian",
      "Xiaowei Chi",
      "Yuming Li",
      "Shizun Wang",
      "Zhiyuan Qin",
      "Xiaozhu Ju",
      "Sirui Han",
      "Shanghang Zhang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "VGGT",
      "Spatial Projection Consistency (SPC) Loss",
      "4D world model",
      "wrist-view videos",
      "anchor views",
      "geometrically consistent wrist-view poses",
      "4D point clouds",
      "temporally coherent wrist-view videos",
      "Droid",
      "Calvin",
      "Franka Panda",
      "task completion length",
      "anchor-wrist view gap"
    ]
  },
  {
    "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
    "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
    "authors": [
      "Yi Xin",
      "Qi Qin",
      "Siqi Luo",
      "Kaiwen Zhu",
      "Juncheng Yan",
      "Yan Tai",
      "Jiayi Lei",
      "Yuewen Cao",
      "Keqi Wang",
      "Yibin Wang",
      "Jinbin Bai",
      "Qian Yu",
      "Dengyang Jiang",
      "Yuandong Pu",
      "Haoxing Chen",
      "Le Zhuo",
      "Junjun He",
      "Gen Luo",
      "Tianbin Li",
      "Ming Hu",
      "Jin Ye",
      "Shenglong Ye",
      "Bo Zhang",
      "Chang Xu",
      "Wenhai Wang",
      "Hongsheng Li",
      "Guangtao Zhai",
      "Tianfan Xue",
      "Bin Fu",
      "Xiaohong Liu",
      "Yu Qiao",
      "Yihao Liu"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "discrete diffusion modeling",
      "autoregressive",
      "hybrid AR-Diffusion",
      "text-to-image generation",
      "image-to-image generation",
      "image editing",
      "subject-driven generation",
      "image inpainting",
      "image understanding"
    ]
  },
  {
    "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
    "summary": "While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. We introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. We apply MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality.",
    "authors": [
      "Rushi Qiang",
      "Yuchen Zhuang",
      "Anikait Singh",
      "Percy Liang",
      "Chao Zhang",
      "Sherry Yang",
      "Bo Dai"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Language Models",
      "machine learning engineering",
      "MLE",
      "MLE-Smith",
      "multi-agent pipeline",
      "generate-verify-execute",
      "task design",
      "standardized refactoring",
      "hybrid verification",
      "structural rules",
      "semantic soundness",
      "empirical solvability",
      "real-world fidelity",
      "interactive execution"
    ]
  },
  {
    "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
    "summary": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.",
    "authors": [
      "Akshit Singh",
      "Shyam Marjit",
      "Wei Lin",
      "Paul Gavrikov",
      "Serena Yeung-Levy",
      "Hilde Kuehne",
      "Rogerio Feris",
      "Sivan Doveh",
      "James Glass",
      "M. Jehanzeb Mirza"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Reinforcement Learning",
      "Group Relative Policy Optimization (GRPO)",
      "entropy",
      "test-time reinforcement learning",
      "vision language models (VLMs)",
      "InternVL 8B",
      "GPT-4o"
    ]
  },
  {
    "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
    "summary": "Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench.",
    "authors": [
      "Fenghe Tang",
      "Chengqi Dong",
      "Wenxin Ma",
      "Zikang Xu",
      "Heqin Zhu",
      "Zihang Jiang",
      "Rongsheng Wang",
      "Yuhao Wang",
      "Chenxu Wu",
      "Shaohua Kevin Zhou"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "U-Net",
      "U-Bench",
      "U-Score",
      "zero-shot generalization",
      "computational efficiency",
      "statistical robustness",
      "model advisor agent"
    ]
  },
  {
    "title": "Bridging Text and Video Generation: A Survey",
    "summary": "Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
    "authors": [
      "Nilay Kumar",
      "Priyansh Bhandari",
      "G. Maragatham"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "adversarial models",
      "diffusion-based models",
      "GANs",
      "VAEs",
      "Diffusion-Transformer (DiT) architectures",
      "datasets",
      "training configurations",
      "evaluation metrics",
      "perception-aligned evaluation strategies"
    ]
  },
  {
    "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
    "summary": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
    "authors": [
      "Siyoon Jin",
      "Seongchan Kim",
      "Dahyun Chung",
      "Jaeho Lee",
      "Hyunwook Choi",
      "Jisu Nam",
      "Jiyoung Kim",
      "Seungryong Kim"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "video DiTs",
      "interaction-aware captions",
      "multi-instance mask tracks",
      "video-to-text attention",
      "video-to-video attention",
      "semantic grounding",
      "semantic propagation",
      "MATRIX regularization",
      "InterGenEval",
      "interaction fidelity",
      "semantic alignment",
      "drift",
      "hallucination"
    ]
  },
  {
    "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
    "summary": "Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
    "authors": [
      "Zecheng Tang",
      "Baibei Ji",
      "Juntao Li",
      "Lijun Wu",
      "Haijia Gui",
      "Min Zhang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "long-context models",
      "contextual noise",
      "Integrated Gradient (IG) score",
      "Context Denoising Training (CDT)",
      "context window scaling",
      "long-context alignment"
    ]
  },
  {
    "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
    "summary": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D^3QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D^3QE across different AR models, with robustness to real-world perturbations. Code is available at https://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
    "authors": [
      "Yanran Zhang",
      "Bingyao Yu",
      "Yu Zheng",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Lei Chen",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "visual autoregressive (AR) models",
      "image generation",
      "synthetic image detection",
      "discrete token prediction",
      "vector-quantized representations",
      "Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE)",
      "discrete distribution discrepancy-aware transformer",
      "attention mechanism",
      "semantic features",
      "quantization error latent",
      "ARForensics dataset"
    ]
  },
  {
    "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
    "summary": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
    "authors": [
      "Haiquan Qiu",
      "Quanming Yao"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "low-precision formats",
      "transformer models",
      "flash attention",
      "training instabilities",
      "low-rank representations",
      "biased rounding errors",
      "error accumulation",
      "weight updates",
      "training dynamics"
    ]
  },
  {
    "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
    "summary": "Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.",
    "authors": [
      "Tianyu Fu",
      "Zihan Min",
      "Hanling Zhang",
      "Jichao Yan",
      "Guohao Dai",
      "Wanli Ouyang",
      "Yu Wang"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Large Language Models",
      "KV-Cache",
      "Cache-to-Cache",
      "neural network",
      "semantic communication",
      "gating mechanism",
      "accuracy",
      "latency"
    ]
  },
  {
    "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
    "authors": [
      "Ming Zhong",
      "Xiang Zhou",
      "Ting-Yun Chang",
      "Qingze Wang",
      "Nan Xu",
      "Xiance Si",
      "Dan Garrette",
      "Shyam Upadhyay",
      "Jeremiah Liu",
      "Jiawei Han",
      "Benoit Schillings",
      "Jiao Sun"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "vibe coding",
      "Vibe Checker",
      "VeriCode",
      "verifiable code instructions",
      "deterministic verifiers",
      "instruction following",
      "functional correctness",
      "human preference",
      "coding preferences"
    ]
  },
  {
    "title": "Multi-Agent Tool-Integrated Policy Optimization",
    "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
    "authors": [
      "Zhanfeng Mo",
      "Xingxuan Li",
      "Yuntao Chen",
      "Lidong Bing"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "multi-turn tool-integrated planning",
      "multi-agent framework",
      "planner-agents",
      "worker-agents",
      "reinforcement learning",
      "role-specific prompts",
      "credit assignment mechanism",
      "planner and worker rollouts",
      "multi-agent RL training"
    ]
  },
  {
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
    "authors": [
      "Yunhao Fang",
      "Weihao Yu",
      "Shu Zhong",
      "Qinghao Ye",
      "Xuehan Xiong",
      "Lai Wei"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "RNN-like models",
      "attention-based Transformers",
      "Multi-Store Model",
      "sliding window",
      "KV cache",
      "Artificial Hippocampus Network (AHN)",
      "Mamba2",
      "DeltaNet",
      "Gated DeltaNet",
      "LV-Eval",
      "InfiniteBench",
      "inference FLOPs",
      "memory cache"
    ]
  },
  {
    "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
    "authors": [
      "Cheng-Han Chiang",
      "Xiaofei Wang",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Shujie Liu",
      "Zhendong Wang",
      "Zhengyuan Yang",
      "Hung-yi Lee",
      "Lijuan Wang"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "SHANKS",
      "spoken language models",
      "unspoken chain-of-thought reasoning",
      "real-time interaction",
      "tool calls",
      "interruption accuracy"
    ]
  },
  {
    "title": "SDQM: Synthetic Data Quality Metric for Object Detection Dataset\n  Evaluation",
    "summary": "The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at https://github.com/ayushzenith/SDQM",
    "authors": [
      "Ayush Zenith",
      "Arnold Zumbrun",
      "Neel Raut",
      "Jing Lin"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "synthetic data",
      "generative models",
      "object detection",
      "Synthetic Dataset Quality Metric",
      "mean Average Precision",
      "YOLOv11"
    ]
  },
  {
    "title": "BACHI: Boundary-Aware Symbolic Chord Recognition Through Masked\n  Iterative Decoding on Pop and Classical Music",
    "summary": "Automatic chord recognition (ACR) via deep learning models has gradually achieved promising recognition accuracy, yet two key challenges remain. First, prior work has primarily focused on audio-domain ACR, while symbolic music (e.g., score) ACR has received limited attention due to data scarcity. Second, existing methods still overlook strategies that are aligned with human music analytical practices. To address these challenges, we make two contributions: (1) we introduce POP909-CL, an enhanced version of POP909 dataset with tempo-aligned content and human-corrected labels of chords, beats, keys, and time signatures; and (2) We propose BACHI, a symbolic chord recognition model that decomposes the task into different decision steps, namely boundary detection and iterative ranking of chord root, quality, and bass (inversion). This mechanism mirrors the human ear-training practices. Experiments demonstrate that BACHI achieves state-of-the-art chord recognition performance on both classical and pop music benchmarks, with ablation studies validating the effectiveness of each module.",
    "authors": [
      "Mingyang Yao",
      "Ke Chen",
      "Shlomo Dubnov",
      "Taylor Berg-Kirkpatrick"
    ],
    "date": "2025-10-08",
    "ai_keywords": [
      "automatic chord recognition",
      "deep learning models",
      "symbolic music",
      "POP909-CL dataset",
      "BACHI model",
      "boundary detection",
      "iterative ranking",
      "chord root",
      "chord quality",
      "bass inversion",
      "human ear-training practices"
    ]
  },
  {
    "title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair\n  Manipulation",
    "summary": "Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the fine-grained physical structure and complex dynamics of hair. In this work, we present DYMO-Hair, a model-based robot hair care system. We introduce a novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with a compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using a novel hair physics simulator, enabling generalization across previously unseen hairstyles. Using the dynamics model with a Model Predictive Path Integral (MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system. Real-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. Together, these results introduce a foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments. More details are available on our project page: https://chengyzhao.github.io/DYMOHair-web/.",
    "authors": [
      "Chengyang Zhao",
      "Uksang Yoo",
      "Arkadeep Narayan Chaudhury",
      "Giljoo Nam",
      "Jonathan Francis",
      "Jeffrey Ichnowski",
      "Jean Oh"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "dynamics learning paradigm",
      "action-conditioned latent state editing",
      "3D latent space",
      "hair physics simulator",
      "Model Predictive Path Integral (MPPI) planner",
      "visual goal-conditioned hair styling",
      "local deformation",
      "closed-loop hair styling tasks",
      "zero-shot transferability"
    ]
  },
  {
    "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in\n  Multi-Platform Dynamic Agent Environments",
    "summary": "Recent works on context and memory benchmarking have primarily focused on conversational instances but the need for evaluating memory in dynamic enterprise environments is crucial for its effective application. We introduce MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking in multi-platform agent environments. MEMTRACK models realistic organizational workflows by integrating asynchronous events across multiple communication and productivity platforms such as Slack, Linear and Git. Each benchmark instance provides a chronologically platform-interleaved timeline, with noisy, conflicting, cross-referring information as well as potential codebase/file-system comprehension and exploration. Consequently, our benchmark tests memory capabilities such as acquistion, selection and conflict resolution. We curate the MEMTRACK dataset through both manual expert driven design and scalable agent based synthesis, generating ecologically valid scenarios grounded in real world software development processes. We introduce pertinent metrics for Correctness, Efficiency, and Redundancy that capture the effectiveness of memory mechanisms beyond simple QA performance. Experiments across SoTA LLMs and memory backends reveal challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. Notably, the best performing GPT-5 model only achieves a 60\\% Correctness score on MEMTRACK. This work provides an extensible framework for advancing evaluation research for memory-augmented agents, beyond existing focus on conversational setups, and sets the stage for multi-agent, multi-platform memory benchmarking in complex organizational settings",
    "authors": [
      "Darshan Deshpande",
      "Varun Gangal",
      "Hersh Mehta",
      "Anand Kannappan",
      "Rebecca Qian",
      "Peng Wang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "MEMTRACK",
      "long-term memory",
      "state tracking",
      "multi-platform agent environments",
      "asynchronous events",
      "Slack",
      "Linear",
      "Git",
      "chronological timeline",
      "noisy information",
      "conflicting information",
      "cross-referring information",
      "codebase comprehension",
      "file-system exploration",
      "memory acquisition",
      "memory selection",
      "conflict resolution",
      "MEMTRACK dataset",
      "Correctness",
      "Efficiency",
      "Redundancy",
      "SoTA LLMs",
      "memory backends",
      "GPT-5",
      "multi-agent",
      "multi-platform memory benchmarking"
    ]
  },
  {
    "title": "Scalable In-context Ranking with Generative Models",
    "summary": "In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.",
    "authors": [
      "Nilesh Gupta",
      "Chong You",
      "Srinadh Bhojanapalli",
      "Sanjiv Kumar",
      "Inderjit Dhillon",
      "Felix Yu"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "in-context ranking",
      "ICR",
      "LLMs",
      "attention operation",
      "inter-document block sparsity",
      "query-document block relevance",
      "BlockRank",
      "BEIR",
      "MSMarco",
      "NQ",
      "Mistral-7B",
      "FLARE Mistral",
      "listwise rankers",
      "contrastive training objective"
    ]
  },
  {
    "title": "Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement\n  Learning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.",
    "authors": [
      "Chenghao Yang",
      "Lin Gui",
      "Chenxiao Yang",
      "Victor Veitch",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "reinforcement learning with verifiable rewards",
      "large language models",
      "exploration strategy",
      "sample quality",
      "training stability",
      "fixed-temperature sampling",
      "exploratory annealed decoding",
      "explore-at-the-beginning",
      "exploit-at-the-end",
      "sampling temperature",
      "target policy",
      "sample efficiency",
      "sequential generation"
    ]
  },
  {
    "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in\n  Complex Chart Question Answering",
    "summary": "Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.",
    "authors": [
      "Rachneet Kaur",
      "Nishan Srishankar",
      "Zhen Zeng",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "multimodal LLMs",
      "visual question answering",
      "unannotated charts",
      "ChartAgent",
      "agentic framework",
      "visual reasoning",
      "spatial domain",
      "chain-of-thought reasoning",
      "visual subtasks",
      "drawing annotations",
      "cropping regions",
      "segmenting pie slices",
      "isolating bars",
      "localizing axes",
      "chart-specific vision tools",
      "ChartBench",
      "ChartX",
      "diverse chart types",
      "visual complexity",
      "reasoning complexity",
      "plug-and-play framework"
    ]
  },
  {
    "title": "GRACE: Generative Representation Learning via Contrastive Policy\n  Optimization",
    "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
    "authors": [
      "Jiashuo Sun",
      "Shixuan Liu",
      "Zhaochen Su",
      "Xianrui Zhong",
      "Pengcheng Jiang",
      "Bowen Jin",
      "Peiran Li",
      "Weijia Shi",
      "Jiawei Han"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "contrastive losses",
      "generative representation learning",
      "contrastive policy optimization",
      "generative policy",
      "human-interpretable rationales",
      "mean pooling",
      "policy gradient optimization",
      "multi-component reward function",
      "MTEB benchmark",
      "supervised setting",
      "unsupervised variant",
      "representation learning",
      "generation"
    ]
  },
  {
    "title": "Adaptive Pruning for Increased Robustness and Reduced Computational\n  Overhead in Gaussian Process Accelerated Saddle Point Searches",
    "summary": "Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.",
    "authors": [
      "Rohit Goswami",
      "Hannes J\u00f3nsson"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Gaussian process regression",
      "saddle point searches",
      "energy surfaces",
      "hyperparameter optimization",
      "geometry-aware optimal transport",
      "active pruning",
      "Wasserstein-1 distances",
      "farthest-point sampling",
      "permutation-invariant metric",
      "trust radius",
      "logarithmic barrier penalty",
      "signal variance"
    ]
  },
  {
    "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research",
    "summary": "Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.",
    "authors": [
      "Gang Liu",
      "Yihan Zhu",
      "Jie Chen",
      "Meng Jiang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "large language models",
      "algorithm evolution",
      "deep research",
      "external knowledge retrieval",
      "cross-file code editing",
      "systematic debugging",
      "feedback-driven iterative loop",
      "executable new algorithms",
      "scientific algorithm discovery"
    ]
  },
  {
    "title": "On Code-Induced Reasoning in LLMs",
    "summary": "Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.",
    "authors": [
      "Abdul Waheed",
      "Zhen Wu",
      "Carolyn Ros\u00e9",
      "Daphne Ippolito"
    ],
    "date": "2025-09-25",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "instruction datasets",
      "programming languages",
      "controlled perturbations",
      "structural perturbations",
      "semantic perturbations",
      "pseudocode",
      "flowcharts",
      "token encoding",
      "syntactic styles",
      "natural language reasoning",
      "math reasoning"
    ]
  },
  {
    "title": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models",
    "summary": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a valley of code reasoning: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition",
    "authors": [
      "Muyu He",
      "Muhammad Ali Shafique",
      "Anand Kumar",
      "Tsach Mackey",
      "Nazneen Rajani"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Large Language Model (LLM)",
      "distillation",
      "coding skills",
      "valley of code reasoning",
      "learning phases",
      "data regimes",
      "coding questions"
    ]
  },
  {
    "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
    "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
    "authors": [
      "Suhyeok Jang",
      "Dongyoung Kim",
      "Changyeon Kim",
      "Youngsuk Kim",
      "Jinwoo Shin"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Vision-Language-Action models",
      "test-time scaling",
      "KL divergence",
      "reference action token distribution",
      "masked states",
      "language conditions",
      "dropout",
      "RoboCasa pick-and-place tasks"
    ]
  },
  {
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Hierarchical Reasoning Model",
      "HRM",
      "Tiny Recursive Model",
      "TRM",
      "recursive reasoning",
      "neural networks",
      "ARC-AGI",
      "Deepseek R1",
      "o3-mini",
      "Gemini 2.5 Pro"
    ]
  },
  {
    "title": "Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for\n  Academic Presentations",
    "summary": "The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: there is no way to improve it when you cannot evaluate it right. To address this, we introduce EvoPresent, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is PresAesth, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce EvoPresent Benchmark, a comprehensive benchmark comprising: Presentation Generation Quality, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and Aesthetic Awareness, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.",
    "authors": [
      "Chengzhi Liu",
      "Yuzhe Yang",
      "Kaiwen Zhou",
      "Zhen Zhang",
      "Yue Fan",
      "Yannan Xie",
      "Peng Qi",
      "Xin Eric Wang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "self-improvement agent framework",
      "PresAesth",
      "multi-task reinforcement learning",
      "aesthetic model",
      "aesthetic scoring",
      "defect adjustment",
      "comparative feedback",
      "EvoPresent Benchmark",
      "Presentation Generation Quality",
      "Aesthetic Awareness"
    ]
  },
  {
    "title": "Drax: Speech Recognition with Discrete Flow Matching",
    "summary": "Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR.",
    "authors": [
      "Aviv Navon",
      "Aviv Shamsian",
      "Neta Glazer",
      "Yael Segal-Feldman",
      "Gill Hetz",
      "Joseph Keshet",
      "Ethan Fetaya"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "diffusion models",
      "flow-based models",
      "non-autoregressive models",
      "discrete flow matching",
      "audio-conditioned probability path",
      "generalization gap",
      "divergences",
      "cumulative velocity errors",
      "ASR",
      "recognition accuracy",
      "accuracy-efficiency trade-offs"
    ]
  },
  {
    "title": "ShapeGen4D: Towards High Quality 4D Shape Generation from Videos",
    "summary": "Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. In this work, we introduce a native video-to-4D shape generation framework that synthesizes a single dynamic 3D representation end-to-end from the video. Our framework introduces three key components based on large-scale pre-trained 3D models: (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; (ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; and (iii) noise sharing across frames to enhance temporal stability. Our method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.",
    "authors": [
      "Jiraphon Yenphraphai",
      "Ashkan Mirzaei",
      "Jianqi Chen",
      "Jiaxu Zou",
      "Sergey Tulyakov",
      "Raymond A. Yeh",
      "Peter Wonka",
      "Chaoyang Wang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "temporal attention",
      "time-aware point sampling",
      "4D latent anchoring",
      "noise sharing",
      "non-rigid motion",
      "volume changes",
      "topological transitions",
      "dynamic 3D representation"
    ]
  },
  {
    "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool\n  Use",
    "summary": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.",
    "authors": [
      "Zhuofeng Li",
      "Haoxiang Zhang",
      "Seungju Han",
      "Sheng Liu",
      "Jianwen Xie",
      "Yu Zhang",
      "Yejin Choi",
      "James Zou",
      "Pan Lu"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "reinforcement learning",
      "large language models",
      "tool-augmented approaches",
      "monolithic policy",
      "agentic systems",
      "specialized modules",
      "trainable framework",
      "planner",
      "executor",
      "verifier",
      "generator",
      "evolving memory",
      "on-policy training",
      "multi-turn interaction",
      "Flow-based Group Refined Policy Optimization",
      "Flow-GRPO",
      "long-horizon",
      "sparse-reward credit assignment",
      "single-turn policy updates",
      "trajectory-level outcome",
      "group-normalized advantages",
      "search tasks",
      "agentic tasks",
      "mathematical tasks",
      "scientific tasks",
      "GPT-4o"
    ]
  },
  {
    "title": "Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI\n  Models for Scatterplot-Related Tasks",
    "summary": "AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.",
    "authors": [
      "Jo\u00e3o Palmeiro",
      "Diogo Duarte",
      "Rita Costa",
      "Pedro Bizarro"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "N-shot prompting",
      "cluster bounding boxes",
      "center coordinates",
      "outlier coordinates",
      "localization-related tasks",
      "Precision",
      "Recall",
      "aspect ratios"
    ]
  },
  {
    "title": "CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical\n  Contrastive Decoding",
    "summary": "Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology.",
    "authors": [
      "Xi Zhang",
      "Zaiqiao Meng",
      "Jake Lever",
      "Edmond S. L. Ho"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "multimodal large language models",
      "MLLMs",
      "radiology",
      "visual perception",
      "natural language understanding",
      "medical hallucinations",
      "prompt-induced hallucinations",
      "clinical sections",
      "Clinical Contrastive Cecoding",
      "CCD",
      "dual-stage contrastive mechanism",
      "token-level logits",
      "radiology report generation",
      "RRG",
      "MIMIC-CXR dataset",
      "RadGraph-F1"
    ]
  },
  {
    "title": "Training Dynamics Impact Post-Training Quantization Robustness",
    "summary": "While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.",
    "authors": [
      "Albert Catalan-Tatjer",
      "Niccol\u00f2 Ajroldi",
      "Jonas Geiping"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "post-training quantization",
      "large language models",
      "quantization degradation",
      "training trajectories",
      "quantization performance",
      "learning rate",
      "training hyperparameters",
      "validation loss",
      "quantization error",
      "dataset scale",
      "training dynamics",
      "quantization robustness",
      "strategic training hyperparameter interventions"
    ]
  },
  {
    "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit\n  Flows",
    "summary": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
    "authors": [
      "John Nguyen",
      "Marton Havasi",
      "Tariq Berrada",
      "Luke Zettlemoyer",
      "Ricky T. Q. Chen"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "non-autoregressive",
      "multimodal model",
      "variable-length",
      "concurrent mixed-modal generation",
      "insertion-based Edit Flow",
      "discrete text tokens",
      "Flow Matching",
      "image latents",
      "hierarchical sampling",
      "content over grammar",
      "training FLOPs",
      "autoregressive baselines",
      "diffusion-based approaches",
      "iterative refinement",
      "natural reasoning-like generation"
    ]
  },
  {
    "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
    "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256times256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
    "authors": [
      "Runqian Wang",
      "Yilun Du"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Equilibrium Matching",
      "EqM",
      "equilibrium dynamics",
      "diffusion models",
      "flow-based generative models",
      "implicit energy landscape",
      "gradient descent",
      "data manifold",
      "partially noised image denoising",
      "OOD detection",
      "image composition",
      "energy-based models"
    ]
  },
  {
    "title": "TensorBLEU: Vectorized GPU-based BLEU Score Implementation for\n  Per-Sentence In-Training Evaluation",
    "summary": "Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using torch.unique, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a \"Token-ID BLEU\" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning.",
    "authors": [
      "Adam Filipek"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "TensorBLEU",
      "BLEU metric",
      "GPU-accelerated",
      "per-sentence computation",
      "PyTorch",
      "memory-efficient",
      "n-grams",
      "torch.unique",
      "token-ID BLEU",
      "RL-based model fine-tuning"
    ]
  },
  {
    "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.",
    "authors": [
      "Nan Huo",
      "Xiaohan Xu",
      "Jinyang Li",
      "Per Jacobsson",
      "Shipei Lin",
      "Bowen Qin",
      "Binyuan Hui",
      "Xiaolong Li",
      "Ge Qu",
      "Shuzheng Si",
      "Linheng Han",
      "Edward Alexander",
      "Xintong Zhu",
      "Rui Qin",
      "Ruihan Yu",
      "Yiyao Jin",
      "Feige Zhou",
      "Weihao Zhong",
      "Yun Chen",
      "Hongyu Liu",
      "Chenhao Ma",
      "Fatma Ozcan",
      "Yannis Papakonstantinou",
      "Reynold Cheng"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "large language models",
      "text-to-SQL",
      "multi-turn interactions",
      "conversation histories",
      "hierarchical knowledge base",
      "metadata files",
      "user simulator",
      "CRUD",
      "business-intelligence",
      "operational use cases",
      "memory grafting",
      "Interaction Test-time Scaling"
    ]
  },
  {
    "title": "Revisiting Modeling and Evaluation Approaches in Speech Emotion\n  Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions",
    "summary": "Over the past two decades, speech emotion recognition (SER) has received growing attention. To train SER systems, researchers collect emotional speech databases annotated by crowdsourced or in-house raters who select emotions from predefined categories. However, disagreements among raters are common. Conventional methods treat these disagreements as noise, aggregating labels into a single consensus target. While this simplifies SER as a single-label task, it ignores the inherent subjectivity of human emotion perception. This dissertation challenges such assumptions and asks: (1) Should minority emotional ratings be discarded? (2) Should SER systems learn from only a few individuals' perceptions? (3) Should SER systems predict only one emotion per sample?   Psychological studies show that emotion perception is subjective and ambiguous, with overlapping emotional boundaries. We propose new modeling and evaluation perspectives: (1) Retain all emotional ratings and represent them with soft-label distributions. Models trained on individual annotator ratings and jointly optimized with standard SER systems improve performance on consensus-labeled tests. (2) Redefine SER evaluation by including all emotional data and allowing co-occurring emotions (e.g., sad and angry). We propose an ``all-inclusive rule'' that aggregates all ratings to maximize diversity in label representation. Experiments on four English emotion databases show superior performance over majority and plurality labeling. (3) Construct a penalization matrix to discourage unlikely emotion combinations during training. Integrating it into loss functions further improves performance. Overall, embracing minority ratings, multiple annotators, and multi-emotion predictions yields more robust and human-aligned SER systems.",
    "authors": [
      "Huang-Cheng Chou",
      "Chi-Chun Lee"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "speech emotion recognition",
      "emotional speech databases",
      "soft-label distributions",
      "co-occurring emotions",
      "all-inclusive rule",
      "penalization matrix",
      "loss functions"
    ]
  },
  {
    "title": "Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models",
    "summary": "Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose Distributional Semantics Tracing (DST), a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific commitment layer where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic associative pathway (akin to System 1) and a slow, deliberate contextual pathway (akin to System 2), leading to predictable failure modes such as Reasoning Shortcut Hijacks. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation (rho = -0.863) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.",
    "authors": [
      "Gagan Bhatia",
      "Somayajulu G Sripada",
      "Kevin Allan",
      "Jacobo Azcona"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Distributional Semantics Tracing",
      "commitment layer",
      "associative pathway",
      "contextual pathway",
      "dual-process theory",
      "Reasoning Shortcut Hijacks",
      "Transformer architecture"
    ]
  },
  {
    "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate\n  Hallucinations in Retrieval-Augmented Generation",
    "summary": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies document-claim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) a domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill large-model reasoning into a smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance.",
    "authors": [
      "Loris Bergeron",
      "Ioana Buhnila",
      "J\u00e9r\u00f4me Fran\u00e7ois",
      "Radu State"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Large Language Models",
      "HalluGuard",
      "Small Reasoning Model",
      "Retrieval-Augmented Generation",
      "document-claim pairs",
      "grounded",
      "hallucinated",
      "evidence-grounded justifications",
      "FineWeb",
      "multi-stage curation",
      "data reformation",
      "preference-based fine-tuning",
      "Odds Ratio Preference Optimization",
      "RAGTruth",
      "LLM-AggreFact benchmark",
      "MiniCheck",
      "Granite Guardian",
      "GPT-4o",
      "balanced accuracy"
    ]
  },
  {
    "title": "Human3R: Everyone Everywhere All at Once",
    "summary": "We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a single forward pass (\"all-at-once\"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R",
    "authors": [
      "Yue Chen",
      "Xingyu Chen",
      "Yuxuan Xue",
      "Anpei Chen",
      "Yuliang Xiu",
      "Gerard Pons-Moll"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "feed-forward framework",
      "4D reconstruction",
      "monocular videos",
      "SMPL-X bodies",
      "dense 3D scene",
      "camera trajectories",
      "CUT3R",
      "parameter-efficient visual prompt tuning",
      "spatiotemporal priors",
      "real-time speed",
      "memory footprint",
      "global human motion estimation",
      "local human mesh recovery",
      "video depth estimation",
      "camera pose estimation"
    ]
  },
  {
    "title": "MixReasoning: Switching Modes to Think",
    "summary": "Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.",
    "authors": [
      "Haiquan Lu",
      "Gongfan Fang",
      "Xinyin Ma",
      "Qi Li",
      "Xinchao Wang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "reasoning models",
      "step-by-step",
      "sub-problems",
      "long chains of thought",
      "adaptive response",
      "dynamic adjustment",
      "depth of reasoning",
      "chain of thought",
      "detailed reasoning",
      "concise inference",
      "GSM8K",
      "MATH-500",
      "AIME"
    ]
  },
  {
    "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and\n  Synthesis for SLMs",
    "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.",
    "authors": [
      "Shreyas Singh",
      "Kunal Singh",
      "Pradeep Moturi"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "DeepResearch Agents",
      "Fathom-DeepResearch",
      "Fathom-Search-4B",
      "DeepSearch",
      "Qwen3-4B",
      "DUETQA",
      "RAPO",
      "GRPO",
      "curriculum pruning",
      "reward-aware advantage scaling",
      "per-prompt replay buffers",
      "steerable step-level reward",
      "Fathom-Synthesizer-4B",
      "DeepResearch Reports",
      "SimpleQA",
      "FRAMES",
      "WebWalker",
      "Seal0",
      "MuSiQue",
      "DeepResearch-Bench",
      "HLE",
      "AIME-25",
      "GPQA-Diamond",
      "MedQA"
    ]
  },
  {
    "title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?",
    "summary": "Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.",
    "authors": [
      "Qingyu Yin",
      "Chak Tou Leong",
      "Linyi Yang",
      "Wenxuan Huang",
      "Wenjie Li",
      "Xiting Wang",
      "Jaehong Yoon",
      "YunXing",
      "XingYu",
      "Jinjin Gu"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "large reasoning models",
      "multi-step reasoning",
      "safety vulnerabilities",
      "mechanistic interpretability",
      "linear probing",
      "refusal cliff",
      "token positions",
      "refusal intentions",
      "causal intervention analysis",
      "attention heads",
      "Cliff-as-a-Judge",
      "data selection method",
      "safety alignment"
    ]
  },
  {
    "title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language\n  Models",
    "summary": "Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.",
    "authors": [
      "Min Woo Sun",
      "Alejandro Lozano",
      "Javier Gamazo Tejero",
      "Vishwesh Nath",
      "Xiao Xiao Sun",
      "James Burgess",
      "Yuhui Zhang",
      "Kun Yuan",
      "Robert Tibshirani",
      "Sean Huver",
      "Serena Yeung-Levy"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "embedding vision-language models",
      "VLMs",
      "text encoders",
      "biomedical captions",
      "context length",
      "token windows",
      "long-format captions",
      "BIOMEDICA-LongCAP",
      "BMC-LongCLIP",
      "long-context modeling",
      "Recall@1",
      "classification performance"
    ]
  },
  {
    "title": "ASPO: Asymmetric Importance Sampling Policy Optimization",
    "summary": "Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.",
    "authors": [
      "Jiakang Wang",
      "Runze Liu",
      "Lei Lin",
      "Wenping Hu",
      "Xiu Li",
      "Fuzheng Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Outcome-Supervised RL",
      "Importance Sampling",
      "Asymmetric Importance Sampling Policy Optimization",
      "soft dual-clipping mechanism",
      "token-level clipping",
      "low-probability tokens",
      "high-probability tokens",
      "premature convergence",
      "training stability",
      "GRPO-based baselines"
    ]
  },
  {
    "title": "CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support\n  Conversation",
    "summary": "Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.",
    "authors": [
      "Jie Zhu",
      "Yuanchen Zhou",
      "Shuo Jiang",
      "Junhui Li",
      "Lifan Guo",
      "Feng Chen",
      "Chi Zhang",
      "Fang Kong"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "CARE",
      "reinforcement learning",
      "cognitive reasoning",
      "emotional support conversations",
      "logical soundness",
      "supportive quality",
      "empathetic",
      "cognitively robust",
      "human-like"
    ]
  },
  {
    "title": "Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context",
    "summary": "A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent \"Ann loves pie\" by binding \"Ann\" to \"pie\", allowing it to later retrieve \"Ann\" when asked \"Who loves pie?\" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where \"Ann\" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving \"Ann\" using its bound counterpart \"pie\") and a reflexive mechanism (retrieving \"Ann\" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.",
    "authors": [
      "Yoav Gur-Arieh",
      "Mor Geva",
      "Atticus Geiger"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "in-context reasoning",
      "language models",
      "entity binding",
      "positional mechanism",
      "lexical mechanism",
      "reflexive mechanism",
      "causal model",
      "next token distributions"
    ]
  },
  {
    "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation",
    "summary": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.",
    "authors": [
      "Lesly Miculicich",
      "Mihir Parmar",
      "Hamid Palangi",
      "Krishnamurthy Dj Dvijotham",
      "Mirko Montanari",
      "Tomas Pfister",
      "Long T. Le"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "LLM-based agents",
      "formal safety guarantees",
      "dual-stage architecture",
      "offline stage",
      "user intent",
      "safety specifications",
      "behavioral policy",
      "formal verification",
      "online stage",
      "runtime monitor"
    ]
  },
  {
    "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation",
    "summary": "Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.",
    "authors": [
      "Jiawei Mao",
      "Yuhan Wang",
      "Lifeng Chen",
      "Can Zhao",
      "Yucheng Tang",
      "Dong Yang",
      "Liangqiong Qu",
      "Daguang Xu",
      "Yuyin Zhou"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "discrete diffusion model",
      "multimodal large language model",
      "bidirectional context",
      "continuous timestep embeddings",
      "FID",
      "METEOR",
      "BLEU"
    ]
  },
  {
    "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning",
    "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
    "authors": [
      "Honglin Lin",
      "Qizhi Pei",
      "Xin Gao",
      "Zhuoshi Pan",
      "Yu Li",
      "Juntao Li",
      "Conghui He",
      "Lijun Wu"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "Chain-of-Thought",
      "CoT prompting",
      "code-based CoT generator",
      "code-driven augmentation",
      "automated validation",
      "code execution",
      "rule-based filtering",
      "reverse-engineering",
      "natural language instructions",
      "language CoTs",
      "Caco-1.3M dataset",
      "mathematical reasoning benchmarks",
      "self-sustaining",
      "trustworthy reasoning systems"
    ]
  },
  {
    "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
    "summary": "Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .",
    "authors": [
      "Yang Xiao",
      "Gen Li",
      "Kaiyuan Deng",
      "Yushu Wu",
      "Zheng Zhan",
      "Yanzhi Wang",
      "Xiaolong Ma",
      "Bo Hui"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "diffusion models",
      "inference process",
      "encoding",
      "denoising",
      "decoding",
      "cache-based acceleration",
      "memory surges",
      "asynchronous cache swapping",
      "feature chunk",
      "slicing latents"
    ]
  },
  {
    "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
    "summary": "Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.",
    "authors": [
      "Hongchi Xia",
      "Chih-Hao Lin",
      "Hao-Yu Hsu",
      "Quentin Leboutet",
      "Katelyn Gao",
      "Michael Paulitsch",
      "Benjamin Ummenhofer",
      "Shenlong Wang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "interactive 3D reconstruction",
      "scene-graph representation",
      "energy-based optimization",
      "sampling-based exploration",
      "gradient-based refinement",
      "digital twins",
      "geometry completeness",
      "physical stability",
      "photorealistic rendering"
    ]
  },
  {
    "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning",
    "summary": "Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.",
    "authors": [
      "Yifan Wang",
      "Bolian Li",
      "Junlin Wu",
      "Zhaoxuan Tan",
      "Zheli Liu",
      "Ruqi Zhang",
      "Ananth Grama",
      "Qingkai Zeng"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "DRIFT",
      "Dissatisfaction-Refined Iterative Preference Training",
      "WildFeedback",
      "UltraFeedback",
      "WildBench Task Score",
      "AlpacaEval2 win rate",
      "iterative DPO",
      "SPIN",
      "gradient degeneration",
      "preference margins",
      "exploratory capacity"
    ]
  },
  {
    "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling",
    "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
    "authors": [
      "Hyung Gyu Rho"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "Best-of-N (BoN) sampling",
      "reward models",
      "pairwise comparison data",
      "discrete choice models",
      "adaptive inference strategy",
      "best of mini-N in-loop",
      "reliability failures",
      "inference speed",
      "IMDB-sentiment setting"
    ]
  },
  {
    "title": "CoDA: Coding LM via Diffusion Adaptation",
    "summary": "Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.",
    "authors": [
      "Haolin Chen",
      "Shiyu Wang",
      "Can Qin",
      "Bo Pang",
      "Zuxin Liu",
      "Jielin Qiu",
      "Jianguo Zhang",
      "Yingbo Zhou",
      "Zeyuan Chen",
      "Ran Xu",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang",
      "Weiran Yao"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "diffusion language models",
      "bidirectional context",
      "infilling capabilities",
      "autoregressive coders",
      "diffusion coder",
      "large-scale diffusion pre-training",
      "code-centric mid-training",
      "instruction tuning",
      "confidence-guided sampling",
      "inference latency",
      "Humaneval",
      "MBPP",
      "EvalPlus",
      "diffusion-based coding assistants"
    ]
  },
  {
    "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
    "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.",
    "authors": [
      "Chengyue Wu",
      "Hao Zhang",
      "Shuchen Xue",
      "Shizhe Diao",
      "Yonggan Fu",
      "Zhijian Liu",
      "Pavlo Molchanov",
      "Ping Luo",
      "Song Han",
      "Enze Xie"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "autoregressive models",
      "large language models",
      "diffusion language models",
      "block diffusion mechanism",
      "attention mask",
      "blockwise bidirectional context modeling",
      "hierarchical caching",
      "block-level cache",
      "sub-block cache",
      "parallel decoding pipeline"
    ]
  },
  {
    "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
    "summary": "Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.",
    "authors": [
      "Deheng Zhang",
      "Yuqian Fu",
      "Runyi Yang",
      "Yang Miao",
      "Tianwen Qian",
      "Xu Zheng",
      "Guolei Sun",
      "Ajad Chhatkuli",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "egocentric vision",
      "visual question answering",
      "day-night aligned videos",
      "synthetic videos",
      "real-world recordings",
      "day-augmented night auto-labeling",
      "multimodal large language models",
      "day-night correspondence retrieval",
      "egocentric depth estimation"
    ]
  },
  {
    "title": "Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation",
    "summary": "Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.",
    "authors": [
      "Zanyi Wang",
      "Dengyang Jiang",
      "Liuzhuozheng Li",
      "Sizhe Dang",
      "Chengzu Li",
      "Harry Yang",
      "Guang Dai",
      "Mengmeng Wang",
      "Jingdong Wang"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Referring Video Object Segmentation",
      "RVOS",
      "natural language description",
      "pixel control",
      "text-video semantic alignment",
      "temporal coherence",
      "FlowRVS",
      "T2V models",
      "continuous flow problem",
      "generative approach",
      "$\\mathcal{J}\\&\\mathcal{F}$",
      "MeViS",
      "Ref-DAVIS17"
    ]
  },
  {
    "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.",
    "authors": [
      "Hyung Gyu Rho"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Direct Preference Optimization",
      "DPO",
      "IPO",
      "\u03b2-DPO",
      "reward model",
      "preference margins",
      "DPO loss",
      "optimization landscape",
      "reward model estimation errors",
      "sentiment generation task"
    ]
  },
  {
    "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems",
    "summary": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
    "authors": [
      "Shambhavi Mishra",
      "Gaurav Sahu",
      "Marco Pedersoli",
      "Laurent Charlin",
      "Jose Dolz",
      "Christopher Pal"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "AInstein",
      "pretrained parametric knowledge",
      "domain-specific fine-tuning",
      "retrieval augmentation",
      "ICLR 2025 submissions",
      "solver agents",
      "iterative critique loops",
      "scientific inquiry",
      "LLM-as-a-judge",
      "structured rubric",
      "targeted manual checks",
      "Success Rate",
      "Rediscovery",
      "Novelty"
    ]
  },
  {
    "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics",
    "summary": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.",
    "authors": [
      "Maojia Song",
      "Renhang Liu",
      "Xinyu Wang",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Soujanya Poria",
      "Jingren Zhou"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "RAG",
      "Retrieval-Augmented Generation",
      "multi-hop deep search",
      "reasoning path",
      "knowledge utilisation",
      "refusal behaviour",
      "Wikipedia sandbox",
      "EvidenceLoop",
      "verification loops",
      "evidence tracking"
    ]
  },
  {
    "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.",
    "authors": [
      "Jiaru Zou",
      "Soumya Roy",
      "Vinay Kumar Verma",
      "Ziyi Wang",
      "David Wipf",
      "Pan Lu",
      "Sumit Negi",
      "James Zou",
      "Jingrui He"
    ],
    "date": "2025-10-07",
    "ai_keywords": [
      "Process Reward Models",
      "large reasoning models",
      "test-time scaling",
      "tabular reasoning",
      "sub-table retrieval",
      "schema interaction",
      "TaTToo",
      "data curation pipeline",
      "step-level annotations",
      "tool-based verification",
      "dual-stage paradigm",
      "cold-start supervised fine-tuning",
      "reinforcement learning",
      "reward shaping",
      "policy improvement",
      "numerical reasoning",
      "fact-checking",
      "data analysis",
      "generalizability"
    ]
  },
  {
    "title": "Optimized Minimal 4D Gaussian Splatting",
    "summary": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.",
    "authors": [
      "Minseo Lee",
      "Byeonghyeon Lee",
      "Lucas Yunkyu Lee",
      "Eunsoo Lee",
      "Sangmin Kim",
      "Seunghyeon Song",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Jaesik Park",
      "Eunbyung Park"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "Gaussian Splatting",
      "Gaussian Sampling",
      "Gaussian Pruning",
      "Gaussian Merging",
      "implicit appearance compression",
      "Sub-Vector Quantization"
    ]
  },
  {
    "title": "Multilingual Routing in Mixture-of-Experts",
    "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
    "authors": [
      "Lucas Bandarkar",
      "Chenyuan Yang",
      "Mohsen Fayyaz",
      "Junlin Hu",
      "Nanyun Peng"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Mixture-of-Experts",
      "MoE",
      "parallel multilingual datasets",
      "layer-wise phenomena",
      "token routing",
      "language-specific routing",
      "cross-lingual routing alignment",
      "parameter-sharing",
      "middle-layer task experts",
      "inference-time interventions",
      "multilingual performance",
      "language-universal experts"
    ]
  },
  {
    "title": "CWM: An Open-Weights LLM for Research on Code Generation with World\n  Models",
    "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.",
    "authors": [
      "FAIR CodeGen team",
      "Quentin Carbonneaux",
      "Gal Cohen",
      "Jonas Gehring",
      "Jacob Kahn",
      "Jannik Kossen",
      "Felix Kreuk",
      "Emily McMilin",
      "Michel Meyer",
      "Yuxiang Wei",
      "David Zhang",
      "Kunhao Zheng",
      "Jordi Armengol-Estap\u00e9",
      "Pedram Bashiri",
      "Maximilian Beck",
      "Pierre Chambon",
      "Abhishek Charnalia",
      "Chris Cummins",
      "Juliette Decugis",
      "Zacharias V. Fisches",
      "Fran\u00e7ois Fleuret",
      "Fabian Gloeckle",
      "Alex Gu",
      "Michael Hassid",
      "Daniel Haziza",
      "Badr Youbi Idrissi",
      "Christian Keller",
      "Rahul Kindi",
      "Hugh Leather",
      "Gallil Maimon",
      "Aram Markosyan",
      "Francisco Massa",
      "Pierre-Emmanuel Mazar\u00e9",
      "Vegard Mella",
      "Naila Murray",
      "Keyur Muzumdar",
      "Peter O'Hearn",
      "Matteo Pagliardini",
      "Dmitrii Pedchenko",
      "Tal Remez",
      "Volker Seeker",
      "Marco Selvi",
      "Oren Sultan",
      "Sida Wang",
      "Luca Wehrstedt",
      "Ori Yoran",
      "Lingming Zhang",
      "Taco Cohen",
      "Yossi Adi",
      "Gabriel Synnaeve"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Code World Model",
      "LLM",
      "observation-action trajectories",
      "Python interpreter",
      "agentic Docker environments",
      "multi-task reasoning RL",
      "world modeling",
      "step-by-step simulation",
      "SWE-bench Verified",
      "LiveCodeBench",
      "Math-500",
      "AIME 2024",
      "decoder-only LLM",
      "context size",
      "pass@1 scores"
    ]
  },
  {
    "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
    "summary": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14times less training data and 16times less compute than the prior decentralized baseline.",
    "authors": [
      "Zhiying Jiang",
      "Raihan Seraj",
      "Marcos Villagra",
      "Bidhan Roy"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "diffusion model",
      "decentralized computation",
      "Distributed Diffusion Training",
      "expert diffusion models",
      "parameter synchronization",
      "gradient updates",
      "semantically coherent clusters",
      "lightweight transformer router",
      "heterogeneous hardware",
      "decentralized training"
    ]
  },
  {
    "title": "Position: Privacy Is Not Just Memorization!",
    "summary": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
    "authors": [
      "Niloofar Mireshghallah",
      "Tianshi Li"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "privacy risks",
      "data collection",
      "inference-time context leakage",
      "autonomous agent capabilities",
      "surveillance",
      "deep inference attacks",
      "privacy frameworks",
      "sociotechnical nature"
    ]
  },
  {
    "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale\n  Data on LLMs?",
    "summary": "Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance.",
    "authors": [
      "Haizhong Zheng",
      "Jiawei Zhao",
      "Bedi Chen"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning",
      "on-policy training",
      "asynchronous RL",
      "rollout generation",
      "staleness",
      "importance weights",
      "M2PO",
      "Second-Moment Trust Policy Optimization",
      "clipped tokens",
      "high-variance tokens",
      "off-policy training"
    ]
  },
  {
    "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM\n  Reasoning",
    "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93 fewer rollouts and a 4.19 reduction in wall-clock time to match GRPO's best accuracy.",
    "authors": [
      "Ziyan Wang",
      "Zheng Wang",
      "Jie Fu",
      "Xingwei Qu",
      "Qi Cheng",
      "Shengpu Tang",
      "Minjia Zhang",
      "Xiaoming Huo"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "Reinforcement learning",
      "large language models",
      "Group Relative Policy Optimization",
      "Slow-Fast Policy Optimization",
      "policy-gradient pipelines",
      "on-policy algorithms",
      "noisy gradients",
      "low-quality rollouts",
      "unstable updates",
      "inefficient exploration",
      "reposition mechanism",
      "off-policy drift",
      "math reasoning benchmarks"
    ]
  },
  {
    "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and\n  Performance",
    "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.",
    "authors": [
      "Ahmed Alajrami",
      "Xingwei Tan",
      "Nikolaos Aletras"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "instruction-tuning",
      "large language models",
      "LLMs",
      "perturbations",
      "stop words",
      "word shuffling",
      "MMLU",
      "BBH",
      "GSM8K",
      "learning dynamics",
      "model behavior",
      "noisy user inputs"
    ]
  },
  {
    "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
    "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.",
    "authors": [
      "Xingjian Zhao",
      "Zhe Xu",
      "Qinyuan Cheng",
      "Zhaoye Fei",
      "Luozhijie Jin",
      "Yang Wang",
      "Hanfu Chen",
      "Yaozhou Jiang",
      "Qinghui Gao",
      "Ke Chen",
      "Ruixiao Li",
      "Mingshu Chen",
      "Ruiming Wang",
      "Wenbo Zhang",
      "Yiyang Zhang",
      "Donghua Yu",
      "Yang Gao",
      "Xiaogui Yang",
      "Yitian Gong",
      "Yuanfan Xu",
      "Yaqian Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "speech-to-speech large language model",
      "modality-based layer-splitting architecture",
      "frozen pre-training strategy",
      "spoken question answering"
    ]
  },
  {
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "summary": "Power transforms are popular parametric techniques for making data more Gaussian-like, and are widely used as preprocessing steps in statistical analysis and machine learning. However, we find that direct implementations of power transforms suffer from severe numerical instabilities, which can lead to incorrect results or even crashes. In this paper, we provide a comprehensive analysis of the sources of these instabilities and propose effective remedies. We further extend power transforms to the federated learning setting, addressing both numerical and distributional challenges that arise in this context. Experiments on real-world datasets demonstrate that our methods are both effective and robust, substantially improving stability compared to existing approaches.",
    "authors": [
      "Xuefeng Xu",
      "Graham Cormode"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "power transforms",
      "federated learning",
      "numerical instabilities",
      "distributional challenges"
    ]
  },
  {
    "title": "Federated Computation of ROC and PR Curves",
    "summary": "Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are fundamental tools for evaluating machine learning classifiers, offering detailed insights into the trade-offs between true positive rate vs. false positive rate (ROC) or precision vs. recall (PR). However, in Federated Learning (FL) scenarios, where data is distributed across multiple clients, computing these curves is challenging due to privacy and communication constraints. Specifically, the server cannot access raw prediction scores and class labels, which are used to compute the ROC and PR curves in a centralized setting. In this paper, we propose a novel method for approximating ROC and PR curves in a federated setting by estimating quantiles of the prediction score distribution under distributed differential privacy. We provide theoretical bounds on the Area Error (AE) between the true and estimated curves, demonstrating the trade-offs between approximation accuracy, privacy, and communication cost. Empirical results on real-world datasets demonstrate that our method achieves high approximation accuracy with minimal communication and strong privacy guarantees, making it practical for privacy-preserving model evaluation in federated systems.",
    "authors": [
      "Xuefeng Xu",
      "Graham Cormode"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Receiver Operating Characteristic (ROC)",
      "Precision-Recall (PR) curves",
      "Federated Learning (FL)",
      "distributed differential privacy",
      "quantiles",
      "prediction score distribution",
      "Area Error (AE)"
    ]
  },
  {
    "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts",
    "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
    "authors": [
      "Jihoon Lee",
      "Hoyeon Moon",
      "Kevin Zhai",
      "Arun Kumar Chithanar",
      "Anit Kumar Sahu",
      "Soummya Kar",
      "Chul Lee",
      "Souradip Chakraborty",
      "Amrit Singh Bedi"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "diffusion-based large language models",
      "dLLMs",
      "semi-autoregressive experts",
      "inference time schedule",
      "HEX",
      "Hidden semiautoregressive EXperts",
      "test-time scaling",
      "GSM8K",
      "MATH",
      "ARC-C",
      "TruthfulQA"
    ]
  },
  {
    "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement\n  Learning",
    "summary": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.",
    "authors": [
      "Jonas H\u00fcbotter",
      "Leander Diaz-Bone",
      "Ido Hakimi",
      "Andreas Krause",
      "Moritz Hardt"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "test-time curriculum",
      "TTC-RL",
      "reinforcement learning",
      "task-specific curriculum",
      "pass@1",
      "pass@8",
      "AIME25",
      "CodeElo",
      "continual training"
    ]
  },
  {
    "title": "Character Mixing for Video Generation",
    "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where characters interact naturally across different worlds? We study inter-character interaction in text-to-video generation, where the key challenge is to preserve each character's identity and behaviors while enabling coherent cross-context interaction. This is difficult because characters may never have coexisted and because mixing styles often causes style delusion, where realistic characters appear cartoonish or vice versa. We introduce a framework that tackles these issues with Cross-Character Embedding (CCE), which learns identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA), which enriches training with synthetic co-existence and mixed-style data. Together, these techniques allow natural interactions between previously uncoexistent characters without losing stylistic fidelity. Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion, enabling new forms of generative storytelling.Additional results and videos are available on our project page: https://tingtingliao.github.io/mimix/.",
    "authors": [
      "Tingting Liao",
      "Chongjian Ge",
      "Guangyi Liu",
      "Hao Li",
      "Yi Zhou"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Cross-Character Embedding",
      "Cross-Character Augmentation",
      "text-to-video generation",
      "identity preservation",
      "interaction quality",
      "style delusion",
      "multimodal sources",
      "synthetic co-existence",
      "mixed-style data",
      "generative storytelling"
    ]
  },
  {
    "title": "Optimal Scaling Needs Optimal Norm",
    "summary": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair (eta^{ast}, B^{ast}) consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple (eta, B) reach the optimal norm, only a unique (eta^{ast}, B^{ast}) achieves the best loss. As a sufficient condition, we provide the first measurement of (eta^{ast}, B^{ast}) scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
    "authors": [
      "Oleg Filatov",
      "Jiangtao Wang",
      "Jan Ebert",
      "Stefan Kesselheim"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "Scion optimizer",
      "operator norm",
      "output layer",
      "norm transfer",
      "learning rate",
      "batch size",
      "Adam optimizer",
      "per-layer-group learning rates",
      "Distributed Scion (Disco)",
      "LLM training dynamics"
    ]
  },
  {
    "title": "Code4MeV2: a Research-oriented Code-completion Platform",
    "summary": "The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates a barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on human--AI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4MeV2, a research-oriented, open-source code completion plugin for JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a client--server architecture and features inline code completion and a context-aware chat assistant. Its core contribution is a modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4MeV2 achieves industry-comparable performance in terms of code completion, with an average latency of 200~ms. We assess our tool through a combination of an expert evaluation and a user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found at https://app.code4me.me.",
    "authors": [
      "Roham Koohestani",
      "Parham Bateni",
      "Aydin Ebrahimi",
      "Behdad Etezadi",
      "Kiarash Karimi",
      "Maliheh Izadi"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "code completion",
      "client-server architecture",
      "inline code completion",
      "context-aware chat assistant",
      "telemetry",
      "context gathering",
      "expert evaluation",
      "user study"
    ]
  },
  {
    "title": "SAEdit: Token-level control for continuous image editing via Sparse\n  AutoEncoder",
    "summary": "Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce a method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ a Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains.",
    "authors": [
      "Ronen Kamenetsky",
      "Sara Dorfman",
      "Daniel Garibi",
      "Roni Paiss",
      "Or Patashnik",
      "Daniel Cohen-Or"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "diffusion models",
      "text-to-image",
      "disentanglement",
      "continuous control",
      "token-level manipulation",
      "text embeddings",
      "sparse autoencoder",
      "sparse latent space",
      "semantically isolated dimensions",
      "model agnostic"
    ]
  },
  {
    "title": "LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL",
    "summary": "Converting natural language questions into SQL queries (Text-to-SQL) enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early NL2SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the LLM era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark: unlike the original WikiSQL, tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural language-to-SQL models.",
    "authors": [
      "Dzmitry Pihulski",
      "Karol Charchut",
      "Viktoria Novogrodskaia",
      "Jan Koco\u0144"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Text-to-SQL",
      "NL2SQL",
      "WikiSQL",
      "LLM",
      "Gemma 3",
      "LLaMA 3.2",
      "Mistral 7B",
      "gpt-oss 20B",
      "Phi-3.5 Mini",
      "Qwen 2.5",
      "OpenAI o4-mini",
      "DeepSeek R1",
      "pointer-network models"
    ]
  },
  {
    "title": "Imperceptible Jailbreaking against Large Language Models",
    "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is \"secretly\" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.",
    "authors": [
      "Kuofeng Gao",
      "Yiming Li",
      "Chao Du",
      "Xin Wang",
      "Xingjun Ma",
      "Shu-Tao Xia",
      "Tianyu Pang"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "adversarial perturbations",
      "Unicode characters",
      "variation selectors",
      "tokenization",
      "chain-of-search pipeline",
      "prompt injection attacks",
      "aligned LLMs"
    ]
  },
  {
    "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition",
    "summary": "Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition.",
    "authors": [
      "Umberto Cappellazzo",
      "Minsu Kim",
      "Pingchuan Ma",
      "Honglie Chen",
      "Xubo Liu",
      "Stavros Petridis",
      "Maja Pantic"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "large language models",
      "audio-visual speech recognition",
      "token compression",
      "Matryoshka representation learning",
      "Mixture-of-Experts",
      "MoME",
      "LRS2",
      "LRS3",
      "ASR",
      "VSR",
      "parameter-efficient",
      "robustness",
      "noise"
    ]
  },
  {
    "title": "Self-Reflective Generation at Test Time",
    "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates a clear need for self-reflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), a lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains a specific corrective vector, which fully exploits the already generated context for a self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques.",
    "authors": [
      "Jian Mu",
      "Qixin Zhang",
      "Zhiyong Wang",
      "Menglin Yang",
      "Shuang Qiu",
      "Chengwei Qin",
      "Zhongxiang Dai",
      "Yao Shu"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Self-Reflective Generation",
      "SRGen",
      "dynamic entropy thresholding",
      "corrective vector",
      "self-reflective generation",
      "token probability distribution",
      "self-consistency voting",
      "AIME2024",
      "DeepSeek-R1-Distill-Qwen-7B",
      "Pass@1",
      "Cons@5",
      "RLHF",
      "SLOT"
    ]
  },
  {
    "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
    "summary": "Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation",
    "authors": [
      "Dustin Wright",
      "Sarah Masud",
      "Jared Moore",
      "Srishti Yadav",
      "Maria Antoniak",
      "Chan Young Park",
      "Isabelle Augenstein"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "epistemic diversity",
      "knowledge collapse",
      "retrieval-augmented generation",
      "RAG"
    ]
  },
  {
    "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
    "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.",
    "authors": [
      "Siwei Han",
      "Jiaqi Liu",
      "Yaofeng Su",
      "Wenbo Duan",
      "Xinyuan Liu",
      "Cihang Xie",
      "Mohit Bansal",
      "Mingyu Ding",
      "Linjun Zhang",
      "Huaxiu Yao"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Alignment Tipping Process",
      "Self-Interested Exploration",
      "Imitative Strategy Diffusion",
      "reinforcement learning-based alignment methods"
    ]
  },
  {
    "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized\n  Problems via Symmetry and Difficulty",
    "summary": "Large Language Models (LLMs) for formal theorem proving have shown significant promise, yet they often lack generalizability and are fragile to even minor transformations of problem statements. To address this limitation, we introduce a novel data augmentation pipeline designed to enhance model robustness from two perspectives: symmetry and difficulty. From the symmetry perspective, we propose two complementary methods: EvolAST, an Abstract Syntax Tree (AST) based approach that targets syntactic symmetry to generate semantically equivalent problem variants, and EvolDomain, which leverages LLMs to address semantic symmetry by translating theorems across mathematical domains. From the difficulty perspective, we propose EvolDifficulty, which uses carefully designed evolutionary instructions to guide LLMs in generating new theorems with a wider range of difficulty. We then use the evolved data to train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8% pass@32 rate, surpassing all models of comparable size, including reasoning-based models. It also sets new SOTA records for non-reasoning models on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our data augmentation pipeline's effectiveness across multiple benchmarks.",
    "authors": [
      "Yuchen Tian",
      "Ruiyuan Huang",
      "Xuanwu Wang",
      "Jing Ma",
      "Zengfeng Huang",
      "Ziyang Luo",
      "Hongzhan Lin",
      "Da Zheng",
      "Lun Du"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Large Language Models",
      "formal theorem proving",
      "data augmentation",
      "Abstract Syntax Tree",
      "EvolAST",
      "EvolDomain",
      "EvolDifficulty",
      "EvolProver",
      "FormalMATH-Lite",
      "MiniF2F-Test",
      "Ineq-Comp-Seed",
      "Ineq-Comp-Transformed"
    ]
  },
  {
    "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World\n  Simulation",
    "summary": "Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, the target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: https://research.nvidia.com/labs/toronto-ai/chronoedit",
    "authors": [
      "Jay Zhangjie Wu",
      "Xuanchi Ren",
      "Tianchang Shen",
      "Tianshi Cao",
      "Kai He",
      "Yifan Lu",
      "Ruiyuan Gao",
      "Enze Xie",
      "Shiyi Lan",
      "Jose M. Alvarez",
      "Jun Gao",
      "Sanja Fidler",
      "Zian Wang",
      "Huan Ling"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "video generative models",
      "temporal consistency",
      "temporal reasoning",
      "denoising",
      "reasoning tokens",
      "PBench-Edit",
      "visual fidelity",
      "physical plausibility"
    ]
  },
  {
    "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning",
    "summary": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.",
    "authors": [
      "Zhenyu Pan",
      "Yiting Zhang",
      "Zhuo Liu",
      "Yolo Yunlong Tang",
      "Zeliang Zhang",
      "Haozheng Luo",
      "Yuwei Han",
      "Jianshu Zhang",
      "Dennis Wu",
      "Hong-Yu Chen",
      "Haoran Lu",
      "Haoyang Fang",
      "Manling Li",
      "Chenliang Xu",
      "Philip S. Yu",
      "Han Liu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "multi-agent systems",
      "planning",
      "tool use",
      "role coordination",
      "jailbreak",
      "prompt-injection",
      "adversarial collaboration",
      "self-verification",
      "external guard modules",
      "co-evolutionary",
      "reinforcement learning",
      "attackers",
      "defenders",
      "adversarial learning",
      "advantage estimation",
      "group-level mean-return baseline",
      "attack-success rate",
      "task accuracy",
      "reasoning tasks"
    ]
  },
  {
    "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual\n  Information",
    "summary": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.",
    "authors": [
      "Jiaxi Li",
      "Yucheng Shi",
      "Jin Lu",
      "Ninghao Liu"
    ],
    "date": "2025-10-04",
    "ai_keywords": [
      "Mutual Information Tree Search",
      "MITS",
      "pointwise mutual information",
      "PMI",
      "beam search",
      "entropy-based dynamic sampling",
      "weighted voting scheme",
      "reasoning paths",
      "search tree expansion",
      "information-theoretic principles"
    ]
  },
  {
    "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
    "summary": "As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.",
    "authors": [
      "Yurun Chen",
      "Xavier Hu",
      "Yuhan Liu",
      "Ziqi Wang",
      "Zeyi Liao",
      "Lin Chen",
      "Feng Wei",
      "Yuxi Qian",
      "Bo Zheng",
      "Keting Yin",
      "Shengyu Zhang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multimodal LLM-driven agents",
      "knowledge graph",
      "subgraph sampling",
      "task templates",
      "meta-paths",
      "node reachability",
      "LLM scoring",
      "similarity analysis",
      "Graph2Eval-Bench",
      "document comprehension",
      "web interaction tasks",
      "Single-Agent",
      "Multi-Agent",
      "Web Agent"
    ]
  },
  {
    "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
    "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.",
    "authors": [
      "Thanapol Popit",
      "Natthapath Rungseesiripak",
      "Monthol Charattrakool",
      "Saksorn Ruangtanusak"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "zero-shot prompting",
      "few-shot prompting",
      "compact LLMs",
      "lightweight transformers",
      "end-of-turn detection",
      "token boundaries"
    ]
  },
  {
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models",
    "summary": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "authors": [
      "Qizheng Zhang",
      "Changran Hu",
      "Shubhangi Upasani",
      "Boyuan Ma",
      "Fenglu Hong",
      "Vamsidhar Kamanuru",
      "Jay Rainton",
      "Chen Wu",
      "Mengmeng Ji",
      "Hanchen Li",
      "Urmish Thakker",
      "James Zou",
      "Kunle Olukotun"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Large language model",
      "context adaptation",
      "Dynamic Cheatsheet",
      "ACE",
      "Agentic Context Engineering",
      "generation",
      "reflection",
      "curation",
      "context collapse",
      "adaptation latency",
      "rollout cost",
      "natural execution feedback",
      "AppWorld leaderboard"
    ]
  },
  {
    "title": "HiKE: Hierarchical Evaluation Framework for Korean-English\n  Code-Switching Speech Recognition",
    "summary": "Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that while most multilingual ASR models initially struggle with CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE will be available at https://github.com/ThetaOne-AI/HiKE.",
    "authors": [
      "Gio Paik",
      "Yongbeom Kim",
      "Soungmin Lee",
      "Sangmin Ahn",
      "Chanwoo Kim"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "multilingual automatic speech recognition",
      "code-switching",
      "HiKE",
      "Hierarchical Korean-English code-switching benchmark",
      "loanword labels",
      "hierarchical CS-level labeling scheme",
      "fine-tuning"
    ]
  },
  {
    "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
    "summary": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.",
    "authors": [
      "Dachuan Shi",
      "Abedelkadir Asi",
      "Keying Li",
      "Xiangchi Yuan",
      "Leyan Pan",
      "Wenke Lee",
      "Wen Xiao"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "latent reasoning",
      "explicit reasoning",
      "next-token distributions",
      "entropy trends",
      "block-wise confidence",
      "thinking-block switches",
      "token efficiency",
      "mathematics benchmarks",
      "STEM benchmarks"
    ]
  },
  {
    "title": "Watch and Learn: Learning to Use Computers from Online Videos",
    "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, we introduce Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, we cast the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, we develop an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment.",
    "authors": [
      "Chan Hee Song",
      "Yiwen Song",
      "Palash Goyal",
      "Yu Su",
      "Oriana Riva",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "inverse dynamics",
      "task-aware video retrieval",
      "UI trajectories",
      "OSWorld benchmark"
    ]
  },
  {
    "title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals",
    "summary": "While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q\\&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.",
    "authors": [
      "Le Zhuo",
      "Songhao Han",
      "Yuandong Pu",
      "Boxiang Qiu",
      "Sayak Paul",
      "Yue Liao",
      "Yihao Liu",
      "Jie Shao",
      "Xi Chen",
      "Si Liu",
      "Hongsheng Li"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "VLM",
      "FLUX Kontext",
      "chain-of-thought reasoning",
      "multimodal understanding",
      "three-stage training curriculum",
      "feature alignment",
      "knowledge infusion",
      "reasoning-augmented generation",
      "StructBench",
      "StructScore",
      "multi-round Q&A protocol"
    ]
  },
  {
    "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
    "summary": "Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
    "authors": [
      "Yunlong Tang",
      "Jing Bi",
      "Pinxin Liu",
      "Zhenyu Pan",
      "Zhangyun Tan",
      "Qianxiang Shen",
      "Jiani Liu",
      "Hang Hua",
      "Junjia Guo",
      "Yunzhong Xiao",
      "Chao Huang",
      "Zhiyuan Wang",
      "Susan Liang",
      "Xinyi Liu",
      "Yizhi Song",
      "Yuhe Nie",
      "Jia-Xing Zhong",
      "Bozheng Li",
      "Daiqing Qi",
      "Ziyun Zeng",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Daiki Shimada",
      "Han Liu",
      "Jiebo Luo",
      "Chenliang Xu"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Video-LMMs",
      "supervised fine-tuning",
      "reinforcement learning",
      "test-time scaling",
      "temporal localization",
      "spatiotemporal grounding",
      "long video efficiency",
      "multimodal evidence integration",
      "reward design",
      "scalability",
      "cost-performance optimization"
    ]
  },
  {
    "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
    "summary": "Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos.",
    "authors": [
      "Ziqi Huang",
      "Ning Yu",
      "Gordon Chen",
      "Haonan Qiu",
      "Paul Debevec",
      "Ziwei Liu"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "video generation",
      "visual reasoning",
      "multimodal models",
      "keyframes",
      "sparse inference-time tuning",
      "pre-trained video generator"
    ]
  },
  {
    "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights",
    "summary": "Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.",
    "authors": [
      "Sangmin Bae",
      "Bilge Acun",
      "Haroun Habeeb",
      "Seungyeon Kim",
      "Chien-Yu Lin",
      "Liang Luo",
      "Junjie Wang",
      "Carole-Jean Wu"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "self-attention mechanisms",
      "structured state space models",
      "Mamba",
      "hybrid architectures",
      "inter-layer fusion",
      "intra-layer fusion",
      "language modeling performance",
      "long-context capabilities",
      "scaling analysis",
      "training efficiency",
      "inference efficiency",
      "computational primitive"
    ]
  },
  {
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "summary": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.",
    "authors": [
      "Zeyu Zhu",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "multi-agent framework",
      "slide generation",
      "layout refinement",
      "tree search visual choice",
      "cursor grounding",
      "subtitling",
      "speech synthesis",
      "talking-head rendering",
      "parallelization",
      "slide-wise generation"
    ]
  },
  {
    "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
    "summary": "The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.",
    "authors": [
      "Grace LeFevre",
      "Qingcheng Zeng",
      "Adam Leif",
      "Jason Jewell",
      "Denis Peskoff",
      "Rob Voigt"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "Natural Language Processing",
      "NLP for Social Good",
      "NLP4SG",
      "ACL Anthology",
      "UN Sustainable Development Goals",
      "author-level perspective",
      "venue-level perspective",
      "core ACL contributors",
      "non-ACL authors",
      "agenda-setting considerations"
    ]
  },
  {
    "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
    "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
    "authors": [
      "Wei Xiong",
      "Chenlu Ye",
      "Baohao Liao",
      "Hanze Dong",
      "Xinxing Xu",
      "Christof Monz",
      "Jiang Bian",
      "Nan Jiang",
      "Tong Zhang"
    ],
    "date": "2025-10-06",
    "ai_keywords": [
      "reinforcement learning",
      "large language models",
      "gradient estimates",
      "inference budget",
      "stochastic gradient variance",
      "adaptive sampling",
      "online successive elimination",
      "reward diversity",
      "advantage baselines",
      "variance-aware",
      "adaptive data curation"
    ]
  },
  {
    "title": "Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions",
    "summary": "The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.",
    "authors": [
      "Zhuohang Li",
      "Xiaowei Li",
      "Chengyu Huang",
      "Guowang Li",
      "Katayoon Goshvadi",
      "Bo Dai",
      "Dale Schuurmans",
      "Paul Zhou",
      "Hamid Palangi",
      "Yiwen Song",
      "Palash Goyal",
      "Murat Kantarcioglu",
      "Bradley A. Malin",
      "Yuan Xue"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "autoraters",
      "preference labels",
      "probabilistic autoraters",
      "distribution-matching objective",
      "verbalized probability predictions",
      "calibration",
      "positional bias"
    ]
  },
  {
    "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
    "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
    "authors": [
      "Syeda Nahida Akter",
      "Shrimai Prabhumoye",
      "Eric Nyberg",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Yejin Choi",
      "Bryan Catanzaro"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "LLMs",
      "reasoning data",
      "pretraining",
      "post-training",
      "mid-training",
      "overfitting",
      "generalization",
      "systematic study",
      "SFT",
      "data allocation",
      "language modeling",
      "reasoning"
    ]
  },
  {
    "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
    "summary": "The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
    "authors": [
      "Adam Filipek"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Transformer architecture",
      "Large Language Models (LLMs)",
      "stateless nature",
      "quadratic computational complexity",
      "Reactive Transformer (RxT)",
      "event-driven paradigm",
      "Short-Term Memory (STM)",
      "generator-decoder",
      "memory-encoder",
      "Memory Attention network",
      "linear scaling",
      "constant-time inference latency"
    ]
  },
  {
    "title": "Utility-Learning Tension in Self-Modifying Agents",
    "summary": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
    "authors": [
      "Charles L. Wang",
      "Keir Dorchen",
      "Peter Jin"
    ],
    "date": "2025-10-05",
    "ai_keywords": [
      "self-improving systems",
      "utility--learning tension",
      "statistical preconditions",
      "reliable learning",
      "generalization",
      "distribution-free guarantees",
      "policy-reachable model family",
      "uniformly capacity-bounded",
      "utility-rational self-changes",
      "learnable tasks",
      "two-gate policies"
    ]
  },
  {
    "title": "Efficient Test-Time Scaling for Small Vision-Language Models",
    "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient alternative to larger models, at the cost of weaker generalization abilities and downstream task performance. These shortcomings could be addressed by test-time scaling techniques, but existing methods are typically computationally demanding, contradicting the resource-efficient design goals of small models. To address these limitations, we propose two novel and efficient test-time scaling strategies that leverage the model-internal features rather than external supervision: (i) Test-Time Augmentation (TTAug), which generates multiple augmented inputs and aggregates outputs at the token level without parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model parameters during inference using consensus-based pseudolabels from TTAug. Through extensive experiments across nine benchmarks, we demonstrate consistent performance improvements while maintaining computational efficiency suitable for resource-constrained environments. The generality of our approach is demonstrated both within models at different scales and across different VLMs without additional tuning.",
    "authors": [
      "Mehmet Onurcan Kaya",
      "Desmond Elliott",
      "Dim P. Papadopoulos"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Test-Time Augmentation",
      "Test-Time Adaptation",
      "vision-language models",
      "token level",
      "parameter updates",
      "consensus-based pseudolabels",
      "resource-constrained environments"
    ]
  },
  {
    "title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate\n  Medical Text- and Time-Series Data",
    "summary": "LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source.",
    "authors": [
      "Patrick Langer",
      "Thomas Kaar",
      "Max Rosenblattl",
      "Maxwell A. Xu",
      "Winnie Chow",
      "Martin Maritsch",
      "Aradhana Verma",
      "Brian Han",
      "Daniel Seung Kim",
      "Henry Chubb",
      "Scott Ceresnak",
      "Aydin Zahedivash",
      "Alexander Tarlochan Singh Sandhu",
      "Fatima Rodriguez",
      "Daniel McDuff",
      "Elgar Fleisch",
      "Oliver Aalami",
      "Filipe Barata",
      "Paul Schmiedmayer"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Time Series Language Models",
      "TSLMs",
      "soft prompting",
      "cross-attention",
      "Chain-of-Thought",
      "HAR-CoT",
      "Sleep-CoT",
      "ECG-QA-CoT",
      "F1 score",
      "GPT-4o",
      "LLaMA-3B"
    ]
  },
  {
    "title": "Less LLM, More Documents: Searching for Improved RAG",
    "summary": "Retrieval-Augmented Generation (RAG) couples document retrieval with large language models (LLMs). While scaling generators improves accuracy, it also raises cost and limits deployability. We explore an orthogonal axis: enlarging the retriever's corpus to reduce reliance on large LLMs. Experimental results show that corpus scaling consistently strengthens RAG and can often serve as a substitute for increasing model size, though with diminishing returns at larger scales. Small- and mid-sized generators paired with larger corpora often rival much larger models with smaller corpora; mid-sized models tend to gain the most, while tiny and large models benefit less. Our analysis shows that improvements arise primarily from increased coverage of answer-bearing passages, while utilization efficiency remains largely unchanged. These findings establish a principled corpus-generator trade-off: investing in larger corpora offers an effective path to stronger RAG, often comparable to enlarging the LLM itself.",
    "authors": [
      "Jingjie Ning",
      "Yibo Kong",
      "Yunfan Long",
      "Jamie Callan"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Retrieval-Augmented Generation",
      "RAG",
      "document retrieval",
      "large language models",
      "LLMs",
      "corpus scaling",
      "answer-bearing passages"
    ]
  },
  {
    "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail\n  At It",
    "summary": "Current large language model (LLM) development treats task-solving and preference alignment as separate challenges, optimizing first for objective correctness, then for alignment to aggregated human preferences. This paradigm fails in human-facing applications where solving a problem correctly is insufficient if the response mismatches the user's needs. This challenge intensifies in just-in-time scenarios where no prior user interaction history exists due to cold-start conditions or privacy constraints. LLMs need to identify what they don't know about user preferences, strategically elicit preference values through questioning, then adapt their reasoning processes and responses accordingly -- a complicated chain of cognitive processes which we term personalized reasoning. We introduce PREFDISCO, an evaluation methodology that transforms static benchmarks into interactive personalization tasks using psychologically-grounded personas with sparse preferences. Our framework creates scenarios where identical questions require different reasoning chains depending on user context, as optimal explanation approaches vary by individual expertise and preferences while maintaining factual accuracy. Evaluation of 21 frontier models across 10 tasks reveals 29.0% of naive personalization attempts produce worse preference alignment than generic responses, yet generic responses also fail to serve individual user needs effectively. These findings suggest personalized reasoning requires dedicated development rather than emerging naturally. PREFDISCO establishes personalized reasoning as a measurable research frontier and reveals fundamental limitations in current LLMs' interactive capabilities, providing a foundation for developing systems that can adapt to individual users in education, healthcare, and technical domains where personalization is critical.",
    "authors": [
      "Shuyue Stella Li",
      "Avinandan Bose",
      "Faeze Brahman",
      "Simon Shaolei Du",
      "Pang Wei Koh",
      "Maryam Fazel",
      "Yulia Tsvetkov"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "large language model",
      "personalized reasoning",
      "PREFDISCO",
      "psychologically-grounded personas",
      "sparse preferences",
      "interactive personalization tasks",
      "reasoning chains",
      "user context",
      "preference alignment",
      "generic responses"
    ]
  },
  {
    "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
    "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming an entire video sequence or chunks of frames are available beforehand. This critically limits their use in interactive applications such as live content creation and emerging generative world models. To address this gap, we introduce the novel task of frame-level online V2A generation, where a model autoregressively generates audio from video without access to future video frames. Furthermore, we propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task. Our design enforces end-to-end causality and targets low per-frame latency with audio-visual synchronization. Our model's backbone is a decoder-only causal transformer over continuous audio latents. For vision conditioning, it leverages grid (patch) features extracted from the smallest variant of the DINOv2 vision encoder, which are aggregated into a single token per frame to maintain end-to-end causality and efficiency. The model is trained through a diffusion pre-training followed by consistency fine-tuning to accelerate the diffusion head decoding. On a benchmark of diverse gameplay videos from AAA titles, our model successfully generates semantically and temporally aligned, high-quality full-band stereo audio, validated by both objective and human evaluations. Furthermore, our model achieves low per-frame waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on 30FPS, 480p videos using a single H100. Demo samples are available at https://koichi-saito-sony.github.io/soundreactor/.",
    "authors": [
      "Koichi Saito",
      "Julian Tanke",
      "Christian Simon",
      "Masato Ishii",
      "Kazuki Shimada",
      "Zachary Novack",
      "Zhi Zhong",
      "Akio Hayakawa",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "frame-level online V2A generation",
      "autoregressive generation",
      "causal transformer",
      "continuous audio latents",
      "DINOv2 vision encoder",
      "diffusion pre-training",
      "consistency fine-tuning",
      "audio-visual synchronization",
      "per-frame latency",
      "waveform-level latency"
    ]
  },
  {
    "title": "Continuously Augmented Discrete Diffusion model for Categorical\n  Generative Modeling",
    "summary": "Standard discrete diffusion models treat all unobserved states identically by mapping them to an absorbing [MASK] token. This creates an 'information void' where semantic information that could be inferred from unmasked tokens is lost between denoising steps. We introduce Continuously Augmented Discrete Diffusion (CADD), a framework that augments the discrete state space with a paired diffusion in a continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than collapsed 'information voids'. At each reverse step, CADD may leverage the continuous latent as a semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and choice of estimator for the continuous latent vector enables a controlled trade-off between mode-coverage (generating diverse outputs) and mode-seeking (generating contextually precise outputs) behaviors. Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines.",
    "authors": [
      "Huangjie Zheng",
      "Shansan Gong",
      "Ruixiang Zhang",
      "Tianrong Chen",
      "Jiatao Gu",
      "Mingyuan Zhou",
      "Navdeep Jaitly",
      "Yizhe Zhang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "discrete diffusion models",
      "absorbing token",
      "information void",
      "continuous latent space",
      "graded",
      "gradually corrupted states",
      "discrete denoising",
      "semantic hint",
      "mode-coverage",
      "mode-seeking",
      "text generation",
      "image synthesis",
      "code modeling"
    ]
  },
  {
    "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
    "summary": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.",
    "authors": [
      "Nishanth Shetty",
      "Madhava Prasath",
      "Chandra Sekhar Seelamantula"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "gradient descent",
      "exponential gradient descent",
      "Dale's law",
      "synaptic weights",
      "Fokker-Planck equation",
      "stochastic differential equation",
      "geometric Brownian motion",
      "multiplicative update rule",
      "denoising score-matching",
      "score-based models",
      "log-normal density"
    ]
  },
  {
    "title": "Pretraining with hierarchical memories: separating long-tail and common\n  knowledge",
    "summary": "The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.",
    "authors": [
      "Hadi Pouransari",
      "David Grangier",
      "C Thomas",
      "Michael Kirchhof",
      "Oncel Tuzel"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "memory-augmented architecture",
      "parametric memory banks",
      "pretraining strategy",
      "hierarchical feed-forward memories",
      "transformers"
    ]
  },
  {
    "title": "Consolidating Reinforcement Learning for Multimodal Discrete Diffusion\n  Models",
    "summary": "Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion.",
    "authors": [
      "Tianren Ma",
      "Mu Zhang",
      "Yibing Wang",
      "Qixiang Ye"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "discrete diffusion model",
      "DDM",
      "rewards",
      "non-autoregressive paradigm",
      "importance sampling",
      "rollout",
      "Group Relative Policy Optimization",
      "GRPO",
      "MaskGRPO",
      "multimodal reinforcement learning",
      "importance estimator",
      "token fluctuation",
      "gradient updates",
      "visual sequences",
      "diverse completions",
      "optimization gradients",
      "math reasoning",
      "visual generation",
      "reasoning performance",
      "generation quality"
    ]
  },
  {
    "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4\n  Quantization",
    "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs.",
    "authors": [
      "Vage Egiazarian",
      "Roberto L. Castro",
      "Denis Kuznedelev",
      "Andrei Panferov",
      "Eldar Kurtic",
      "Shubhra Pandit",
      "Alexandre Marques",
      "Mark Kurtz",
      "Saleh Ashkboos",
      "Torsten Hoefler",
      "Dan Alistarh"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "MXFP4",
      "NVFP4",
      "post-training quantization",
      "GPTQ",
      "block-wise Hadamard transforms",
      "format-specific optimizations",
      "GPU kernels",
      "rotation fusion",
      "online computation of activations",
      "accuracy-performance trade-offs"
    ]
  },
  {
    "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking",
    "summary": "Large reasoning models (LRMs) \"think\" by generating structured chain-of-thought (CoT) before producing a final answer, yet they still lack the ability to reason critically about safety alignment and are easily biased when a flawed premise is injected into their thought process. We propose RECAP (Robust Safety Alignment via Counter-Aligned Prefilling), a principled reinforcement learning (RL) method for post-training that explicitly teaches models to override flawed reasoning trajectories and reroute to safe and helpful responses. RECAP trains on a mixture of synthetically generated counter-aligned CoT prefills and standard prompts, requires no additional training cost or modifications beyond vanilla reinforcement learning from human feedback (RLHF), and substantially improves safety and jailbreak robustness, reduces overrefusal, and preserves core reasoning capability -- all while maintaining inference token budget. Extensive analysis shows that RECAP-trained models engage in self-reflection more frequently and remain robust under adaptive attacks, preserving safety even after repeated attempts to override their reasoning.",
    "authors": [
      "ShengYun Peng",
      "Eric Smith",
      "Ivan Evtimov",
      "Song Jiang",
      "Pin-Yu Chen",
      "Hongyuan Zhan",
      "Haozhu Wang",
      "Duen Horng Chau",
      "Mahesh Pasupuleti",
      "Jianfeng Chi"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "chain-of-thought",
      "reinforcement learning",
      "counter-aligned prefills",
      "standard prompts",
      "reinforcement learning from human feedback",
      "jailbreak robustness",
      "overrefusal",
      "self-reflection",
      "adaptive attacks"
    ]
  },
  {
    "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
    "summary": "Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.",
    "authors": [
      "Kai-Wei Chang",
      "En-Pei Hu",
      "Chun-Yi Kuan",
      "Wenze Ren",
      "Wei-Chih Chen",
      "Guan-Ting Lin",
      "Yu Tsao",
      "Shao-Hua Sun",
      "Hung-yi Lee",
      "James Glass"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Conversational Spoken Language Models",
      "Game-Time Benchmark",
      "temporal dynamics",
      "timing",
      "tempo",
      "simultaneous speaking",
      "conversational fluency",
      "instruction-following tasks",
      "temporal constraints",
      "tempo adherence",
      "synchronized responses",
      "time awareness",
      "full-duplex interaction"
    ]
  },
  {
    "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
    "summary": "While the recent developments in large language models (LLMs) have successfully enabled generative recommenders with natural language interactions, their recommendation behavior is limited, leaving other simpler yet crucial components such as metadata or attribute filtering underutilized in the system. We propose an LLM-based music recommendation system with tool calling to serve as a unified retrieval-reranking pipeline. Our system positions an LLM as an end-to-end recommendation system that interprets user intent, plans tool invocations, and orchestrates specialized components: boolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding similarity), and generative retrieval (semantic IDs). Through tool planning, the system predicts which types of tools to use, their execution order, and the arguments needed to find music matching user preferences, supporting diverse modalities while seamlessly integrating multiple database filtering methods. We demonstrate that this unified tool-calling framework achieves competitive performance across diverse recommendation scenarios by selectively employing appropriate retrieval methods based on user queries, envisioning a new paradigm for conversational music recommendation systems.",
    "authors": [
      "Seungheon Doh",
      "Keunwoo Choi",
      "Juhan Nam"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "LLMs",
      "generative recommenders",
      "natural language interactions",
      "boolean filters",
      "sparse retrieval",
      "BM25",
      "dense retrieval",
      "embedding similarity",
      "generative retrieval",
      "semantic IDs",
      "tool planning",
      "retrieval-reranking pipeline",
      "conversational music recommendation systems"
    ]
  },
  {
    "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern",
    "summary": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .",
    "authors": [
      "Lekang Yang",
      "Yuetong Liu",
      "Yitong Zhang",
      "Jia Li"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "diffusion LLMs",
      "dLLMs",
      "unit test generation",
      "UTG",
      "abstract syntax tree analysis",
      "test coverage",
      "TestEval benchmark",
      "Java",
      "C++"
    ]
  },
  {
    "title": "Scaling Policy Compliance Assessment in Language Models with Policy\n  Reasoning Traces",
    "summary": "Policy compliance assessment is a fundamental task of evaluating whether an input case strictly complies with a set of human-defined rules, more generally known as policies. In practice, human experts follow a systematic, step-by-step process to identify violations with respect to specific stipulations outlined in the policy. However, such documentation of gold-standard, expert-level reasoning processes is costly to acquire. In this paper, we introduce Policy Reasoning Traces (PRT), a form of specialized generated reasoning chains that serve as a reasoning bridge to improve an LLM's policy compliance assessment capabilities. Our empirical evaluations demonstrate that the use of PRTs for both inference-time and training-time scenarios significantly enhances the performance of open-weight and commercial models, setting a new state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also highlight how PRTs can improve an LLM's ability to accurately cite policy clauses, as well as influence compliance decisions through their high utilization from the raw chains of thought.",
    "authors": [
      "Joseph Marvin Imperial",
      "Harish Tayyar Madabushi"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Policy Reasoning Traces",
      "PRT",
      "LLM",
      "policy compliance assessment",
      "reasoning chains",
      "HIPAA",
      "GDPR"
    ]
  },
  {
    "title": "OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features",
    "summary": "Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural network activations into human-interpretable features. However, current SAEs suffer from feature absorption, where specialized features capture instances of general features creating representation holes, and feature composition, where independent features merge into composite representations. In this work, we introduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these issues by enforcing orthogonality between the learned features. By implementing a new training procedure that penalizes high pairwise cosine similarity between SAE features, OrtSAE promotes the development of disentangled features while scaling linearly with the SAE size, avoiding significant computational overhead. We train OrtSAE across different models and layers and compare it with other methods. We find that OrtSAE discovers 9% more distinct features, reduces feature absorption (by 65%) and composition (by 15%), improves performance on spurious correlation removal (+6%), and achieves on-par performance for other downstream tasks compared to traditional SAEs.",
    "authors": [
      "Anton Korznikov",
      "Andrey Galichin",
      "Alexey Dontsov",
      "Oleg Rogov",
      "Elena Tutubalina",
      "Ivan Oseledets"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "sparse autoencoders",
      "feature absorption",
      "feature composition",
      "orthogonal SAE",
      "OrtSAE",
      "disentangled features",
      "cosine similarity",
      "spurious correlation removal"
    ]
  },
  {
    "title": "LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks\n  for Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework.",
    "authors": [
      "Ci-Siang Lin",
      "Min-Hung Chen",
      "Yu-Yang Sheng",
      "Yu-Chiang Frank Wang"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "MLLMs",
      "out-of-distribution",
      "OOD",
      "medical imaging",
      "labeled data",
      "LEAML",
      "label-efficient adaptation",
      "QA generator",
      "caption distillation",
      "domain-relevant",
      "pseudo question-answer pairs",
      "unlabeled images",
      "selective update",
      "neurons",
      "question-answering",
      "domain-specific knowledge",
      "gastrointestinal endoscopy",
      "sports VQA",
      "standard fine-tuning"
    ]
  },
  {
    "title": "NuRisk: A Visual Question Answering Dataset for Agent-Level Risk\n  Assessment in Autonomous Driving",
    "summary": "Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving.",
    "authors": [
      "Yuan Gao",
      "Mattia Piccinini",
      "Roberto Brusnicki",
      "Yuchen Zhang",
      "Johannes Betz"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Visual Question Answering (VQA)",
      "Vision Language Models (VLMs)",
      "spatio-temporal reasoning",
      "Bird-Eye-View (BEV)",
      "agent-level risk annotations",
      "nuScenes",
      "Waymo",
      "CommonRoad simulator",
      "fine-tuned VLM"
    ]
  },
  {
    "title": "Apriel-1.5-15b-Thinker",
    "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.",
    "authors": [
      "Shruthan Radhakrishna",
      "Aman Tiwari",
      "Aanjaneya Shukla",
      "Masoud Hashemi",
      "Rishabh Maheshwary",
      "Shiva Krishna Reddy Malay",
      "Jash Mehta",
      "Pulkit Pattnaik",
      "Saloni Mittal",
      "Khalil Slimi",
      "Kelechi Ogueji",
      "Akintunde Oladipo",
      "Soham Parikh",
      "Oluwanifemi Bamgbose",
      "Toby Liang",
      "Ahmed Masry",
      "Khyati Mahajan",
      "Sai Rajeswar Mudumba",
      "Vikas Yadav",
      "Sathwik Tejaswi Madhusudhan",
      "Torsten Scholak",
      "Sagar Davasam",
      "Srinivas Sunkara",
      "Nicholas Chapados"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multimodal reasoning model",
      "depth upscaling",
      "staged continual pre-training",
      "synthetic data generation",
      "spatial structure",
      "compositional understanding",
      "fine-grained perception",
      "text-only supervised fine-tuning",
      "reasoning traces",
      "Artificial Analysis Intelligence Index",
      "single-GPU deployment"
    ]
  },
  {
    "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
    "summary": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Neural Radiance Fields",
      "3D Gaussian Splatting",
      "differentiable splatting",
      "triangle parametrization",
      "opaque triangles",
      "mesh-based novel view synthesis",
      "Mip-NeRF360",
      "Tanks & Temples",
      "semi-connected meshes",
      "physics-based simulation",
      "interactive walkthroughs"
    ]
  },
  {
    "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
    "summary": "We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
    "authors": [
      "Ruiyi Wang",
      "Prithviraj Ammanabrolu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multi-turn reinforcement learning",
      "large language models",
      "TextWorld",
      "ALFWorld",
      "SWE-Gym",
      "task complexity",
      "state space",
      "action space",
      "optimal solution length",
      "reward sparsity",
      "dense turn-level rewards",
      "RL algorithms",
      "PPO",
      "GRPO",
      "RLOO",
      "policy gradient methods",
      "Supervised Fine-tuning",
      "SFT",
      "RL training ratio"
    ]
  },
  {
    "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
    "summary": "With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT",
    "authors": [
      "Beomsu Kim",
      "Byunghee Cha",
      "Jong Chul Ye"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "diffusion models",
      "flow matching models",
      "Consistency Models",
      "CMs",
      "diffusion sampling",
      "probability flow ordinary differential equation",
      "PF-ODE",
      "training dynamics",
      "CM tangents",
      "manifold feature distance",
      "MFD",
      "Align Your Tangent",
      "AYT",
      "learned perceptual image patch similarity metric",
      "LPIPS"
    ]
  },
  {
    "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
    "summary": "Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.",
    "authors": [
      "Yinuo Liu",
      "Ruohan Xu",
      "Xilong Wang",
      "Yuqi Jia",
      "Neil Zhenqiang Gong"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "prompt injection attacks",
      "web agents",
      "threat model",
      "malicious text segments",
      "benign text segments",
      "malicious images",
      "benign images",
      "text-based detection",
      "image-based detection"
    ]
  },
  {
    "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
    "summary": "Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.",
    "authors": [
      "Zichen Chen",
      "Jiefeng Chen",
      "Sercan \u00d6. Arik",
      "Misha Sra",
      "Tomas Pfister",
      "Jinsung Yoon"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "LLM agents",
      "metadata analysis",
      "task planning",
      "code generation",
      "self-reflection",
      "token limits",
      "quality-driven refinement"
    ]
  },
  {
    "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
    "summary": "Visual tokens consume substantial computational resources in multi-modal large models (MLLMs), significantly compromising their efficiency. Recent works have attempted to improve efficiency by compressing visual tokens during training, either through modifications to model components or by introducing additional parameters. However, they often overlook the increased learning difficulty caused by such compression, as the model's parameter space struggles to quickly adapt to the substantial perturbations in the feature space induced by token compression. In this work, we propose to develop Efficient MLLMs via Progressive Consistency Distillation (EPIC), a progressive learning framework. Specifically, by decomposing the feature space perturbations introduced by token compression along the token-wise and layer-wise dimensions, we introduce token consistency distillation and layer consistency distillation, respectively, aiming to reduce the training difficulty by leveraging guidance from a teacher model and following a progressive learning trajectory. Extensive experiments demonstrate the superior effectiveness, robustness, and generalization capabilities of our proposed framework.",
    "authors": [
      "Zichen Wen",
      "Shaobo Wang",
      "Yufa Zhou",
      "Junyuan Zhang",
      "Qintong Zhang",
      "Yifeng Gao",
      "Zhaorun Chen",
      "Bin Wang",
      "Weijia Li",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "visual tokens",
      "multi-modal large models",
      "MLLMs",
      "token compression",
      "parameter space",
      "feature space",
      "progressive learning framework",
      "token consistency distillation",
      "layer consistency distillation",
      "teacher model",
      "progressive learning trajectory"
    ]
  },
  {
    "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
    "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research.",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning with verifiable rewards",
      "RLVR",
      "large language models",
      "LLMs",
      "meta-RLVR",
      "Length-aware Sampling for Policy Optimization",
      "LSPO",
      "dynamic sampling",
      "length signals",
      "ablation study"
    ]
  },
  {
    "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
    "summary": "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.",
    "authors": [
      "Shuai Shao",
      "Qihan Ren",
      "Chen Qian",
      "Boyi Wei",
      "Dadi Guo",
      "Jingyi Yang",
      "Xinhao Song",
      "Linfeng Zhang",
      "Weinan Zhang",
      "Dongrui Liu",
      "Jing Shao"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Large Language Models",
      "self-evolving agents",
      "Misevolution",
      "evolutionary pathways",
      "safety alignment",
      "memory accumulation",
      "tool creation",
      "vulnerability introduction"
    ]
  },
  {
    "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs",
    "summary": "Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
    "authors": [
      "Jia Jun Cheng Xian",
      "Muchen Li",
      "Haotian Yang",
      "Xin Tao",
      "Pengfei Wan",
      "Leonid Sigal",
      "Renjie Liao"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "diffusion-based text-to-image",
      "T2I models",
      "reinforcement learning with human feedback",
      "RLHF",
      "Text Preference Optimization",
      "TPO",
      "matched prompts",
      "mismatched prompts",
      "large language model",
      "DPO",
      "KTO",
      "TDPO",
      "TKTO"
    ]
  },
  {
    "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
    "summary": "Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs.",
    "authors": [
      "Ming Zhao",
      "Wenhui Dong",
      "Yang Zhang",
      "Xiang Zheng",
      "Zhonghao Zhang",
      "Zian Zhou",
      "Yunzhi Guan",
      "Liukun Xu",
      "Wei Peng",
      "Zhaoyang Gong",
      "Zhicheng Zhang",
      "Dachuan Li",
      "Xiaosheng Ma",
      "Yuli Ma",
      "Jianing Ni",
      "Changjiang Jiang",
      "Lixia Tian",
      "Qixin Chen",
      "Kaishun Xia",
      "Pingping Liu",
      "Tongshun Zhang",
      "Zhiqiang Liu",
      "Zhongan Bi",
      "Chenyang Si",
      "Tiansheng Sun",
      "Caifeng Shan"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "vertebral-level reasoning",
      "X-ray",
      "CT",
      "MRI",
      "instruction data",
      "spine-specific benchmarks",
      "SpineMed",
      "SpineMed-450k",
      "SpineBench",
      "clinician-in-the-loop pipeline",
      "LLM generation method",
      "large vision-language models (LVLMs)",
      "fine-grained",
      "level-specific reasoning"
    ]
  },
  {
    "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
    "summary": "Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.",
    "authors": [
      "Yisu Wang",
      "Ming Wang",
      "Haoyuan Song",
      "Wenjie Huang",
      "Chaozheng Wang",
      "Yi Xie",
      "Xuming Ran"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "REPAIR",
      "Robust Editing",
      "Progressive Adaptive Intervention",
      "Reintegration",
      "lifelong editing framework",
      "large language models",
      "closed-loop feedback mechanism",
      "dynamic memory management",
      "knowledge fusion",
      "locality guards",
      "knowledge forgetting"
    ]
  },
  {
    "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
    "summary": "Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Gr\\\"onwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
    "authors": [
      "Jiahang Cao",
      "Yize Huang",
      "Hanzhong Guo",
      "Rui Zhang",
      "Mu Nan",
      "Weijian Mai",
      "Jiaxu Wang",
      "Hao Cheng",
      "Jingkai Sun",
      "Gang Han",
      "Wen Zhao",
      "Qiang Zhang",
      "Yijie Guo",
      "Qihao Zheng",
      "Chunfeng Song",
      "Xiao Li",
      "Ping Luo",
      "Andrew F. Luo"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "diffusion-based models",
      "vision-language-action",
      "vision-action",
      "distributional scores",
      "convex composition",
      "Gr\\\"onwall-type bound",
      "General Policy Composition",
      "GPC",
      "pre-trained policies",
      "Robomimic",
      "PushT",
      "RoboTwin",
      "real-world robotic evaluations"
    ]
  },
  {
    "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
    "summary": "Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.",
    "authors": [
      "Imene Kerboua",
      "Sahar Omidi Shayegan",
      "Megh Thakkar",
      "Xing Han L\u00f9",
      "L\u00e9o Boisvert",
      "Massimo Caccia",
      "J\u00e9r\u00e9my Espinas",
      "Alexandre Aussem",
      "V\u00e9ronique Eglin",
      "Alexandre Lacoste"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "large language models",
      "LLM",
      "accessibility tree",
      "AxTree",
      "prompt injection",
      "WorkArena",
      "WebArena",
      "targeted LLM-based retrieval"
    ]
  },
  {
    "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
    "summary": "Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).",
    "authors": [
      "Zhaojun Sun",
      "Xuzhou Zhu",
      "Xuanhe Zhou",
      "Xin Tong",
      "Shuo Wang",
      "Jie Fu",
      "Guoliang Li",
      "Zhiyuan Liu",
      "Fan Wu"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "DeepResearch agents",
      "LLM4Survey",
      "SurveyBench",
      "arXiv papers",
      "high-quality surveys",
      "outline quality",
      "content quality",
      "synthesis granularity",
      "logical coherence",
      "non-textual richness",
      "content-based evaluation",
      "quiz-based evaluation"
    ]
  },
  {
    "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
    "summary": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
    "authors": [
      "Zhiting Mei",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "generative video models",
      "text-to-video",
      "uncertainty quantification",
      "UQ",
      "large language models",
      "LLMs",
      "robust rank correlation estimation",
      "latent modeling",
      "aleatoric uncertainty",
      "epistemic uncertainty",
      "latent space",
      "benchmark video datasets"
    ]
  },
  {
    "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
    "summary": "GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.",
    "authors": [
      "Suyuchen Wang",
      "Tianyu Zhang",
      "Ahmed Masry",
      "Christopher Pal",
      "Spandana Gella",
      "Bang Liu",
      "Perouz Taslakian"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "RULER tokens",
      "Interleaved MRoPE",
      "I-MRoPE",
      "spatial encoding",
      "GUI grounding",
      "ScreenSpot",
      "ScreenSpot-V2",
      "ScreenSpot-Pro"
    ]
  },
  {
    "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
    "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.",
    "authors": [
      "Shijian Deng",
      "Kai Wang",
      "Tianyu Yang",
      "Harsh Singh",
      "Yapeng Tian"
    ],
    "date": "2025-10-03",
    "ai_keywords": [
      "Large Language Models",
      "Multimodal LLMs",
      "self-improvement",
      "data collection",
      "data organization",
      "model optimization",
      "evaluations",
      "downstream applications",
      "open challenges",
      "future research directions"
    ]
  },
  {
    "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
    "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own \"thoughts\" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025).",
    "authors": [
      "Lovish Madaan",
      "Aniket Didolkar",
      "Suchin Gururangan",
      "John Quan",
      "Ruan Silva",
      "Ruslan Salakhutdinov",
      "Manzil Zaheer",
      "Sanjeev Arora",
      "Anirudh Goyal"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "LLMs",
      "long chains of thought",
      "long CoT",
      "self-checking",
      "context length",
      "token/compute cost",
      "answer latency",
      "metacognition",
      "improvement operator",
      "Parallel-Distill-Refine",
      "PDR",
      "bounded",
      "textual workspace",
      "refine",
      "degree of parallelism",
      "Sequential Refinement",
      "SR",
      "Reinforcement Learning",
      "RL",
      "AIME 2024",
      "AIME 2025"
    ]
  },
  {
    "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
    "summary": "Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmark's difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the model's parametric knowledge.",
    "authors": [
      "Yehonatan Peisakhovsky",
      "Zorik Gekhman",
      "Yosi Mass",
      "Liat Ein-Dor",
      "Roi Reichart"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "LLMs",
      "context-grounded hallucinations",
      "meta-evaluation",
      "human annotation",
      "F1 score",
      "prompting strategies",
      "parametric knowledge"
    ]
  },
  {
    "title": "Aristotle: IMO-level Automated Theorem Proving",
    "summary": "We introduce Aristotle, an AI system that combines formal verification with informal reasoning, achieving gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems. Aristotle integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver. Our system demonstrates state-of-the-art performance with favorable scaling properties for automated theorem proving.",
    "authors": [
      "Tudor Achim",
      "Alex Best",
      "Kevin Der",
      "Math\u00efs F\u00e9d\u00e9rico",
      "Sergei Gukov",
      "Daniel Halpern-Leister",
      "Kirsten Henningsgard",
      "Yury Kudryashov",
      "Alexander Meiburg",
      "Martin Michelsen",
      "Riley Patterson",
      "Eric Rodriguez",
      "Laura Scharff",
      "Vikram Shanker",
      "Vladmir Sicca",
      "Hari Sowrirajan",
      "Aidan Swope",
      "Matyas Tamas",
      "Vlad Tenev",
      "Jonathan Thomm",
      "Harold Williams",
      "Lawrence Wu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Lean proof search",
      "formal verification",
      "informal reasoning",
      "lemmas",
      "geometry solver",
      "automated theorem proving"
    ]
  },
  {
    "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
    "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models",
    "authors": [
      "Adam Filipek"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Transformer architecture",
      "Multi-Head Attention (MHA)",
      "Multi-Query Attention (MQA)",
      "Grouped-Query Attention (GQA)",
      "Sparse Query Attention (SQA)",
      "floating-point operations (FLOPs)",
      "attention score computation",
      "Reactive Transformer architecture"
    ]
  },
  {
    "title": "Controlled Generation for Private Synthetic Text",
    "summary": "Text anonymization is essential for responsibly developing and deploying AI in high-stakes domains such as healthcare, social services, and law. In this work, we propose a novel methodology for privacy-preserving synthetic text generation that leverages the principles of de-identification and the Hiding In Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes to guide controllable generation using either in-context learning (ICL) or prefix tuning. The ICL variant ensures privacy levels consistent with the underlying de-identification system, while the prefix tuning variant incorporates a custom masking strategy and loss function to support scalable, high-quality generation. Experiments on legal and clinical datasets demonstrate that our method achieves a strong balance between privacy protection and utility, offering a practical and effective solution for synthetic text generation in sensitive domains.",
    "authors": [
      "Zihao Zhao",
      "Anjalie Field"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "de-identification",
      "Hiding In Plain Sight (HIPS)",
      "entity-aware control codes",
      "in-context learning (ICL)",
      "prefix tuning",
      "custom masking strategy",
      "loss function"
    ]
  },
  {
    "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
    "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
    "authors": [
      "Joykirat Singh",
      "Justin Chih-Yao Chen",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Akshay Nambi",
      "Mohit Bansal"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "self-attention",
      "long reasoning trajectory",
      "adaptive",
      "attentive compression",
      "under-adaptivity",
      "reasoning budget",
      "task-difficulty calibration",
      "TRAAC",
      "Qwen3-4B",
      "AIME",
      "AMC",
      "GPQA-D",
      "BBEH",
      "OptimalThinkingBench"
    ]
  },
  {
    "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
    "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the \"battle\" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.",
    "authors": [
      "Raphael Tang",
      "Crystina Zhang",
      "Wenyan Li",
      "Carmen Lai",
      "Pontus Stenetorp",
      "Yao Lu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "arena-style evaluation",
      "large language models",
      "Elo rating system",
      "battle outcome prediction",
      "query difficulty",
      "query properties"
    ]
  },
  {
    "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient",
    "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.",
    "authors": [
      "Rui Ming",
      "Haoyuan Wu",
      "Shoubo Hu",
      "Zhuolun He",
      "Bei Yu"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "supervised fine-tuning",
      "large language models",
      "reinforcement learning",
      "one-token rollout",
      "policy gradient method",
      "autoregressive learning",
      "Monte Carlo rollout",
      "on-policy data",
      "off-policy data",
      "generalization",
      "mathematical reasoning",
      "code generation",
      "domain reasoning"
    ]
  },
  {
    "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
    "summary": "Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.",
    "authors": [
      "Nima Sheikholeslami",
      "Erfan Hosseini",
      "Patrice Bechard",
      "Srivatsava Daruru",
      "Sai Rajeswar"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Noise Contrastive Estimation",
      "Contrastive Loss",
      "Mann-Whitney U statistic",
      "Area under the ROC Curve",
      "AUC",
      "binary cross entropy",
      "retrieval-augmented generation",
      "RAG"
    ]
  },
  {
    "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
    "summary": "Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available at https://huggingface.co/ModernVBERT.",
    "authors": [
      "Paul Teiletche",
      "Quentin Mac\u00e9",
      "Max Conti",
      "Antonio Loison",
      "Gautier Viaud",
      "Pierre Colombo",
      "Manuel Faysse"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multimodal embedding models",
      "document retrieval",
      "vision-language decoders",
      "contrastive losses",
      "attention masking",
      "image resolution",
      "modality alignment",
      "contrastive objectives",
      "vision-language encoder"
    ]
  },
  {
    "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
    "summary": "Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.",
    "authors": [
      "Anton Korznikov",
      "Andrey Galichin",
      "Alexey Dontsov",
      "Oleg Y. Rogov",
      "Ivan Oseledets",
      "Elena Tutubalina"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "activation steering",
      "LLM behavior",
      "semantically meaningful vectors",
      "hidden states",
      "model alignment safeguards",
      "harmful requests",
      "sparse autoencoder",
      "universal attack"
    ]
  },
  {
    "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
    "summary": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.",
    "authors": [
      "Ningyuan Yang",
      "Guanliang Lyu",
      "Mingchen Ma",
      "Yiyi Lu",
      "Yiming Li",
      "Zhihui Gao",
      "Hancheng Ye",
      "Jianyi Zhang",
      "Tingjun Chen",
      "Yiran Chen"
    ],
    "date": "2025-09-25",
    "ai_keywords": [
      "Large Language Models",
      "Internet-of-Things",
      "Model Context Protocol",
      "edge-deployed servers",
      "IoT-MCP",
      "IoT-MCP Bench",
      "Basic Tasks",
      "Complex Tasks",
      "sensor types",
      "microcontroller units",
      "tool calls",
      "response time",
      "memory footprint"
    ]
  },
  {
    "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
    "summary": "Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.",
    "authors": [
      "Yuxiao Qu",
      "Anikait Singh",
      "Yoonho Lee",
      "Amrith Setlur",
      "Ruslan Salakhutdinov",
      "Chelsea Finn",
      "Aviral Kumar"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "reinforcement learning",
      "reasoning abstractions",
      "procedural knowledge",
      "factual knowledge",
      "abstraction generator",
      "solution generator",
      "RLAD",
      "structured exploration",
      "generalization"
    ]
  },
  {
    "title": "Generalized Parallel Scaling with Interdependent Generations",
    "summary": "Parallel LLM inference scaling involves sampling a set of N>1 responses for a single input prompt. However, these N parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.",
    "authors": [
      "Harry Dong",
      "David Brandfonbrener",
      "Eryk Helenowski",
      "Yun He",
      "Mrinal Kumar",
      "Han Fang",
      "Yuejie Chi",
      "Karthik Abinav Sankararaman"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "LLM",
      "parallel inference",
      "response length scaling",
      "batched LLM hidden states",
      "holistic tensors",
      "reinforcement learning",
      "verifiable rewards",
      "generation width",
      "post-generation aggregation"
    ]
  },
  {
    "title": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking\n  for Training-free Zero-Shot Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from a vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the user's intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in a single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained, demonstrating its potential applicability.",
    "authors": [
      "Ren-Di Wu",
      "Yu-Yen Lin",
      "Huei-Fang Yang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "MLLMs",
      "vision-language model",
      "VLM",
      "CLIP",
      "Semantic Query-Augmented Fusion",
      "SQAF",
      "Efficient Batch Reranking",
      "EBR",
      "joint visual-semantic reasoning",
      "Composed Image Retrieval",
      "CIR"
    ]
  },
  {
    "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "summary": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.",
    "authors": [
      "Enxin Song",
      "Wenhao Chai",
      "Shusheng Yang",
      "Ethan Armand",
      "Xiaojun Shan",
      "Haiyang Xu",
      "Jianwen Xie",
      "Zhuowen Tu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Native Sparse Attention",
      "NSA",
      "VideoNSA",
      "Qwen2.5-VL",
      "token-compression",
      "sparse baselines",
      "long-video understanding",
      "temporal reasoning",
      "spatial benchmarks",
      "global-local attention",
      "dynamic attention sinks"
    ]
  },
  {
    "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
    "summary": "Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.",
    "authors": [
      "Ruohao Guo",
      "Afshin Oroojlooy",
      "Roshan Sridhar",
      "Miguel Ballesteros",
      "Alan Ritter",
      "Dan Roth"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "adversarial attacks",
      "multi-turn interaction",
      "reinforcement learning",
      "tree search",
      "sequential decision-making",
      "dialogue policies",
      "attack success rate"
    ]
  },
  {
    "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
    "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.",
    "authors": [
      "Xinlei Yu",
      "Chengming Xu",
      "Guibin Zhang",
      "Yongbo He",
      "Zhangquan Chen",
      "Zhucun Xue",
      "Jiangning Zhang",
      "Yue Liao",
      "Xiaobin Hu",
      "Yu-Gang Jiang",
      "Shuicheng Yan"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Multi-Agent System",
      "Visual Language Models",
      "visual hallucination snowballing",
      "turn-wise attention",
      "layer-wise attention",
      "token-wise attention",
      "visual attention allocation",
      "vision tokens",
      "unimodal attention peak",
      "Visual Flow",
      "attention reallocation"
    ]
  },
  {
    "title": "Learning to Reason for Hallucination Span Detection",
    "summary": "Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.",
    "authors": [
      "Hsuan Su",
      "Ting-Yao Hu",
      "Hema Swetha Koppula",
      "Kundan Krishna",
      "Hadi Pouransari",
      "Cheng-Yu Hsieh",
      "Cem Koc",
      "Joseph Yitan Cheng",
      "Oncel Tuzel",
      "Raviteja Vemulapalli"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Chain-of-Thought",
      "reinforcement learning",
      "span-level reward function",
      "Group Relative Policy Optimization",
      "Class-Aware Policy Optimization",
      "RAGTruth benchmark",
      "summarization",
      "question answering",
      "data-to-text",
      "hallucination span detection"
    ]
  },
  {
    "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
    "summary": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .",
    "authors": [
      "Yu Zeng",
      "Wenxuan Huang",
      "Shiting Huang",
      "Xikun Bao",
      "Yukun Qi",
      "Yiming Zhao",
      "Qiuchen Wang",
      "Lin Chen",
      "Zehui Chen",
      "Huaian Chen",
      "Wanli Ouyang",
      "Feng Zhao"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Vision-Language Models",
      "VLMs",
      "jigsaw tasks",
      "perceptual capabilities",
      "reasoning capabilities",
      "AGILE",
      "Agentic jiGsaw Interaction Learning",
      "executable code",
      "fine-grained visual feedback",
      "multimodal understanding",
      "multimodal reasoning",
      "multimodal reinforcement learning"
    ]
  },
  {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
    "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Neural Radiance Fields",
      "3D Gaussian Splatting",
      "image-level poisoning attacks",
      "density-guided poisoning",
      "Gaussian points",
      "Kernel Density Estimation",
      "viewpoint-dependent illusory objects",
      "multi-view consistency",
      "KDE-based evaluation protocol"
    ]
  },
  {
    "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
    "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.",
    "authors": [
      "Sicheng Feng",
      "Kaiwen Tuo",
      "Song Wang",
      "Lingdong Kong",
      "Jianke Zhu",
      "Huan Wang"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "ReasonMap",
      "ReasonMap-Plus",
      "Visual Question Answering (VQA)",
      "reinforcement learning (RL)",
      "sparse rewards",
      "unstable optimization",
      "difficulty-aware reward design",
      "multi-stage RL",
      "cold-start training",
      "Supervised Fine-Tuning (SFT)",
      "spatial reasoning",
      "fine-grained visual reasoning"
    ]
  },
  {
    "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
    "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.",
    "authors": [
      "Eric Tillmann Bill",
      "Enis Simsar",
      "Thomas Hofmann"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "flow matching",
      "stochastic optimal control",
      "subject disentanglement",
      "training-free test-time controller",
      "Adjoint Matching",
      "flow-diffusion correspondence",
      "fine-tuning",
      "multi-subject alignment",
      "Stable Diffusion",
      "FLUX",
      "Stable Diffusion XL",
      "FOCUS"
    ]
  },
  {
    "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
    "summary": "As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.",
    "authors": [
      "Nandan Kumar Jha",
      "Brandon Reagen"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "feed-forward networks",
      "FFNs",
      "Hard Rank",
      "Soft Rank",
      "Spectral Concentration",
      "Spectral Utilization Index",
      "SUI",
      "latent space",
      "spectral scaling law",
      "dominant-mode subspaces",
      "tail capacity",
      "dominant-mode capacity",
      "inference-efficient LLM design"
    ]
  },
  {
    "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
    "summary": "Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.",
    "authors": [
      "Haokun Zhao",
      "Xiang Zhang",
      "Jiaqi Wei",
      "Yiwei Xu",
      "Yuting He",
      "Siqi Sun",
      "Chenyu You"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "LLM-driven",
      "agentic framework",
      "LLM-guided diagnostics",
      "multi-modal diagnostics",
      "self-planning",
      "model fitting",
      "validation",
      "ensemble strategy",
      "white-box system",
      "interpretable",
      "extensible"
    ]
  },
  {
    "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
    "summary": "Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi",
    "authors": [
      "Chetwin Low",
      "Weimin Wang",
      "Calder Katyal"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "twin-DiT modules",
      "blockwise cross-modal fusion",
      "scaled-RoPE embeddings",
      "bidirectional cross-attention",
      "cinematic storytelling",
      "movie-grade video clips"
    ]
  },
  {
    "title": "ExGRPO: Learning to Reason from Experience",
    "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
    "authors": [
      "Runzhe Zhan",
      "Yafu Li",
      "Zhi Wang",
      "Xiaoye Qu",
      "Dongrui Liu",
      "Jing Shao",
      "Derek F. Wong",
      "Yu Cheng"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "reinforcement learning from verifiable rewards",
      "RLVR",
      "on-policy training",
      "rollout experiences",
      "experience characteristics",
      "rollout correctness",
      "entropy",
      "ExGRPO",
      "Experiential Group Relative Policy Optimization",
      "mixed-policy objective",
      "exploration",
      "experience exploitation",
      "reasoning performance",
      "mathematical benchmarks",
      "general benchmarks"
    ]
  },
  {
    "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
    "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.",
    "authors": [
      "Harold Haodong Chen",
      "Xianfeng Wu",
      "Wen-Jie Shu",
      "Rongjin Guo",
      "Disen Lan",
      "Harry Yang",
      "Ying-Cong Chen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "test-time scaling",
      "next-token prediction",
      "autoregressive image generation",
      "token entropy",
      "profile level",
      "policy level",
      "GenEval",
      "TIIF-Bench",
      "visual token consumption",
      "robustness"
    ]
  },
  {
    "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
    "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
    "authors": [
      "Wen Yang",
      "Junhong Wu",
      "Chong Li",
      "Chengqing Zong",
      "Jiajun Zhang"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Reinforcement Post-Training",
      "Large Reasoning Models",
      "cross-linguistic perspective",
      "multilingual reasoning benchmarks",
      "cross-lingual transferability",
      "interventional studies",
      "parallel training",
      "Parallel Leap",
      "Parallel Scaling Law",
      "Monolingual Generalization Gap"
    ]
  },
  {
    "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
    "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
    "authors": [
      "Chaorui Yao",
      "Yanxi Chen",
      "Yuchang Sun",
      "Yushuo Chen",
      "Wenhao Zhang",
      "Xuchen Pan",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "off-policy reinforcement learning",
      "large language models",
      "REINFORCE",
      "Group Relative Policy Optimization",
      "GRPO",
      "Online Policy Mirror Descent",
      "OPMD",
      "Asymmetric REINFORCE",
      "AsymRE",
      "importance sampling",
      "clipping",
      "data-weighting strategies"
    ]
  },
  {
    "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
    "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.",
    "authors": [
      "Gonzalo Gonzalez-Pumariega",
      "Vincent Tu",
      "Chih-Lun Lee",
      "Jiachen Yang",
      "Ang Li",
      "Xin Eric Wang"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Behavior Best-of-N",
      "bBoN",
      "rollouts",
      "behavior narratives",
      "trajectory selection",
      "OSWorld",
      "WindowsAgentArena",
      "AndroidWorld"
    ]
  },
  {
    "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
    "authors": [
      "Angen Ye",
      "Zeyu Zhang",
      "Boyuan Wang",
      "Xiaofeng Wang",
      "Dapeng Zhang",
      "Zheng Zhu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "VLA models",
      "Reinforcement Learning from Verifiable Rewards (RLVR)",
      "Group Relative Policy Optimization (GRPO)",
      "region alignment",
      "trajectory consistency",
      "output formatting",
      "chain-of-thought supervision",
      "affordance",
      "trajectory annotations"
    ]
  },
  {
    "title": "RLP: Reinforcement as a Pretraining Objective",
    "summary": "The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
    "authors": [
      "Ali Hatamizadeh",
      "Syeda Nahida Akter",
      "Shrimai Prabhumoye",
      "Jan Kautz",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Yejin Choi"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "reinforcement learning",
      "pre-training",
      "next-token prediction",
      "chain-of-thought",
      "information gain",
      "log-likelihood",
      "dense reward signal",
      "Qwen3-1.7B-Base",
      "AIME25",
      "MMLU-Pro",
      "Nemotron-Nano-12B-v2",
      "scientific reasoning"
    ]
  },
  {
    "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
    "summary": "Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Yuxuan Zhang",
      "Yi Lu",
      "Tianle Gu",
      "Lingyu Li",
      "Dingyi Zhao",
      "Keming Wu",
      "Haozhe Wang",
      "Ping Nie",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Deep Research Agents",
      "task decomposition",
      "cross-source retrieval",
      "multi-stage reasoning",
      "structured output",
      "benchmark",
      "evaluation framework",
      "thematic domains",
      "reference bundles",
      "semantic quality",
      "topical focus",
      "retrieval trustworthiness"
    ]
  },
  {
    "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
    "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness.",
    "authors": [
      "Zefeng He",
      "Xiaoye Qu",
      "Yafu Li",
      "Siyuan Huang",
      "Daizong Liu",
      "Yu Cheng"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Large Vision-Language Models",
      "LVLMs",
      "video understanding",
      "long video reasoning",
      "frame sampling",
      "textual reasoning",
      "FrameThinker",
      "iterative interrogation",
      "video reasoning capabilities",
      "Supervised Fine-Tuning",
      "SFT",
      "Reinforcement Learning",
      "RL",
      "reward functions",
      "Video-Holmes",
      "LongVideo-Reason",
      "LongVideoBench",
      "MLVU",
      "VideoMME",
      "LVBench",
      "LongVILA-R1"
    ]
  },
  {
    "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
    "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
    "authors": [
      "Erfan Shayegani",
      "Keegan Hines",
      "Yue Dong",
      "Nael Abu-Ghazaleh",
      "Roman Lutz",
      "Spencer Whitehead",
      "Vidhisha Balachandran",
      "Besmira Nushi",
      "Vibhav Vineet"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Blind Goal-Directedness",
      "contextual reasoning",
      "ambiguity",
      "contradictory goals",
      "benchmark",
      "OSWorld",
      "LLM-based judges",
      "execution-first bias",
      "thought-action disconnect",
      "request-primacy"
    ]
  },
  {
    "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
    "summary": "Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.",
    "authors": [
      "Seongjae Kang",
      "Dong Bok Lee",
      "Juho Jung",
      "Dongseop Kim",
      "Won Hwa Kim",
      "Sunghoon Joo"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "structured radiology report generation",
      "chest X-ray images",
      "clinical context",
      "temporal hallucinations",
      "multimodal large language models",
      "multi-view X-ray images",
      "clinical indication",
      "imaging techniques",
      "prior studies",
      "patient histories"
    ]
  },
  {
    "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
    "summary": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
    "authors": [
      "Yanxu Chen",
      "Zijun Yao",
      "Yantao Liu",
      "Jin Ye",
      "Jianing Yu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "large language models",
      "LLM agents",
      "reasoning",
      "tool use",
      "sequential decision-making",
      "software engineering",
      "scientific discovery",
      "finance domain",
      "financial benchmarks",
      "question answering",
      "dynamic trading",
      "contamination-free benchmark",
      "daily market signals",
      "prices",
      "fundamentals",
      "news",
      "sequential buy",
      "sell",
      "hold decisions",
      "cumulative return",
      "maximum drawdown",
      "Sortino ratio",
      "GPT-5",
      "Claude-4",
      "Qwen3",
      "Kimi-K2",
      "GLM-4.5",
      "buy-and-hold baseline",
      "static financial knowledge tasks",
      "trading strategies"
    ]
  },
  {
    "title": "LongCodeZip: Compress Long Context for Code Language Models",
    "summary": "Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
    "authors": [
      "Yuling Shi",
      "Yichun Qian",
      "Hongyu Zhang",
      "Beijun Shen",
      "Xiaodong Gu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Large Language Models",
      "code LLMs",
      "context pruning",
      "LLMLingua",
      "conditional perplexity",
      "function-level chunks",
      "fine-grained compression",
      "token budget",
      "code completion",
      "code summarization",
      "question answering",
      "code intelligence applications"
    ]
  },
  {
    "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
    "summary": "Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.",
    "authors": [
      "Hu Wei",
      "Ze Xu",
      "Boyu Yang",
      "Linlin Miao",
      "Weiqi Zhai",
      "Yihan Li",
      "Zixuan Li",
      "Zhijun Wang",
      "Boya Wang",
      "Jianwei Yu",
      "Jialing Yuan",
      "Xiaoyue Zhang",
      "Cheng He",
      "Minglei Chen",
      "Zifan Zhang",
      "Qianhui Li",
      "Wei Wang",
      "Xiang Xu"
    ],
    "date": "2025-09-24",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "SKYLENAGE-ReasoningMATH",
      "SKYLENAGE-MATH",
      "structure-aware diagnostic set",
      "numeric density",
      "symbolic complexity",
      "contest-style suite",
      "subject x model performance",
      "grade x model performance",
      "mathematical reasoning"
    ]
  },
  {
    "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce VOGUE (Visual Uncertainty Guided Exploration), a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.",
    "authors": [
      "Rui Liu",
      "Dian Yu",
      "Tong Zheng",
      "Runpeng Dai",
      "Zongxia Li",
      "Wenhao Yu",
      "Zhenwen Liang",
      "Linfeng Song",
      "Haitao Mi",
      "Pratap Tokekar",
      "Dong Yu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning with verifiable rewards",
      "multimodal LLMs",
      "visual uncertainty",
      "stochastic context",
      "symmetric KL divergence",
      "uncertainty-aware exploration",
      "token-entropy bonus",
      "annealed sampling schedule",
      "GRPO",
      "pass@1 accuracy",
      "pass@4 performance",
      "exploration decay"
    ]
  },
  {
    "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
    "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
    "authors": [
      "Ziyin Zhang",
      "Zihan Liao",
      "Hang Yu",
      "Peng Di",
      "Rui Wang"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "embedding models",
      "foundation models",
      "contrastive pretraining",
      "training pipelines",
      "synthetic training data",
      "query-document-negative tuples",
      "MTEB English leaderboard",
      "parameter-efficient fine-tuning"
    ]
  },
  {
    "title": "Rethinking the shape convention of an MLP",
    "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
    "authors": [
      "Meng-Hsi Chen",
      "Yu-Ang Lee",
      "Feng-Ting Liao",
      "Da-shan Shiu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "multi-layer perceptrons",
      "MLPs",
      "wide-narrow-wide",
      "Hourglass",
      "skip connections",
      "residual computation",
      "parameter-matched designs",
      "random initialization",
      "generative tasks",
      "Pareto frontiers",
      "architectural search",
      "Transformers",
      "residual networks"
    ]
  },
  {
    "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
    "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.",
    "authors": [
      "Jiyao Liu",
      "Jinjie Wei",
      "Wanying Qu",
      "Chenglong Ma",
      "Junzhi Ning",
      "Yunheng Li",
      "Ying Chen",
      "Xinzhe Luo",
      "Pengcheng Chen",
      "Xin Gao",
      "Ming Hu",
      "Huihui Xu",
      "Xin Wang",
      "Shujian Gao",
      "Dingkang Yang",
      "Zhongying Deng",
      "Jin Ye",
      "Lihao Liu",
      "Junjun He",
      "Ningsheng Xu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Multi-modal Large Language Models",
      "MedQ-Bench",
      "MedQ-Perception",
      "MedQ-Reasoning",
      "perception-reasoning paradigm",
      "human-curated questions",
      "visual attributes",
      "no-reference reasoning",
      "comparison reasoning",
      "multi-dimensional judging protocol",
      "human-AI alignment validation"
    ]
  },
  {
    "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
    "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.",
    "authors": [
      "Zhangchen Xu",
      "Adriana Meza Soria",
      "Shawn Tan",
      "Anurag Roy",
      "Ashish Sunil Agrawal",
      "Radha Poovendran",
      "Rameswar Panda"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Large Language Model (LLM)",
      "tool-agentic training data",
      "Model Context Protocols (MCPs)",
      "tool-use queries",
      "model-based quality filtering",
      "agentic trajectories",
      "teacher models",
      "agentic frameworks",
      "BFCL V3 benchmark",
      "MCP-Universe Bench"
    ]
  },
  {
    "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
    "summary": "Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
    "authors": [
      "Wei-Yao Wang",
      "Kazuya Tateishi",
      "Qiyu Wu",
      "Shusuke Takahashi",
      "Yuki Mitsufuji"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multimodal representation learning",
      "vision-language models",
      "visual-interactive capabilities",
      "segmentation model",
      "visual prompts",
      "entity-level information",
      "global representations",
      "Segmentation-and-Scene Caption Retrieval",
      "MMEB",
      "SCaR"
    ]
  },
  {
    "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
    "summary": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.",
    "authors": [
      "Zihan Zhou",
      "Shilin Lu",
      "Shuli Leng",
      "Shaocong Zhang",
      "Zhuming Lian",
      "Xinlei Yu",
      "Adams Wai-Kin Kong"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "DragFlow",
      "FLUX",
      "flow matching",
      "DiT",
      "UNet",
      "DDPMs",
      "affine transformations",
      "pretrained open-domain personalization adapters",
      "IP-Adapter",
      "gradient mask-based hard constraints",
      "multimodal large language models",
      "MLLMs",
      "Region-based Dragging benchmark",
      "ReD Bench",
      "DragBench-DR"
    ]
  },
  {
    "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
    "summary": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
    "authors": [
      "Justin Cui",
      "Jie Wu",
      "Ming Li",
      "Tao Yang",
      "Xiaojie Li",
      "Rui Wang",
      "Andrew Bai",
      "Yuanhao Ban",
      "Cho-Jui Hsieh"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "diffusion models",
      "transformer architectures",
      "autoregressive formulations",
      "bidirectional teachers",
      "latent space",
      "quality degradation",
      "temporal consistency",
      "position embedding"
    ]
  },
  {
    "title": "Transformers Discover Molecular Structure Without Graph Priors",
    "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinatesx2013without predefined graphs or physical priorsx2013can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patternsx2013such as attention weights that decay inversely with interatomic distancex2013and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",
    "authors": [
      "Tobias Kreiman",
      "Yutong Bai",
      "Fadi Atieh",
      "Elizabeth Weaver",
      "Eric Qu",
      "Aditi S. Krishnapriyan"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Graph Neural Networks",
      "GNNs",
      "molecular machine learning",
      "molecular property prediction",
      "machine learning interatomic potentials",
      "MLIPs",
      "message passing",
      "predefined graphs",
      "fixed radius cutoff",
      "k-nearest neighbor",
      "receptive field",
      "inference",
      "sparse graph operations",
      "Transformers",
      "Cartesian coordinates",
      "physical priors",
      "energy and force mean absolute errors",
      "OMol25 dataset",
      "attention weights",
      "interatomic distance",
      "molecular environments",
      "hard-coded biases",
      "scaling training resources",
      "empirical scaling laws",
      "standardized architectures",
      "molecular modeling"
    ]
  },
  {
    "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
    "summary": "Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.",
    "authors": [
      "Wentao Zhang",
      "Yang Young Lu",
      "Yuntian Deng"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Interactive Training",
      "control server",
      "optimizer hyperparameters",
      "training data",
      "model checkpoints",
      "training stability",
      "sensitivity to initial hyperparameters",
      "adaptability",
      "AI agents",
      "training logs",
      "training dynamics"
    ]
  },
  {
    "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
    "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).",
    "authors": [
      "Zhenwen Liang",
      "Ruosen Li",
      "Yujun Zhou",
      "Linfeng Song",
      "Dian Yu",
      "Xinya Du",
      "Haitao Mi",
      "Dong Yu"
    ],
    "date": "2025-10-02",
    "ai_keywords": [
      "Large Language Model",
      "hidden states",
      "token embeddings",
      "semantic features",
      "lexical features",
      "output logits",
      "confidence-related information",
      "Clue",
      "Clustering and Experience-based Verification",
      "nearest-centroid distance",
      "AIME",
      "GPQA"
    ]
  },
  {
    "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance",
    "summary": "Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.",
    "authors": [
      "Tong Chen",
      "Yinuo Zhang",
      "Pranam Chatterjee"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Rectified Discrete Flows",
      "Tchebycheff scalarization",
      "locally balanced proposals",
      "annealed Metropolis-Hastings updates",
      "Pareto front",
      "peptide design",
      "SMILES sequence design",
      "affinity",
      "solubility",
      "hemolysis",
      "half-life",
      "non-fouling"
    ]
  },
  {
    "title": "Pay-Per-Search Models are Abstention Models",
    "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.   We run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leveraged for making abstention decisions.",
    "authors": [
      "Mustafa Omer Gul",
      "Claire Cardie",
      "Tanya Goyal"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "LLMs",
      "parametric knowledge boundaries",
      "hallucination",
      "selective help-seeking",
      "reinforcement learning",
      "pay-per-search reward",
      "knowledge-intensive QA datasets",
      "multi-hop datasets",
      "answer accuracy",
      "off-the-shelf abstention",
      "specialized abstention approaches"
    ]
  },
  {
    "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
    "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.",
    "authors": [
      "Yue Wang",
      "Ruotian Ma",
      "Xingyu Chen",
      "Zhengliang Shi",
      "Wanshun Chen",
      "Huang Liu",
      "Jiadi Yao",
      "Qu Yang",
      "Qingxuan Jiang",
      "Fanghua Ye",
      "Juntao Li",
      "Min Zhang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Linus"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Large Language Models",
      "multimodel models",
      "speech synthesis",
      "instruction-following capabilities",
      "Text-to-Speech",
      "operationalism",
      "BatonVoice",
      "BatonTTS",
      "vocal features",
      "controllable speech synthesis",
      "emotional speech synthesis",
      "zero-shot cross-lingual generalization"
    ]
  },
  {
    "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
    "summary": "In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256times256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.",
    "authors": [
      "Bowei Chen",
      "Sai Bi",
      "Hao Tan",
      "He Zhang",
      "Tianyuan Zhang",
      "Zhengqi Li",
      "Yuanjun Xiong",
      "Jianming Zhang",
      "Kai Zhang"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "visual encoders",
      "tokenizers",
      "latent diffusion models",
      "semantic latent space",
      "adapter",
      "decoder",
      "semantic preservation loss",
      "perceptual details",
      "high-level semantics",
      "gFID",
      "ImageNet",
      "LAION",
      "FLUX VAE",
      "classifier-free guidance"
    ]
  },
  {
    "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality\n  Instruction and Reasoning Data Built from Permissive-First Text Sources",
    "summary": "We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae",
    "authors": [
      "Huu Nguyen",
      "Victor May",
      "Harsh Raj",
      "Marianna Nezhurina",
      "Yishan Wang",
      "Yanqi Luo",
      "Minh Chien Vu",
      "Taishi Nakamura",
      "Ken Tsui",
      "Van Khue Nguyen",
      "David Salinas",
      "Aleksandra Krasnod\u0119bska",
      "Christoph Schuhmann",
      "Mats Leon Richter",
      "Xuan-Son",
      "Vu",
      "Jenia Jitsev"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "pretraining corpus",
      "risk-mitigated sourcing strategy",
      "public-domain",
      "permissively licensed text",
      "CC-BY",
      "Apache",
      "government works",
      "EU TDM-eligible sources",
      "targeted instruction",
      "reasoning",
      "synthetic data",
      "license-aware filtering",
      "safety screening",
      "quality screening",
      "domain-aware mixing",
      "open-sci-ref training protocol",
      "LLMs",
      "FineWeb-Edu",
      "DCLM",
      "math/code",
      "QA tasks"
    ]
  },
  {
    "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic\n  Tasks",
    "summary": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
    "authors": [
      "Yue Meng",
      "Fei Chen",
      "Chuchu Fan"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Signal Temporal Logic (STL)",
      "Reinforcement Learning (RL)",
      "timed subgoals",
      "invariant constraints",
      "hierarchical framework",
      "time-conditioned policy",
      "dense",
      "stage-wise reward signal",
      "Metropolis-Hastings sampling",
      "temporal search",
      "task success rate"
    ]
  },
  {
    "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
    "summary": "Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets.",
    "authors": [
      "Keming Wu",
      "Sicong Jiang",
      "Max Ku",
      "Ping Nie",
      "Minghao Liu",
      "Wenhu Chen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "reward model",
      "human preference dataset",
      "instruction-guided image editing",
      "GenAI-Bench",
      "AURORA-Bench",
      "ImagenHub",
      "reinforcement learning",
      "post-training",
      "test-time scaling"
    ]
  },
  {
    "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free\n  Low-Precision LLM Weights",
    "summary": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.",
    "authors": [
      "Lorenz K. M\u00fcller",
      "Philippe Bich",
      "Jiawei Zhuang",
      "Ahmet \u00c7elik",
      "Luca Benfenati",
      "Lukas Cavigelli"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "post-training quantization",
      "large language models",
      "perplexity",
      "calibration-free",
      "uniform quantization",
      "second-axis scale factor",
      "Sinkhorn-Knopp-style algorithm",
      "matrix imbalance",
      "Qwen3",
      "DeepSeek-V2.5",
      "WikiText2",
      "C4",
      "non-uniform quantization levels"
    ]
  },
  {
    "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
    "summary": "Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.",
    "authors": [
      "Cong Yu",
      "Valter Uotila",
      "Shilong Deng",
      "Qingyuan Wu",
      "Tuo Shi",
      "Songlin Jiang",
      "Lei You",
      "Bo Zhao"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "quantum circuits",
      "parameterized quantum gates",
      "large language model",
      "reinforcement learning",
      "quantum circuit verification",
      "hierarchical reward mechanism",
      "Pass@1",
      "Pass@10",
      "GPT-4o",
      "GPT-5",
      "DeepSeek-V3",
      "supervised-fine-tuning",
      "RL-only"
    ]
  },
  {
    "title": "It Takes Two: Your GRPO Is Secretly DPO",
    "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.",
    "authors": [
      "Yihong Wu",
      "Liheng Ma",
      "Lei Ding",
      "Muzhi Li",
      "Xinyu Wang",
      "Kejia Chen",
      "Zhan Su",
      "Zhanguang Zhang",
      "Chenyang Huang",
      "Yingxue Zhang",
      "Mark Coates",
      "Jian-Yun Nie"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning",
      "Large Language Models",
      "Group Relative Policy Optimization",
      "contrastive learning",
      "Direct Preference Optimization",
      "rollouts"
    ]
  },
  {
    "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
    "summary": "Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.",
    "authors": [
      "Zhaoyang Li",
      "Dongjun Qian",
      "Kai Su",
      "Qishuai Diao",
      "Xiangyang Xia",
      "Chang Liu",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Zehuan Yuan"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Diffusion Transformer",
      "BindWeave",
      "MLLM-DiT",
      "multimodal large language model",
      "deep cross-modal reasoning",
      "subject-aware hidden states",
      "subject consistency",
      "naturalness",
      "text relevance",
      "OpenS2V benchmark"
    ]
  },
  {
    "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
    "summary": "Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.",
    "authors": [
      "Alexander Kovrigin",
      "Aleksandra Eliseeva",
      "Konstantin Grotov",
      "Egor Bogomolov",
      "Yaroslav Zharov"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "supervised fine-tuning",
      "Reinforcement Learning with Verifiable Rewards",
      "RLVR",
      "Bash scripts",
      "EnvBench-Python",
      "Qwen3-8B",
      "Qwen3-32B",
      "GPT-4o"
    ]
  },
  {
    "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
    "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.",
    "authors": [
      "Marco Bronzini",
      "Carlo Nicolini",
      "Bruno Lepri",
      "Jacopo Staiano",
      "Andrea Passerini"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Large Language Models",
      "Hyperdimensional Probe",
      "Vector Symbolic Architectures",
      "direct logit attribution",
      "sparse autoencoders",
      "residual stream",
      "next-token prediction",
      "syntactic pattern recognition",
      "key-value associations",
      "abstract inference",
      "question-answering",
      "neural probing",
      "symbolic representations"
    ]
  },
  {
    "title": "VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs",
    "summary": "Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.",
    "authors": [
      "Peng Liu",
      "Haozhan Shen",
      "Chunxin Fang",
      "Zhicheng Sun",
      "Jiajia Liao",
      "Tiancheng Zhao"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Vision-Language Models",
      "VLM-FO1",
      "Hybrid Fine-grained Region Encoder",
      "HFRE",
      "dual vision encoder",
      "region tokens",
      "token-based referencing system",
      "object grounding",
      "region generational understanding",
      "visual region reasoning",
      "two-stage training strategy"
    ]
  },
  {
    "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
    "summary": "Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.",
    "authors": [
      "Yongcheng Zeng",
      "Zexu Sun",
      "Bokai Ji",
      "Erxue Min",
      "Hengyi Cai",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Haifeng Zhang",
      "Xu Chen",
      "Jun Wang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "curriculum learning",
      "large language models",
      "reinforcement learning",
      "gradient optimization",
      "prompt selection",
      "rollout quantities",
      "gradient descent",
      "Bayesian posterior estimation",
      "Group Relative Policy Optimization"
    ]
  },
  {
    "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction",
    "summary": "Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/",
    "authors": [
      "Mark Boss",
      "Andreas Engelhardt",
      "Simon Donn\u00e9",
      "Varun Jampani"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Sliced Wasserstein Distance",
      "SWD",
      "Weighted Reservoir Sampling",
      "Reservoir SWD",
      "ReSWD",
      "variance reduction",
      "gradient stability",
      "color correction",
      "diffusion guidance"
    ]
  },
  {
    "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
    "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.",
    "authors": [
      "Minki Kang",
      "Wei-Ning Chen",
      "Dongge Han",
      "Huseyin A. Inan",
      "Lukas Wutschitz",
      "Yanzhi Chen",
      "Robert Sim",
      "Saravan Rajmohan"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Agent Context Optimization",
      "ACON",
      "context compression",
      "environment observations",
      "interaction histories",
      "compression guideline optimization",
      "natural language space",
      "paired trajectories",
      "LLM compressor",
      "distillation",
      "long-horizon tasks",
      "memory usage",
      "task performance",
      "accuracy",
      "performance improvement"
    ]
  },
  {
    "title": "Eliciting Secret Knowledge from Language Models",
    "summary": "We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.",
    "authors": [
      "Bartosz Cywi\u0144ski",
      "Emil Ryd",
      "Rowan Wang",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy",
      "Samuel Marks"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "secret elicitation",
      "prefill attacks",
      "logit lens",
      "sparse autoencoders",
      "SAEs"
    ]
  },
  {
    "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.",
    "authors": [
      "Jian Hu",
      "Mingjie Liu",
      "Ximing Lu",
      "Fang Wu",
      "Zaid Harchaoui",
      "Shizhe Diao",
      "Yejin Choi",
      "Pavlo Molchanov",
      "Jun Yang",
      "Jan Kautz",
      "Yi Dong"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Reinforcement Learning with Verifiable Rewards",
      "RLVR",
      "ProRL",
      "rollouts",
      "exploration",
      "mass balance equation",
      "probability mass",
      "correct tokens",
      "incorrect tokens",
      "one-step RL",
      "net reward balance",
      "correct-mass expansion",
      "unsampled terms"
    ]
  },
  {
    "title": "Making, not Taking, the Best of N",
    "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.",
    "authors": [
      "Ammar Khairi",
      "Daniel D'souza",
      "Marzieh Fadaee",
      "Julia Kreutzer"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "Best-of-N",
      "Fusion-of-N",
      "LLM judge",
      "test-time scaling",
      "synthetic data generation",
      "diverse teachers",
      "student model",
      "polylithic nature"
    ]
  },
  {
    "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
    "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
    "authors": [
      "Ziniu Li",
      "Congliang Chen",
      "Tianyun Yang",
      "Tian Ding",
      "Ruoyu Sun",
      "Ge Zhang",
      "Wenhao Huang",
      "Zhi-Quan Luo"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "reinforcement learning",
      "trajectories",
      "Group Relative Policy Optimization (GRPO)",
      "exploration budget allocation",
      "knapsack problem",
      "policy gradients",
      "computational resources",
      "mathematical reasoning benchmarks"
    ]
  },
  {
    "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
    "summary": "Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
    "authors": [
      "Fang Wu",
      "Weihao Xuan",
      "Heli Qi",
      "Ximing Lu",
      "Aaron Tu",
      "Li Erran Li",
      "Yejin Choi"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "RLVR",
      "Monte Carlo Tree Search",
      "training loop",
      "systematic exploration",
      "credit assignment",
      "global frontier selection",
      "entropy-based guidance",
      "adaptive replay buffer",
      "solution caching",
      "mathematical reasoning benchmarks"
    ]
  },
  {
    "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
    "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.",
    "authors": [
      "EunJeong Hwang",
      "Yuwei Yin",
      "Giuseppe Carenini",
      "Peter West",
      "Vered Shwartz"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Theory of Mind",
      "ToM",
      "dialogue lookahead",
      "Sotopia benchmark",
      "strategic reasoning",
      "long-horizon adaptation"
    ]
  },
  {
    "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.",
    "authors": [
      "Tianrui Qin",
      "Qianben Chen",
      "Sinuo Wang",
      "He Xing",
      "King Zhu",
      "He Zhu",
      "Dingfeng Shi",
      "Xinxin Liu",
      "Ge Zhang",
      "Jiaheng Liu",
      "Yuchen Eleanor Jiang",
      "Xitong Gao",
      "Wangchunshu Zhou"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "parallel agent reasoning framework",
      "directed acyclic graphs (DAGs)",
      "subtasks",
      "explicit dependencies",
      "concurrent execution",
      "dynamic workflow optimization",
      "summary module",
      "BrowseComp",
      "xbench-DeepSearch",
      "agent execution steps",
      "distilling",
      "backbone architectures"
    ]
  },
  {
    "title": "Boolean Satisfiability via Imitation Learning",
    "summary": "We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT",
    "authors": [
      "Zewei Zhang",
      "Huan Liu",
      "Yuanhao Yu",
      "Jun Chen",
      "Xiangyu Xu"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "ImitSAT",
      "conflict-driven clause learning",
      "CDCL",
      "Boolean satisfiability problem",
      "SAT",
      "imitation learning",
      "KeyTrace",
      "decision-level supervision",
      "propagations",
      "wall-clock time",
      "prefix-conditioned supervision",
      "learned approaches"
    ]
  },
  {
    "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
    "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.",
    "authors": [
      "Mohammed Mehedi Hasan",
      "Hao Li",
      "Emad Fallahzadeh",
      "Gopi Krishnan Rajbahadur",
      "Bram Adams",
      "Ahmed E. Hassan"
    ],
    "date": "2025-09-23",
    "ai_keywords": [
      "Foundation model",
      "AI agents",
      "non-determinism",
      "non-reproducibility",
      "testing practices",
      "DeepEval",
      "negative testing",
      "membership testing",
      "Resource Artifacts",
      "Coordination Artifacts",
      "Plan Body",
      "Trigger component",
      "prompt regression testing"
    ]
  },
  {
    "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
    "summary": "Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
    "authors": [
      "Jiarun Liu",
      "Shiyue Xu",
      "Shangkun Liu",
      "Yang Li",
      "Wen Liu",
      "Min Liu",
      "Xiaoqing Zhou",
      "Hanmin Wang",
      "Shilin Jia",
      "zhen Wang",
      "Shaohua Tian",
      "Hanhao Li",
      "Junbo Zhang",
      "Yongli Yu",
      "Peng Cao",
      "Haofen Wang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "multi-agent framework",
      "planning agents",
      "execution agents",
      "critic model voting",
      "hierarchical memory system",
      "working memory",
      "semantic memory",
      "procedural memory",
      "tool suite",
      "search",
      "code execution",
      "multimodal parsing"
    ]
  },
  {
    "title": "GEM: A Gym for Agentic LLMs",
    "summary": "The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
    "authors": [
      "Zichen Liu",
      "Anya Sims",
      "Keyu Duan",
      "Changyu Chen",
      "Simon Yu",
      "Xiangxin Zhou",
      "Haotian Xu",
      "Shaopan Xiong",
      "Bo Liu",
      "Chenmien Tan",
      "Chuen Yang Beh",
      "Weixun Wang",
      "Hao Zhu",
      "Weiyan Shi",
      "Diyi Yang",
      "Michael Shieh",
      "Yee Whye Teh",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "large language models",
      "experience-based learning",
      "GEM",
      "environment simulator",
      "OpenAI-Gym",
      "asynchronous vectorized execution",
      "flexible wrappers",
      "REINFORCE",
      "Return Batch Normalization",
      "ReBN",
      "GRPO",
      "PPO",
      "benchmarking",
      "agentic LLM research"
    ]
  },
  {
    "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
    "summary": "Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.",
    "authors": [
      "Kung-Hsiang Huang",
      "Haoyi Qiu",
      "Yutong Dai",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "vision-language models",
      "GUI agents",
      "attention patterns",
      "transformer layers",
      "attention sparsity",
      "KV cache compression",
      "spatial saliency guidance",
      "temporal redundancy scoring",
      "AgentNetBench benchmark",
      "decoding FLOPs",
      "step accuracy"
    ]
  },
  {
    "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
    "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
    "authors": [
      "Gaotang Li",
      "Ruizhong Qiu",
      "Xiusi Chen",
      "Heng Ji",
      "Hanghang Tong"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "supervised fine-tuning",
      "large language models",
      "negative log likelihood",
      "probability-based objectives",
      "model-capability continuum",
      "prior-leaning objectives"
    ]
  },
  {
    "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
    "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
    "authors": [
      "Hengtao Li",
      "Pengxiang Ding",
      "Runze Suo",
      "Yihao Wang",
      "Zirui Ge",
      "Dongyuan Zang",
      "Kexian Yu",
      "Mingyang Sun",
      "Hongyin Zhang",
      "Donglin Wang",
      "Weihua Su"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning",
      "imitation learning",
      "distribution shift",
      "world model",
      "policy rollouts",
      "trajectory-level rewards",
      "goal-achieving references",
      "sample requirements",
      "generalization",
      "robustness"
    ]
  },
  {
    "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
    "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
    "authors": [
      "Yuchen Cai",
      "Ding Cao",
      "Xin Xu",
      "Zijun Yao",
      "Yuqing Huang",
      "Zhenyu Tan",
      "Benyi Zhang",
      "Guiquan Liu",
      "Junfeng Fang"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "reinforcement learning",
      "parameter updates",
      "Rank-1 Dominance",
      "singular subspace",
      "Rank-1 Linear Dynamics",
      "AlphaRL",
      "large language models"
    ]
  },
  {
    "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
    "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.",
    "authors": [
      "Brandon Ong",
      "Tej Deep Pala",
      "Vernon Toh",
      "William Chandra Tjhi",
      "Soujanya Poria"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Process Reward Models",
      "Vision Language Models",
      "Monte Carlo Tree Search",
      "hybrid data synthesis",
      "perception-focused supervision",
      "Outcome Reward Models",
      "test-time scaling",
      "multimodal benchmarks",
      "visual grounding",
      "process errors",
      "latent reasoning abilities"
    ]
  },
  {
    "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
    "summary": "Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.",
    "authors": [
      "Youngbin Choi",
      "Minjong Lee",
      "Saemi Moon",
      "Seunghyuk Cho",
      "Chaehyeon Chung",
      "MoonJeong Park",
      "Dongwoo Kim"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "large language models",
      "multi-turn reasoning",
      "in-place feedback",
      "feedback paradigms",
      "token usage",
      "reasoning-intensive benchmarks",
      "controlled environments"
    ]
  },
  {
    "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
    "summary": "While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
    "authors": [
      "Yanzhe Chen",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "generative models",
      "pixel-space video synthesis",
      "renderable environment",
      "logical commands",
      "code-centric agent framework",
      "Planner",
      "Coder",
      "Critic",
      "vision-language models",
      "VLM",
      "visual anchor prompts",
      "MMMC",
      "TeachQuiz",
      "direct code generation",
      "human-crafted tutorials"
    ]
  },
  {
    "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
    "summary": "Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
    "authors": [
      "Xin Xu",
      "Xunzhi He",
      "Churan Zhi",
      "Ruizhe Chen",
      "Julian McAuley",
      "Zexue He"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "large language models",
      "bias mitigation",
      "BiasFreeBench",
      "prompting-based methods",
      "training-based methods",
      "multi-choice QA",
      "open-ended multi-turn QA",
      "Bias-Free Score",
      "model size",
      "generalization"
    ]
  },
  {
    "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
    "summary": "Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
    "authors": [
      "Xiaoyan Bai",
      "Itamar Pres",
      "Yuntian Deng",
      "Chenhao Tan",
      "Stuart Shieber",
      "Fernanda Vi\u00e9gas",
      "Martin Wattenberg",
      "Andrew Lee"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "implicit chain-of-thought",
      "logit attributions",
      "linear probes",
      "long-range dependencies",
      "attention",
      "directed acyclic graph",
      "Minkowski sums",
      "Fourier basis",
      "learning dynamics",
      "standard fine-tuning",
      "local optimum",
      "auxiliary loss",
      "linear regression probe",
      "inductive bias"
    ]
  },
  {
    "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
    "summary": "The reliability of large language models (LLMs) during test-time scaling is often assessed with external verifiers or reward models that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM (\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not competitive, and (iii) overall, \\GenORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at https://github.com/db-Lee/Multi-RM{\\small\\texttt{https://github.com/db-Lee/Multi-RM}} to facilitate future research in multi-domain settings.",
    "authors": [
      "Dong Bok Lee",
      "Seanie Lee",
      "Sangwoo Park",
      "Minki Kang",
      "Jinheon Baek",
      "Dongki Kim",
      "Dominik Wagner",
      "Jiongdao Jin",
      "Heejun Lee",
      "Tobias Bocklet",
      "Jinyu Wang",
      "Jingjing Fu",
      "Sung Ju Hwang",
      "Jiang Bia",
      "Lei Song"
    ],
    "date": "2025-10-01",
    "ai_keywords": [
      "large language models",
      "external verifiers",
      "reward models",
      "process reward models",
      "outcome reward models",
      "discriminative ORM",
      "discriminative PRM",
      "generative ORM",
      "generative PRM",
      "label noise",
      "self-correcting reasoning",
      "step-wise aggregation",
      "multi-domain deployment"
    ]
  },
  {
    "title": "The Pitfalls of KV Cache Compression",
    "summary": "KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.",
    "authors": [
      "Alex Chen",
      "Renato Geh",
      "Aditya Grover",
      "Guy Van den Broeck",
      "Daniel Israel"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "KV cache compression",
      "LLMs",
      "multi-instruction prompting",
      "system prompt leakage",
      "KV eviction policies"
    ]
  },
  {
    "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained\n  MLLM Perception",
    "summary": "Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations.To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN.",
    "authors": [
      "Yuheng Shi",
      "Xiaohuan Pei",
      "Minjing Dong",
      "Chang Xu"
    ],
    "date": "2025-09-21",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "Region-of-Interest",
      "RoI",
      "Region Proposal Network",
      "RPN",
      "self-distilled",
      "denoising",
      "attention maps",
      "LLaVA-1.5",
      "TextVQA",
      "DocVQA",
      "V-Star"
    ]
  },
  {
    "title": "Swift: An Autoregressive Consistency Model for Efficient Weather\n  Forecasting",
    "summary": "Diffusion models offer a physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, a single-step consistency model that, for the first time, enables autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running 39times faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks a step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales.",
    "authors": [
      "Jason Stock",
      "Troy Arcomano",
      "Rao Kotamarthi"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "diffusion models",
      "consistency model",
      "autoregressive finetuning",
      "probability flow model",
      "continuous ranked probability score",
      "CRPS",
      "ensemble forecasting",
      "medium-range",
      "seasonal-scales"
    ]
  },
  {
    "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
    "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.",
    "authors": [
      "Yifan Yu",
      "Moyan Li",
      "Shaoyuan Xu",
      "Jinmiao Fu",
      "Xinhai Hou",
      "Fan Lai",
      "Bryan Wang"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "multi-agent systems",
      "inter-agent coordination",
      "tool use",
      "long-horizon reasoning",
      "error recognition",
      "error schemata",
      "CORRECT",
      "CORRECT-Error",
      "error localization",
      "error-injection pipeline"
    ]
  },
  {
    "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation",
    "summary": "Current search techniques are limited to standard RAG query-document applications. In this paper, we propose a novel technique to expand the code and index for predicting the required APIs, directly enabling high-quality, end-to-end code generation for auto-completion and agentic AI applications. We address the problem of API leaks in current code-to-code benchmark datasets by introducing a new dataset built from real-world ServiceNow Script Includes that capture the challenge of unclear API usage intent in the code. Our evaluation metrics show that this method achieves 87.86% top-40 retrieval accuracy, allowing the critical context with APIs needed for successful downstream code generation. To enable real-time predictions, we develop a comprehensive post-training pipeline that optimizes a compact 0.6B reranker through synthetic dataset generation, supervised fine-tuning, and reinforcement learning. This approach enables our compact reranker to outperform a much larger 8B model while maintaining 2.5x reduced latency, effectively addressing the nuances of enterprise-specific code without the computational overhead of larger models.",
    "authors": [
      "Esakkivel Esakkiraja",
      "Denis Akhiyarov",
      "Aditya Shanmugham",
      "Chitra Ganapathy"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "API leaks",
      "code generation",
      "auto-completion",
      "agentic AI",
      "ServiceNow Script Includes",
      "top-40 retrieval accuracy",
      "post-training pipeline",
      "synthetic dataset generation",
      "supervised fine-tuning",
      "reinforcement learning",
      "compact reranker",
      "latency"
    ]
  },
  {
    "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL",
    "summary": "Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.",
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Yihao Feng",
      "Bowen Jin",
      "Zhaoran Wang",
      "John Peebles",
      "Zirui Wang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "reinforcement learning",
      "mid-training",
      "action subspace",
      "value approximation error",
      "pruning efficiency",
      "RL convergence",
      "action abstractions",
      "sequential variational lower bound",
      "latent structures",
      "fine-tuning",
      "HumanEval",
      "MBPP",
      "RLVR",
      "LiveCodeBench",
      "Codeforces"
    ]
  },
  {
    "title": "LLM Watermark Evasion via Bias Inversion",
    "summary": "Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the Bias-Inversion Rewriting Attack (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses.",
    "authors": [
      "Jeongyeon Hwang",
      "Sangdon Park",
      "Jungseul Ok"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "watermarking",
      "large language models",
      "LLMs",
      "Bias-Inversion Rewriting Attack",
      "BIRA",
      "logits",
      "watermark signal",
      "adversarial evasion",
      "semantic content",
      "stress testing",
      "robust defenses"
    ]
  },
  {
    "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
    "summary": "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.",
    "authors": [
      "Shuche Wang",
      "Fengzhuo Zhang",
      "Jiaxiang Li",
      "Cunxiao Du",
      "Chao Du",
      "Tianyu Pang",
      "Zhuoran Yang",
      "Mingyi Hong",
      "Vincent Y. F. Tan"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Muon optimizer",
      "Adam",
      "Large Language Models (LLMs)",
      "associative memory",
      "transformer components",
      "Value and Output (VO) attention weights",
      "Feed-Forward Networks (FFNs)",
      "heavy-tailed data",
      "singular spectrum",
      "tail classes",
      "class-imbalanced data",
      "balanced learning",
      "outer-product structure",
      "linear associative memories"
    ]
  },
  {
    "title": "Nudging the Boundaries of LLM Reasoning",
    "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are \"unsolvable\" to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. Consequently, the model's \"upper limit\" remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard samples cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a CoT and then produces a hint containing the core knowledge needed to solve the problem. During training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated, avoiding distributional shift and do not rely on external models. NuRL achieves consistent improvements across 6 benchmarks and 3 models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model's upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level, and are most beneficial when applied necessarily and after GRPO has converged.",
    "authors": [
      "Justin Chih-Yao Chen",
      "Becky Xiangyu Peng",
      "Prafulla Kumar Choubey",
      "Kung-Hsiang Huang",
      "Jiaxin Zhang",
      "Mohit Bansal",
      "Chien-Sheng Wu"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "online reinforcement learning",
      "RL",
      "GRPO",
      "LLM reasoning",
      "unsolvable problems",
      "rollouts",
      "rewards",
      "gradients",
      "nudging method",
      "self-generated hints",
      "core knowledge",
      "pass rate",
      "training signals",
      "distributional shift",
      "test-time scaling",
      "pass@1024",
      "effective hints",
      "abstract hints",
      "high-level hints"
    ]
  },
  {
    "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
    "summary": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these causal effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object's geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The code is available at https://github.com/buxiangzhiren/GeoRemover.",
    "authors": [
      "Zixin Zhu",
      "Haoxiang Li",
      "Xuelu Feng",
      "He Wu",
      "Chunming Qiao",
      "Junsong Yuan"
    ],
    "date": "2025-09-23",
    "ai_keywords": [
      "geometry-aware framework",
      "geometry removal",
      "appearance rendering",
      "mask-aligned supervision",
      "preference-driven objective",
      "positive and negative sample pairs",
      "photorealistic RGB image",
      "3D geometry",
      "structural insertions"
    ]
  },
  {
    "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models\n  More Effective and Efficient Reasoners",
    "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.",
    "authors": [
      "Xin Xu",
      "Cliveb AI",
      "Kai Yang",
      "Tianhao Chen",
      "Yang Wang",
      "Saiyong Yang",
      "Can Yang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Reinforcement Learning with Verifiable Reward (RLVR)",
      "Chain-of-Thought (CoT) distillation",
      "ThinkFree operation",
      "token usage",
      "RL convergence",
      "performance ceiling",
      "token-efficient reasoning models",
      "AIME24",
      "LiveCodeBench"
    ]
  },
  {
    "title": "Convolutional Set Transformer",
    "summary": "We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer).",
    "authors": [
      "Federico Chinello",
      "Giacomo Boracchi"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Convolutional Set Transformer",
      "CST",
      "set-input networks",
      "Deep Sets",
      "Set Transformer",
      "3D image tensors",
      "feature extraction",
      "contextual modeling",
      "Set Classification",
      "Set Anomaly Detection",
      "Grad-CAM",
      "Transfer Learning",
      "CST-15",
      "ImageNet"
    ]
  },
  {
    "title": "Video Object Segmentation-Aware Audio Generation",
    "summary": "Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at https://saganet.notion.site",
    "authors": [
      "Ilpo Viertola",
      "Vladimir Iashin",
      "Esa Rahtu"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "multimodal audio generation",
      "video object segmentation-aware audio generation",
      "SAGANet",
      "sound synthesis",
      "visual segmentation masks",
      "Segmented Music Solos",
      "Foley synthesis"
    ]
  },
  {
    "title": "Context Is What You Need: The Maximum Effective Context Window for Real\n  World Limits of LLMs",
    "summary": "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.",
    "authors": [
      "Norman Paulsen"
    ],
    "date": "2025-09-21",
    "ai_keywords": [
      "large language model",
      "maximum context window",
      "maximum effective context window",
      "context window effectiveness",
      "model efficacy",
      "problem types",
      "model accuracy",
      "model hallucination rates"
    ]
  },
  {
    "title": "Specialization after Generalization: Towards Understanding Test-Time\n  Training in Foundation Models",
    "summary": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.",
    "authors": [
      "Jonas H\u00fcbotter",
      "Patrik Wolf",
      "Alexander Shevchenko",
      "Dennis J\u00fcni",
      "Andreas Krause",
      "Gil Kur"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "test-time training",
      "TTT",
      "out-of-distribution adaptation",
      "privileged data",
      "foundation models",
      "underparameterized",
      "specialization",
      "generalization",
      "linear representation hypothesis",
      "sparse autoencoder",
      "ImageNet",
      "semantically related data points",
      "scaling studies",
      "image tasks",
      "language tasks"
    ]
  },
  {
    "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
    "summary": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.   We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\n locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.   BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.   BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.   BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.",
    "authors": [
      "Adrian Kosowski",
      "Przemys\u0142aw Uzna\u0144ski",
      "Jan Chorowski",
      "Zuzanna Stamirowska",
      "Micha\u0142 Bartoszkiewicz"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Large Language Model",
      "scale-free network",
      "Hebbian learning",
      "synaptic plasticity",
      "spiking neurons",
      "graph model",
      "GPU-friendly",
      "Transformer-like scaling laws",
      "monosemanticity",
      "interpretability"
    ]
  },
  {
    "title": "d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
    "summary": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce Dual aDaptive Cache (d^2Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d^2Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d^2Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d^2Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.",
    "authors": [
      "Yuchu Jiang",
      "Yue Cai",
      "Xiangzhong Luo",
      "Jiale Fu",
      "Jiarui Wang",
      "Chonghan Liu",
      "Xu Yang"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "diffusion-based large language models",
      "dLLMs",
      "bidirectional attention",
      "key-value cache",
      "autoregressive models",
      "Dual aDaptive Cache",
      "d\u00b2Cache",
      "two-stage fine-grained selection",
      "quasi left-to-right generation",
      "LLaDA",
      "Dream"
    ]
  },
  {
    "title": "EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series\n  Forecasting",
    "summary": "Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.",
    "authors": [
      "Sachith Abeywickrama",
      "Emadeldeen Eldele",
      "Min Wu",
      "Xiaoli Li",
      "Chau Yuen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Transformer-based models",
      "patch-based input strategies",
      "temporal coherence",
      "conditional entropy",
      "Entropy-based Dynamic Patcher",
      "Adaptive Patch Encoder",
      "pooling",
      "cross-attention",
      "global transformer",
      "long-term forecasting benchmarks"
    ]
  },
  {
    "title": "Estimating Time Series Foundation Model Transferability via In-Context\n  Learning",
    "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via large-scale pre-training, yet fine-tuning remains critical for boosting performance in domains with limited public data. With the growing number of TSFMs, efficiently identifying the best model for downstream fine-tuning becomes increasingly challenging. In this work, we introduce TimeTic, a transferability estimation framework that recasts model selection as an in-context-learning problem: given observations on known (source) datasets, it predicts how a TSFM will perform after fine-tuning on a downstream (target) dataset. TimeTic flexibly organizes the observed model-data relationships as contextual information, allowing it to adapt seamlessly to various test-time scenarios. Leveraging the natural tabular structure formed by dataset meta-features, model characteristics, and fine-tuned performance, we employ tabular foundation models to serve as in-context learners. We further introduce a novel model characterization based on entropy evolution across model layers, capturing embedding-space distinctions and enabling TimeTic to generalize across arbitrary model sets. We establish a comprehensive benchmark for transferability estimation including 10 datasets, 10 foundation models, and 3 forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong alignment with actual fine-tuned performance for previously unseen datasets, achieving a mean rank correlation of approximately 0.6 and a 30% improvement compared to using zero-shot performance as the transferability score.",
    "authors": [
      "Qingren Yao",
      "Ming Jin",
      "Chengqi Zhang",
      "Chao-Han Huck Yang",
      "Jun Qi",
      "Shirui Pan"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "time series foundation models",
      "zero-shot forecasting",
      "fine-tuning",
      "transferability estimation",
      "in-context-learning",
      "contextual information",
      "tabular foundation models",
      "entropy evolution",
      "model characterization",
      "mean rank correlation"
    ]
  },
  {
    "title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token\n  Pruning for Efficient Supervised Fine-Tuning",
    "summary": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies--high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\\% average improvement over the full-data SFT baseline using only 12.5\\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT.",
    "authors": [
      "Shaobo Wang",
      "Jiaming Wang",
      "Jiajun Zhang",
      "Cong Wang",
      "Yue Min",
      "Zichen Wen",
      "Fei Huang",
      "Huiqiang Jiang",
      "Junyang Lin",
      "Dayiheng Liu",
      "Linfeng Zhang"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "supervised fine-tuning",
      "data efficiency",
      "large language models",
      "data pruning",
      "sample-level pruning",
      "token-level pruning",
      "Error-Uncertainty (EU) Plane",
      "Quadrant-based Tuning",
      "sample-level triage",
      "asymmetric token-pruning",
      "context-aware scoring mechanism",
      "misconception samples",
      "calibration samples",
      "dynamic pruning"
    ]
  },
  {
    "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
    "summary": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers.",
    "authors": [
      "Feng Wang",
      "Yuqing Li",
      "Han Xiao"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "multilingual document reranker",
      "causal self-attention",
      "BEIR",
      "nDCG@10",
      "generative listwise rerankers"
    ]
  },
  {
    "title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source\n  Software",
    "summary": "Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.",
    "authors": [
      "Zehua Zhang",
      "Ati Priya Bajaj",
      "Divij Handa",
      "Siyu Liu",
      "Arvind S Raj",
      "Hongkai Chen",
      "Hulin Wang",
      "Yibo Liu",
      "Zion Leonahenahe Basque",
      "Souradip Nath",
      "Vishal Juneja",
      "Nikhil Chapre",
      "Yan Shoshitaishvili",
      "Adam Doup\u00e9",
      "Chitta Baral",
      "Ruoyu Wang"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "LLM Agents",
      "Large Language Models",
      "BUILD-BENCH",
      "OSS-BUILD-AGENT",
      "build instruction retrieval",
      "software engineering tasks"
    ]
  },
  {
    "title": "ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency\n  Estimation",
    "summary": "Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment.",
    "authors": [
      "Edoardo Bianchi",
      "Jacopo Staiano",
      "Antonio Liotta"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "AttentiveGatedProjector",
      "TimeSformer",
      "generative reasoning",
      "vision-language model",
      "multi-view features",
      "EgoExo4D",
      "expert commentaries",
      "natural language critiques"
    ]
  },
  {
    "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
    "summary": "While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.",
    "authors": [
      "Yixuan Weng",
      "Minjun Zhu",
      "Qiujie Xie",
      "Qiyao Sun",
      "Zhen Lin",
      "Sifan Liu",
      "Yue Zhang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Bayesian Optimization",
      "hierarchical evaluation",
      "Findings Memory",
      "exploration",
      "exploitation",
      "scientific discovery",
      "state-of-the-art",
      "AI tasks"
    ]
  },
  {
    "title": "TTT3R: 3D Reconstruction as Test-Time Training",
    "summary": "Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a 2times improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R",
    "authors": [
      "Xingyu Chen",
      "Yue Chen",
      "Yuliang Xiu",
      "Andreas Geiger",
      "Anpei Chen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Recurrent Neural Networks",
      "3D reconstruction",
      "linear-time complexity",
      "length generalization",
      "Test-Time Training",
      "memory state",
      "incoming observations",
      "closed-form learning rate",
      "global pose estimation",
      "FPS",
      "GPU memory"
    ]
  },
  {
    "title": "A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects",
    "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and innovation in artificial intelligence (AI). However, the methods of collaboration used to develop open LLMs both before and after their public release have not yet been comprehensively studied, limiting our understanding of how open LLM projects are initiated, organized, and governed as well as what opportunities there are to foster this ecosystem even further. We address this gap through an exploratory analysis of open collaboration throughout the development and reuse lifecycle of open LLMs, drawing on semi-structured interviews with the developers of 14 open LLMs from grassroots projects, research institutes, startups, and Big Tech companies in North America, Europe, Africa, and Asia. We make three key contributions to research and practice. First, collaboration in open LLM projects extends far beyond the LLMs themselves, encompassing datasets, benchmarks, open source frameworks, leaderboards, knowledge sharing and discussion forums, and compute partnerships, among others. Second, open LLM developers have a variety of social, economic, and technological motivations, from democratizing AI access and promoting open science to building regional ecosystems and expanding language representation. Third, the sampled open LLM projects exhibit five distinct organizational models, ranging from single company projects to non-profit-sponsored grassroots projects, which vary in their centralization of control and community engagement strategies used throughout the open LLM lifecycle. We conclude with practical recommendations for stakeholders seeking to support the global community building a more open future for AI.",
    "authors": [
      "Johan Lin\u00e5ker",
      "Cailean Osborne",
      "Jennifer Ding",
      "Ben Burtenshaw"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "large language models",
      "open collaboration",
      "datasets",
      "benchmarks",
      "open source frameworks",
      "leaderboards",
      "knowledge sharing",
      "discussion forums",
      "compute partnerships",
      "democratizing AI",
      "open science",
      "regional ecosystems",
      "language representation",
      "organizational models",
      "community engagement"
    ]
  },
  {
    "title": "Who invented deep residual learning?",
    "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the most cited scientific article of the 21st century is an NN paper on deep residual learning with residual connections. Who invented this? We present a timeline of the evolution of deep residual learning.",
    "authors": [
      "Juergen Schmidhuber"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "deep artificial neural networks",
      "NNs",
      "deep residual learning",
      "residual connections"
    ]
  },
  {
    "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
    "summary": "Adversarial purification with diffusion models has emerged as a promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, a magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original classifier, while boosting robust accuracy by 2.15, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method.",
    "authors": [
      "Xiaoyi Huang",
      "Junwei Wu",
      "Kejia Zhang",
      "Carl Yang",
      "Zhiming Luo"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "adversarial purification",
      "diffusion models",
      "uniform noise",
      "high-frequency regions",
      "magnitude spectrum",
      "frequency-targeted noise",
      "CIFAR-10",
      "ImageNet-1K",
      "RobustBench"
    ]
  },
  {
    "title": "Mem-\u03b1: Learning Memory Construction via Reinforcement Learning",
    "summary": "Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.",
    "authors": [
      "Yu Wang",
      "Ryuichi Takanobu",
      "Zhiqi Liang",
      "Yuzhen Mao",
      "Yuanzhe Hu",
      "Julian McAuley",
      "Xiaojian Wu"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "reinforcement learning",
      "memory-augmented agents",
      "memory management",
      "interaction patterns",
      "episodic memory",
      "semantic memory",
      "memory operations",
      "question-answering accuracy",
      "generalization"
    ]
  },
  {
    "title": "DA^2: Depth Anything in Any Direction",
    "summary": "Panorama has a full FoV (360^circtimes180^circ), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose DA^{2}: Depth Anything in Any Direction, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create sim543K panoramic RGB-depth pairs, bringing the total to sim607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data will be released. Project page: https://depth-any-in-any-dir.github.io/.",
    "authors": [
      "Haodong Li",
      "Wangguangdong Zheng",
      "Jing He",
      "Yuhao Liu",
      "Xin Lin",
      "Xin Yang",
      "Ying-Cong Chen",
      "Chunchao Guo"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "panoramic depth estimation",
      "3D vision",
      "zero-shot generalization",
      "perspective splitting",
      "cubemaps",
      "data curation engine",
      "SphereViT",
      "spherical coordinates",
      "spherical geometric consistency",
      "AbsRel",
      "in-domain methods",
      "fusion-based approaches"
    ]
  },
  {
    "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
    "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.",
    "authors": [
      "Xingyu Fu",
      "Siyi Liu",
      "Yinuo Xu",
      "Pan Lu",
      "Guangqiuse Hu",
      "Tianbo Yang",
      "Taran Anantasagar",
      "Christopher Shen",
      "Yikai Mao",
      "Yuanzhe Liu",
      "Keyush Shah",
      "Chung Un Lee",
      "Yejin Choi",
      "James Zou",
      "Dan Roth",
      "Chris Callison-Burch"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "deepfake traces",
      "spatiotemporal grounded visual artifacts",
      "fine-grained",
      "spatially- and temporally- aware benchmark",
      "multimodal language models",
      "reward models",
      "binary fake v.s. real classification",
      "natural language explanations",
      "spatial grounding",
      "temporal labeling"
    ]
  },
  {
    "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
    "summary": "Designers craft and edit graphic designs in a layer representation, but layer-based editing becomes impossible once composited into a raster image. In this work, we propose LayerD, a method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose a simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop a quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing.",
    "authors": [
      "Tomoyuki Suzuki",
      "Kang-Jun Liu",
      "Naoto Inoue",
      "Kota Yamaguchi"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "layer representation",
      "raster image",
      "decomposition",
      "foreground layers",
      "uniform appearance",
      "quality metric",
      "state-of-the-art image generators",
      "layer-based editing"
    ]
  },
  {
    "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
    "summary": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
    "authors": [
      "Gongrui Zhang",
      "Jialiang Zhu",
      "Ruiqi Yang",
      "Kai Qiu",
      "Miaosen Zhang",
      "Zhirong Wu",
      "Qi Dai",
      "Bei Liu",
      "Chong Luo",
      "Zhengyuan Yang",
      "Linjie Li",
      "Lijuan Wang",
      "Weizhu Chen",
      "Yuan Zhang",
      "Xin Li",
      "Zhaoyi Liu",
      "Xin Geng",
      "Baining Guo"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "deep research agent",
      "data synthesis pipeline",
      "entity trees",
      "sub-tree sampling",
      "entity fuzzification",
      "self-hosted search infrastructure",
      "cold-start supervised finetuning",
      "reinforcement learning",
      "BrowseComp",
      "BrowseComp-ZH",
      "Xbench-DS",
      "WebSailor-72B",
      "DeepDive-32B"
    ]
  },
  {
    "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation",
    "summary": "Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.",
    "authors": [
      "Agneet Chatterjee",
      "Rahim Entezari",
      "Maksym Zhuravinskyi",
      "Maksim Lapin",
      "Reshinth Adithyan",
      "Amit Raj",
      "Chitta Baral",
      "Yezhou Yang",
      "Varun Jampani"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "video generation",
      "Stable Cinemetrics",
      "taxonomies",
      "Setup",
      "Event",
      "Lighting",
      "Camera",
      "fine-grained control nodes",
      "benchmark",
      "prompt categorization",
      "question generation",
      "human study",
      "automatic evaluator",
      "vision-language model",
      "zero-shot baselines",
      "SCINE"
    ]
  },
  {
    "title": "Humanline: Online Alignment as Perceptual Loss",
    "summary": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
    "authors": [
      "Sijia Liu",
      "Niklas Muennighoff",
      "Kawin Ethayarajh"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "GRPO",
      "DPO",
      "prospect theory",
      "on-policy sampling",
      "PPO",
      "perceptual bias",
      "perceptual losses",
      "humanline variants",
      "KTO"
    ]
  },
  {
    "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
    "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.",
    "authors": [
      "Chenxing Wei",
      "Hong Wang",
      "Ying He",
      "Fei Yu",
      "Yao Shu"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "multi-turn interaction",
      "Test-Time Policy Adaptation",
      "T2PAM",
      "latent optimal policy",
      "parameter adaptation",
      "Optimum-Referenced One-Step Adaptation",
      "ROSA",
      "theoretical optimal policy",
      "gradient-based optimization",
      "computational overhead"
    ]
  },
  {
    "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
    "summary": "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/",
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Mengqi He",
      "Fabian Waschkowski",
      "Lukas Wesemann",
      "Peter Tu",
      "Jing Zhang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Reinforcement Learning",
      "Group Relative Policy Optimization",
      "Vision-Language Models",
      "multimodal reasoning",
      "logical inference",
      "perceptual grounding",
      "visual forgetting",
      "Vision-Anchored Policy Optimization"
    ]
  },
  {
    "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
    "summary": "MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of 127 high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below 30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution turns and 17.4 tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
    "authors": [
      "Zijian Wu",
      "Xiangyan Liu",
      "Xinyuan Zhang",
      "Lingjun Chen",
      "Fanqing Meng",
      "Lingxiao Du",
      "Yiran Zhao",
      "Fanshi Zhang",
      "Yaoqi Ye",
      "Jiawei Wang",
      "Zirui Wang",
      "Jinjie Ni",
      "Yufan Yang",
      "Arvin Xu",
      "Michael Qizhe Shieh"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "MCP",
      "LLMs",
      "general agents",
      "MCP benchmarks",
      "MCPMark",
      "high-quality tasks",
      "domain experts",
      "AI agents",
      "initial state",
      "programmatic script",
      "automatic verification",
      "CRUD operations",
      "minimal agent framework",
      "tool-calling loop",
      "gpt-5-medium",
      "claude-sonnet-4",
      "o3",
      "pass@1",
      "pass^4",
      "execution turns",
      "tool calls"
    ]
  },
  {
    "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
    "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma and Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.",
    "authors": [
      "Jingdi Lei",
      "Varun Gumma",
      "Rishabh Bhardwaj",
      "Seok Min Lim",
      "Chuan Li",
      "Amir Zadeh",
      "Soujanya Poria"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Large Language Model (LLM)",
      "operational safety",
      "OffTopicEval",
      "query grounding (Q-ground)",
      "system-prompt grounding (P-ground)",
      "out-of-distribution (OOD) refusal"
    ]
  },
  {
    "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
    "summary": "Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
    "authors": [
      "Jiayi Guo",
      "Chuanhao Yan",
      "Xingqian Xu",
      "Yulin Wang",
      "Kai Wang",
      "Gao Huang",
      "Humphrey Shi"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "diffusion-generated images",
      "multimodal alignment",
      "diffusion weight",
      "preference data",
      "editing-based methods",
      "Implicit Multimodal Guidance",
      "multimodal large language model",
      "Implicit Aligner",
      "diffusion conditioning features",
      "Iteratively Updated Preference Objective",
      "SDXL",
      "SDXL-DPO",
      "FLUX"
    ]
  },
  {
    "title": "Knowledge Homophily in Large Language Models",
    "summary": "Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.",
    "authors": [
      "Utkarsh Sahu",
      "Zhisheng Qi",
      "Mahantesh Halappanavar",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Yu Zhang",
      "Yao Ma",
      "Yu Wang"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "Large Language Models",
      "knowledge homophily",
      "graph representation",
      "knowledge checking",
      "triplet",
      "entity",
      "Graph Neural Network",
      "knowledgeability scores",
      "active labeling",
      "multi-hop path retrieval"
    ]
  },
  {
    "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
    "summary": "Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream.",
    "authors": [
      "Yi-Cheng Lin",
      "Yu-Hua Chen",
      "Jia-Kai Dong",
      "Yueh-Hsuan Huang",
      "Szu-Chi Chen",
      "Yu-Chen Chen",
      "Chih-Yao Chen",
      "Yu-Jung Lin",
      "Yu-Ling Chen",
      "Zih-Yu Chen",
      "I-Ning Tsai",
      "Hsiu-Hsuan Wang",
      "Ho-Lam Chung",
      "Ke-Han Lu",
      "Hung-yi Lee"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "audio-language models",
      "soundmarks",
      "benchmark",
      "LLM-assisted question generation",
      "multimodal evaluation"
    ]
  },
  {
    "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
    "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce J-Detector, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of J-Detector and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.",
    "authors": [
      "Dawei Li",
      "Zhen Tan",
      "Chengshuai Zhao",
      "Bohan Jiang",
      "Baixiang Huang",
      "Pingchuan Ma",
      "Abdullah Alnaibari",
      "Kai Shu",
      "Huan Liu"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Large Language Model",
      "judgment detection",
      "neural detector",
      "linguistic features",
      "LLM-enhanced features",
      "judgment scores",
      "candidate content",
      "biases",
      "detectability"
    ]
  },
  {
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.",
    "authors": [
      "Yida Xue",
      "Mingjun Mao",
      "Xiangyuan Ru",
      "Yuqi Zhu",
      "Baochang Ren",
      "Shuofei Qiao",
      "Mengru Wang",
      "Shumin Deng",
      "Xinyu An",
      "Ningyu Zhang",
      "Ying Chen",
      "Huajun Chen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Multi-modal Large Language Models",
      "MLLMs",
      "optical data",
      "sonar data",
      "sequential decision-making",
      "embodied AI",
      "autonomous ocean underwater vehicles"
    ]
  },
  {
    "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
    "summary": "While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced \"critical point\"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.",
    "authors": [
      "Minhui Zhu",
      "Minyang Tian",
      "Xiaocheng Yang",
      "Tianci Zhou",
      "Penghao Zhu",
      "Eli Chertkov",
      "Shengyan Liu",
      "Yufeng Du",
      "Lifan Yuan",
      "Ziming Ji",
      "Indranil Das",
      "Junyi Cao",
      "Yufeng Du",
      "Jinchen He",
      "Yifan Su",
      "Jiabin Yu",
      "Yikun Jiang",
      "Yujie Zhang",
      "Chang Liu",
      "Ze-Min Huang",
      "Weizhen Jia",
      "Xinan Chen",
      "Peixue Wu",
      "Yunkai Wang",
      "Juntai Zhou",
      "Yong Zhao",
      "Farshid Jafarpour",
      "Jessie Shelton",
      "Aaron Young",
      "John Bartolotta",
      "Wenchao Xu",
      "Yue Sun",
      "Anjun Chu",
      "Victor Colussi",
      "Chris Akers",
      "Nathan Brooks",
      "Wenbo Fu",
      "Christopher Wilson",
      "Jinchao Zhao",
      "Marvin Qi",
      "Anqi Mu",
      "Yubo Yang",
      "Allen Zang",
      "Yang Lyu",
      "Peizhi Mai",
      "Xuefei Guo",
      "Luyu Gao",
      "Ze Yang",
      "Chi Xue",
      "Dmytro Bandak",
      "Ya\u00efr Hein",
      "Yonatan Kahn",
      "Kevin Zhou",
      "John Drew Wilson Jarrod T. Reilly",
      "Di Luo",
      "Daniel Inafuku",
      "Hao Tong",
      "Liang Yang",
      "Ruixing Zhang",
      "Xueying Wang",
      "Ofir Press",
      "Nicolas Chia",
      "Eliu Huerta",
      "Hao Peng"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "large language models",
      "reasoning capabilities",
      "high-school math competitions",
      "coding",
      "CritPt",
      "benchmark",
      "research-level reasoning tasks",
      "condensed matter",
      "quantum physics",
      "atomic",
      "molecular & optical physics",
      "astrophysics",
      "high energy physics",
      "mathematical physics",
      "statistical physics",
      "nuclear physics",
      "nonlinear dynamics",
      "fluid dynamics",
      "biophysics",
      "composite research challenges",
      "checkpoint tasks",
      "guess-resistant",
      "machine-verifiable",
      "automated grading pipeline",
      "advanced physics-specific output formats",
      "GPT-5"
    ]
  },
  {
    "title": "Regression Language Models for Code",
    "summary": "We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms.",
    "authors": [
      "Yash Akhauri",
      "Xingyou Song",
      "Arissa Wongpanich",
      "Bryan Lewandowski",
      "Mohamed S. Abdelfattah"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Regression Language Model (RLM)",
      "T5Gemma",
      "Spearman-rank",
      "Kendall-Tau",
      "NAS design spaces",
      "graph neural networks"
    ]
  },
  {
    "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
    "summary": "Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.",
    "authors": [
      "Chenhui Zhu",
      "Yilu Wu",
      "Shuai Wang",
      "Gangshan Wu",
      "Limin Wang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "diffusion models",
      "MotionRAG",
      "Context-Aware Motion Adaptation (CAMA)",
      "video encoder",
      "specialized resamplers",
      "causal transformer architecture",
      "attention-based motion injection adapter",
      "zero-shot generalization"
    ]
  },
  {
    "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
    "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.   Benchmark: http://paulgavrikov.github.io/visualoverload",
    "authors": [
      "Paul Gavrikov",
      "Wei Lin",
      "M. Jehanzeb Mirza",
      "Soumya Jahagirdar",
      "Muhammad Huzaifa",
      "Sivan Doveh",
      "Serena Yeung-Levy",
      "James Glass",
      "Hilde Kuehne"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "visual question answering",
      "VQA",
      "densely populated scenes",
      "high-resolution scans",
      "public-domain paintings",
      "manual annotation",
      "task categories",
      "error analysis",
      "counting skills",
      "OCR",
      "logical inconsistencies",
      "vision models"
    ]
  },
  {
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "summary": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
    "authors": [
      "Zhepei Wei",
      "Xiao Yang",
      "Kai Sun",
      "Jiaqi Wang",
      "Rulin Shao",
      "Sean Chen",
      "Mohammad Kachuee",
      "Teja Gollapudi",
      "Tony Liao",
      "Nicolas Scheffer",
      "Rakesh Wanga",
      "Anuj Kumar",
      "Yu Meng",
      "Wen-tau Yih",
      "Xin Luna Dong"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "hallucination",
      "untruthful responses",
      "parametric knowledge",
      "truthfulness",
      "reinforcement learning",
      "RL",
      "GRPO",
      "ternary reward",
      "abstention",
      "accuracy-driven methods",
      "supervised fine-tuning",
      "binary reward",
      "knowledge-intensive benchmarks",
      "Qwen",
      "Llama",
      "retrieval",
      "non-retrieval setups",
      "ablation study"
    ]
  },
  {
    "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
    "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
    "authors": [
      "Qinsi Wang",
      "Bo Liu",
      "Tianyi Zhou",
      "Jing Shi",
      "Yueqian Lin",
      "Yiran Chen",
      "Hai Helen Li",
      "Kun Wan",
      "Wentian Zhao"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "reinforcement learning",
      "vision-language models",
      "strategic self-play framework",
      "self-play",
      "reinforcement learning with verifiable rewards",
      "Iterative Self-Play Policy Optimization"
    ]
  },
  {
    "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
    "summary": "Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.",
    "authors": [
      "Zhen Yang",
      "Zi-Yi Dou",
      "Di Feng",
      "Forrest Huang",
      "Anh Nguyen",
      "Keen You",
      "Omar Attia",
      "Yuhao Yang",
      "Michael Feng",
      "Haotian Zhang",
      "Ram Ramrakhya",
      "Chao Jia",
      "Jeffrey Nichols",
      "Alexander Toshev",
      "Yinfei Yang",
      "Zhe Gan"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "chain-of-thought reasoning",
      "visual tool-use",
      "reinforcement learning",
      "GUI agent",
      "GUI grounding",
      "GUI navigation",
      "ScreenSpot-V2",
      "ScreenSpot-Pro",
      "OSWorld-G",
      "AndroidWorld",
      "OSWorld"
    ]
  },
  {
    "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
    "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at http://cslikai.cn/Dolphin/.",
    "authors": [
      "Kai Li",
      "Kejun Gao",
      "Xiaolin Hu"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "dual-path lightweight video encoder",
      "DP-LipCoder",
      "discrete audio-aligned semantic tokens",
      "lightweight encoder-decoder separator",
      "global-local attention (GLA) block",
      "multi-scale dependencies",
      "state-of-the-art (SOTA)",
      "MACs",
      "GPU inference speed"
    ]
  },
  {
    "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
    "summary": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/",
    "authors": [
      "Wei He",
      "Yueqing Sun",
      "Hongyan Hao",
      "Xueyuan Hao",
      "Zhikang Xia",
      "Qi Gu",
      "Chengcheng Han",
      "Dengchang Zhao",
      "Hui Su",
      "Kefeng Zhang",
      "Man Gao",
      "Xi Su",
      "Xiaodong Cai",
      "Xunliang Cai",
      "Yu Yang",
      "Yunke Zhao"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "LLM-based agents",
      "VitaBench",
      "interactive tasks",
      "real-world settings",
      "food delivery",
      "in-store consumption",
      "online travel services",
      "life-serving simulation environment",
      "domain-specific policies",
      "flexible composition",
      "cross-scenario tasks",
      "single-scenario tasks",
      "real user requests",
      "temporal dimensions",
      "spatial dimensions",
      "complex tool sets",
      "ambiguous instructions",
      "shifting user intent",
      "multi-turn conversations",
      "rubric-based sliding window evaluator",
      "stochastic interactions"
    ]
  },
  {
    "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
    "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.",
    "authors": [
      "Runze Liu",
      "Jiakang Wang",
      "Yuling Shi",
      "Zhihui Xie",
      "Chenxin An",
      "Kaiyan Zhang",
      "Jian Zhao",
      "Xiaodong Gu",
      "Lei Lin",
      "Wenping Hu",
      "Xiu Li",
      "Fuzheng Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Reinforcement Learning",
      "Large Language Models",
      "Process-Supervised RL",
      "PSRL",
      "AttnRL",
      "attention scores",
      "reasoning behaviors",
      "adaptive sampling strategy",
      "one-step off-policy training",
      "mathematical reasoning benchmarks"
    ]
  },
  {
    "title": "dParallel: Learnable Parallel Decoding for dLLMs",
    "summary": "Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel",
    "authors": [
      "Zigeng Chen",
      "Gongfan Fang",
      "Xinyin Ma",
      "Ruonan Yu",
      "Xinchao Wang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "diffusion large language models",
      "dLLMs",
      "autoregressive generation",
      "parallel token prediction",
      "parallel decoding",
      "masked tokens",
      "certainty-forcing distillation",
      "LLaDA-8B-Instruct",
      "GSM8K",
      "MBPP benchmark"
    ]
  },
  {
    "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
    "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing \"thinking time\" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.",
    "authors": [
      "Yueqian Lin",
      "Zhengmian Hu",
      "Qinsi Wang",
      "Yudong Liu",
      "Hengfan Zhang",
      "Jayakumar Subramanian",
      "Nikos Vlassis",
      "Hai Helen Li",
      "Yiran Chen"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "voice-interactive systems",
      "reasoning ability",
      "real-time conversational constraints",
      "voice-native episodes",
      "text benchmarks",
      "speech interaction",
      "text-voice comparison",
      "architectural choices",
      "reliability",
      "latency-accuracy analyses",
      "low-latency plateau",
      "thinking time",
      "decoupled cascade",
      "error signatures",
      "native streaming",
      "end-to-end",
      "cascade designs",
      "real-time voice assistants",
      "fluent reasoning"
    ]
  },
  {
    "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
    "summary": "The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.",
    "authors": [
      "Yein Park",
      "Minbyul Jeong",
      "Jaewoo Kang"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "supervised fine-tuning",
      "reinforcement learning",
      "circuit analysis",
      "attention heads",
      "structured reasoning",
      "Qwen families",
      "DeepSeek-distilled model",
      "group relative policy optimization",
      "think on/off models",
      "ablation analysis",
      "qualitative analysis",
      "over-thinking failure modes"
    ]
  },
  {
    "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
    "summary": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
    "authors": [
      "Junyu Chen",
      "Wenkun He",
      "Yuchao Gu",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Junsong Chen",
      "Dongyun Zou",
      "Yujun Lin",
      "Zhekai Zhang",
      "Muyang Li",
      "Haocheng Xi",
      "Ligeng Zhu",
      "Enze Xie",
      "Song Han",
      "Han Cai"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "DC-VideoGen",
      "video diffusion model",
      "deep compression latent space",
      "lightweight fine-tuning",
      "Deep Compression Video Autoencoder",
      "chunk-causal temporal design",
      "AE-Adapt-V",
      "Wan-2.1-14B model",
      "inference latency",
      "high-resolution video generation"
    ]
  },
  {
    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
    "authors": [
      "Siwei Wang",
      "Yifei Shen",
      "Haoran Sun",
      "Shi Feng",
      "Shang-Hua Teng",
      "Li Dong",
      "Yaru Hao",
      "Wei Chen"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "reinforcement learning",
      "Large Language Models",
      "policy gradient",
      "Q-learning",
      "supervised fine-tuning",
      "co-occurrence-based spurious solutions",
      "diversity collapse",
      "off-policy learning",
      "reward hacking",
      "Blocksworld"
    ]
  },
  {
    "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
    "summary": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.",
    "authors": [
      "Junlin Han",
      "Shengbang Tong",
      "David Fan",
      "Yufan Ren",
      "Koustuv Sinha",
      "Philip Torr",
      "Filippos Kokkinos"
    ],
    "date": "2025-09-30",
    "ai_keywords": [
      "Large Language Models",
      "LLMs",
      "visual priors",
      "latent visual capabilities",
      "multimodal data",
      "visual tasks",
      "implicit knowledge",
      "visual world",
      "perception priors",
      "reasoning priors",
      "pre-training",
      "reasoning-centric data",
      "transferable",
      "visual reasoning",
      "perception ability",
      "vision encoder",
      "visual instruction tuning",
      "text describing the visual world",
      "vision-aware LLMs",
      "data-centric recipe",
      "pre-training",
      "visual alignment",
      "supervised multimodal fine-tuning",
      "Multi-Level Existence Bench",
      "MLE-Bench"
    ]
  },
  {
    "title": "DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed\n  Latent Space",
    "summary": "Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.",
    "authors": [
      "Wenkun He",
      "Yuchao Gu",
      "Junyu Chen",
      "Dongyun Zou",
      "Yujun Lin",
      "Zhekai Zhang",
      "Haocheng Xi",
      "Muyang Li",
      "Ligeng Zhu",
      "Jincheng Yu",
      "Junsong Chen",
      "Enze Xie",
      "Song Han",
      "Han Cai"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "diffusion models",
      "latent space",
      "DC-Gen",
      "embedding alignment",
      "LoRA fine-tuning",
      "SANA",
      "FLUX.1-Krea",
      "NVFP4 SVDQuant"
    ]
  },
  {
    "title": "Where LLM Agents Fail and How They can Learn From Failures",
    "summary": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug",
    "authors": [
      "Kunlun Zhu",
      "Zijia Liu",
      "Bingxuan Li",
      "Muxin Tian",
      "Yingxuan Yang",
      "Jiaxun Zhang",
      "Pengrui Han",
      "Qipeng Xie",
      "Fuyang Cui",
      "Weijia Zhang",
      "Xiaoteng Ma",
      "Xiaodong Yu",
      "Gowtham Ramesh",
      "Jialian Wu",
      "Zicheng Liu",
      "Pan Lu",
      "James Zou",
      "Jiaxuan You"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "AgentErrorTaxonomy",
      "AgentErrorBench",
      "AgentDebug",
      "failure modes",
      "memory",
      "reflection",
      "planning",
      "action",
      "system-level operations",
      "ALFWorld",
      "GAIA",
      "WebShop",
      "all-correct accuracy",
      "step accuracy",
      "task success"
    ]
  },
  {
    "title": "Scalable GANs with Transformers",
    "summary": "Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
    "authors": [
      "Sangeek Hyun",
      "MinKyu Lee",
      "Jae-Pil Heo"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Generative Adversarial Networks",
      "GANs",
      "Variational Autoencoder",
      "latent space",
      "transformers",
      "perceptual fidelity",
      "intermediate supervision",
      "width-aware learning-rate adjustment",
      "GAT",
      "FID",
      "ImageNet-256"
    ]
  },
  {
    "title": "DepthLM: Metric Depth From Vision Language Models",
    "summary": "Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, a key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables a single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below.",
    "authors": [
      "Zhipeng Cai",
      "Ching-Feng Yeh",
      "Hu Xu",
      "Zhuang Liu",
      "Gregory Meyer",
      "Xinjie Lei",
      "Changsheng Zhao",
      "Shang-Wen Li",
      "Vikas Chandra",
      "Yangyang Shi"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "vision language models",
      "VLMs",
      "GPT-5",
      "metric depth estimation",
      "text-based supervised-finetuning",
      "sparse labels",
      "visual prompting",
      "intrinsic-conditioned augmentation",
      "DepthLM",
      "over-smoothing",
      "flying points"
    ]
  },
  {
    "title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete\n  Diffusion",
    "summary": "Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre-trained diffusion model is optimized to generate paths that lead to a reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), a novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune a pre-trained discrete diffusion model under a stochastic optimal control objective. We validate our framework on single- and multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation.",
    "authors": [
      "Sophia Tang",
      "Yuchen Zhu",
      "Molei Tao",
      "Pranam Chatterjee"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "reinforcement learning",
      "stochastic optimal control",
      "diffusion fine-tuning",
      "reward-tilted distribution",
      "rollouts",
      "sub-optimal trajectories",
      "TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion",
      "TR2-D2",
      "discrete diffusion trajectories",
      "Monte Carlo Tree Search",
      "MCTS",
      "trajectory-aware fine-tuning",
      "biological sequence diffusion models",
      "single-objective fine-tuning",
      "multi-objective fine-tuning"
    ]
  },
  {
    "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See\n  and Critique like Photographers",
    "summary": "While editing directly from life, photographers have found it too difficult to see simultaneously both the blue and the sky. Photographer and curator, Szarkowski insightfully revealed one of the notable gaps between general and aesthetic visual understanding: while the former focuses on identifying the factual element in an image (sky), the latter transcends such object identification, viewing it instead as an aesthetic component--a pure color block (blue). Such fundamental distinctions between general (detection, localization, etc.) and aesthetic (color, lighting, composition, etc.) visual understanding present a significant challenge for Multimodal Large Language Models (MLLMs). Although some recent works have made initial explorations, they are often limited to general and basic aesthetic commonsense. As a result, they frequently fall short in real-world scenarios (Fig. 1), which require extensive expertise--including photographic techniques, photo pre/post-processing knowledge, and more, to provide a detailed analysis and description. To fundamentally enhance the aesthetics understanding of MLLMs, we first introduce a novel dataset, PhotoCritique, derived from extensive discussions among professional photographers and enthusiasts, and characterized by the large scale, expertise, and diversity. Then, to better learn visual aesthetics from PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a languageguided multi-view vision fusion mechanism to understand image aesthetics from multiple perspectives. Finally, we present a novel benchmark, PhotoBench, a comprehensive and professional benchmark for aesthetic visual understanding. On existing benchmarks and PhotoBench, our model demonstrates clear advantages over existing models.",
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Jing Shi",
      "Simon Jenni",
      "Yifei Fan",
      "Franck Dernoncourt",
      "Scott Cohen",
      "Sheng Li"
    ],
    "date": "2025-09-23",
    "ai_keywords": [
      "Multimodal Large Language Models",
      "PhotoCritique",
      "PhotoEye",
      "language-guided",
      "multi-view vision fusion",
      "PhotoBench",
      "aesthetic visual understanding"
    ]
  },
  {
    "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
    "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable off-the-shelf video representation by predicting masked regions in latent space with an exponential moving average (EMA)-updated teacher. While EMA prevents representation collapse, it complicates scalable model selection and couples teacher and student architectures. We revisit masked-latent prediction and show that a frozen teacher suffices. Concretely, we (i) train a target encoder with a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze it and train a student to predict the teacher's latents on masked regions. This leads to a two-stage, unregularized scheme that we refer to as SALT (Static-teacher Asymmetric Latent Training). SALT decouples optimization into pixel reconstruction (teacher) and masked latent prediction (student), increasing transparency, efficiency, and scalability while preserving the ability of representation to generalize under frozen evaluation. Empirically, our student models outperform recently proposed V-JEPA 2 encoders under frozen backbone evaluation across diverse benchmarks. They are also more compute-optimal: at matched pretraining FLOPs, our method achieves higher probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs Pareto frontier. Finally, we find that student quality is remarkably robust to teacher quality: high-performing students emerge even with small, sub-optimal teachers. This points to a compute budget allocation that should overwhelmingly favor the student. These results position SALT as a simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning.",
    "authors": [
      "Xianhang Li",
      "Chen Huang",
      "Chun-Liang Li",
      "Eran Malach",
      "Josh Susskind",
      "Vimal Thilak",
      "Etai Littwin"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Video Joint Embedding Predictive Architectures",
      "V-JEPA",
      "masked regions",
      "latent space",
      "exponential moving average",
      "EMA",
      "representation collapse",
      "pixel-reconstruction objective",
      "SALT",
      "Static-teacher Asymmetric Latent Training",
      "probing accuracy",
      "Pareto frontier"
    ]
  },
  {
    "title": "ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in\n  Biographical Reasoning",
    "summary": "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating and improving multimodal large language models (MLLMs) in biographical reasoning. To the best of our knowledge, this is the first work to systematically examine LLM capabilities in biography, a critical yet underexplored dimension of factual knowledge. At its core, AdamDB is a multilingual and multimodal dataset covering over 4 million individuals across geography, time, and profession, while AdamBench provides cognitively structured evaluations based on Bloom's taxonomy, spanning six reasoning levels in both English and native languages. To address hallucinations, particularly for lesser-known individuals, we propose AdamRAG, a retrieval-augmented generation system tailored to biographical contexts. Experiments show that AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with the largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller, less consistent improvements than retrieval. ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing the development of multilingual, accurate, and hallucination-resistant MLLMs.",
    "authors": [
      "Jasin Cekinmez",
      "Omid Ghahroodi",
      "Saad Fowad Chandle",
      "Dhiman Gupta",
      "Ehsaneddin Asgari"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "multimodal large language models",
      "biographical reasoning",
      "AdamDB",
      "AdamBench",
      "Bloom's taxonomy",
      "AdamRAG",
      "retrieval-augmented generation",
      "hallucinations",
      "multimodal input",
      "face images"
    ]
  },
  {
    "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
    "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing Large Language Models (LLMs), where pre-training and RL post-training share the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectives--score/flow matching loss. In this work, we establish a novel theoretical analysis: DDPO is an implicit form of score/flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce Advantage Weighted Matching (AWM), a policy-gradient method for diffusion. It uses the same score/flow-matching loss as pretraining to obtain a lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, reduces variance, and yields faster convergence. This simple yet effective design yields substantial benefits: on GenEval, OCR, and PickScore benchmarks, AWM delivers up to a 24times speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality. Code is available at https://github.com/scxue/advantage_weighted_matching.",
    "authors": [
      "Shuchen Xue",
      "Chongjian Ge",
      "Shilong Zhang",
      "Yichen Li",
      "Zhi-Ming Ma"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Reinforcement Learning",
      "Large Language Models",
      "Denoising Diffusion Policy Optimization",
      "DDPO",
      "score/flow matching",
      "noisy targets",
      "policy-gradient method",
      "Advantage Weighted Matching",
      "AWM",
      "GenEval",
      "OCR",
      "PickScore",
      "Stable Diffusion",
      "FLUX"
    ]
  },
  {
    "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
    "summary": "Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals a universal phenomenon: each model exhibits a Visual Integration Point (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model-dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs.",
    "authors": [
      "Lin Long",
      "Changdae Oh",
      "Seongheon Park",
      "Yixuan Li"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Large vision-language models",
      "language prior",
      "chain-of-embedding",
      "layer-wise representation dynamics",
      "Visual Integration Point",
      "Total Visual Integration estimator"
    ]
  },
  {
    "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for\n  RGB-to-Thermal Image Translation",
    "summary": "Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day, Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: http://xjh19971.github.io/ThermalGen",
    "authors": [
      "Jiuhong Xiao",
      "Roshan Nayak",
      "Ning Zhang",
      "Daniel Tortei",
      "Giuseppe Loianno"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "flow-based generative model",
      "RGB image conditioning",
      "style-disentangled mechanism",
      "RGB-T image translation",
      "GAN-based methods",
      "diffusion-based methods"
    ]
  },
  {
    "title": "TENET: Leveraging Tests Beyond Validation for Code Generation",
    "summary": "Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting.",
    "authors": [
      "Yiran Hu",
      "Nan Jiang",
      "Shanchao Liang",
      "Yi Wu",
      "Lin Tan"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Test-Driven Development (TDD)",
      "large language models (LLMs)",
      "test cases",
      "executable specifications",
      "test suite",
      "context retrieval",
      "code refinement",
      "test harness",
      "reflection-based refinement",
      "RepoCod",
      "RepoEval"
    ]
  },
  {
    "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
    "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Despite its similarities to compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, bridging the gap between human and machine intelligence.",
    "authors": [
      "Samuel Schapiro",
      "Sumuk Shashidhar",
      "Alexi Gladstone",
      "Jonah Black",
      "Royce Moon",
      "Dilek Hakkani-Tur",
      "Lav R. Varshney"
    ],
    "date": "2025-09-25",
    "ai_keywords": [
      "Large Language Models",
      "compositional generalization",
      "combinatorial creativity",
      "novelty",
      "utility",
      "model depths",
      "model widths",
      "ideation-execution gap",
      "creativity algorithms"
    ]
  },
  {
    "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
    "summary": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.",
    "authors": [
      "Shaobo Wang",
      "Zhengbo Jiao",
      "Zifan Zhang",
      "Yilang Peng",
      "Xu Ze",
      "Boyu Yang",
      "Wei Wang",
      "Hu Wei",
      "Linfeng Zhang"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "reasoning tasks",
      "data synthesis",
      "distillation",
      "curriculum generation",
      "Teacher",
      "Solver",
      "Generator",
      "preference feedback",
      "trajectories",
      "question-design strategy",
      "self-improving curriculum",
      "mathematical reasoning benchmarks",
      "Socratic-Solver-8B",
      "Socratic-Generator-32B",
      "Qwen3",
      "GLM4",
      "Qwen3-235B-A22B",
      "DeepSeek-V3.1-671B",
      "GPT-5",
      "Gemini-2.5-Pro",
      "Grok-4",
      "Claude-4.1-Opus"
    ]
  },
  {
    "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns",
    "summary": "Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a \"Correctness Model\" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.",
    "authors": [
      "Hanqi Xiao",
      "Vaidehi Patil",
      "Hyunji Lee",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "LLMs",
      "confidence estimates",
      "self-knowledge",
      "Correctness Model (CM)",
      "Generalized Correctness Model (GCM)",
      "historical predictions",
      "answer phrasing",
      "in-context examples",
      "post-hoc calibration",
      "Qwen3-8B",
      "MMLU",
      "TriviaQA",
      "selective prediction task"
    ]
  },
  {
    "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement\n  Learning",
    "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.",
    "authors": [
      "Xin Qiu",
      "Yulu Gan",
      "Conor F. Hayes",
      "Qiyao Liang",
      "Elliot Meyerson",
      "Babak Hodjat",
      "Risto Miikkulainen"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "reinforcement learning",
      "evolution strategies",
      "large language models",
      "fine-tuning",
      "sample efficiency",
      "long-horizon rewards",
      "reward hacking",
      "stable performance"
    ]
  },
  {
    "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient\n  Chain-of-Thought Training",
    "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.",
    "authors": [
      "Hongcheng Wang",
      "Yinuo Huang",
      "Sukai Wang",
      "Guanghui Ren",
      "Hao Dong"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "GRPO",
      "Reinforcement Learning",
      "Chain-of-Thought",
      "Large Language Models",
      "Vision-Language Models",
      "gradient coupling",
      "sparse reward signals",
      "unstable advantage estimation",
      "GRPO-MA",
      "multi-answer generation",
      "thought advantage",
      "gradient spikes"
    ]
  },
  {
    "title": "The Era of Real-World Human Interaction: RL from User Conversations",
    "summary": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
    "authors": [
      "Chuanyang Jin",
      "Jing Xu",
      "Bo Liu",
      "Leitian Tao",
      "Olga Golovneva",
      "Tianmin Shu",
      "Wenting Zhao",
      "Xian Li",
      "Jason Weston"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Reinforcement Learning from Human Interaction (RLHI)",
      "User-Guided Rewrites",
      "User-Based Rewards",
      "persona-conditioned preference optimization",
      "WildChat"
    ]
  },
  {
    "title": "LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge\n  Signals",
    "summary": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality.",
    "authors": [
      "Min-Hsuan Yeh",
      "Yixuan Li",
      "Tanwi Mallick"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Retrieval-Augmented Generation",
      "RAG",
      "large language models",
      "LLMs",
      "hallucinations",
      "distributional distance",
      "transformer layers",
      "AUROC",
      "AUPRC",
      "HalluRAG"
    ]
  },
  {
    "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
    "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench.",
    "authors": [
      "Min-Hsuan Yeh",
      "Yixuan Li"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "large language models",
      "LLMs",
      "human feedback",
      "reward models",
      "alignment",
      "automated data cleaning",
      "benchmark",
      "alignment performance",
      "generalizability",
      "datasets",
      "model architectures",
      "optimization algorithms",
      "data preprocessing",
      "responsible AI development"
    ]
  },
  {
    "title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient\n  SpeechLLMs",
    "summary": "Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks.",
    "authors": [
      "Yuhan Song",
      "Linhao Zhang",
      "Chuhan Wu",
      "Aiwei Liu",
      "Wei Jia",
      "Houfeng Wang",
      "Xiao Zhou"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "semantic speech tokenizers",
      "acoustic perturbations",
      "Signal-to-Noise Ratios",
      "SNRs",
      "learning burden",
      "downstream LLMs",
      "brittle single-path quantization",
      "training signal",
      "multi-branch architecture",
      "bit-wise voting mechanism",
      "token stability",
      "Unit Edit Distance",
      "UED",
      "SpeechLLMs"
    ]
  },
  {
    "title": "Rethinking Large Language Model Distillation: A Constrained Markov\n  Decision Process Perspective",
    "summary": "We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.",
    "authors": [
      "Matthieu Zimmer",
      "Xiaotong Ji",
      "Tu Nguyen",
      "Haitham Bou Ammar"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "large language model (LLM) distillation",
      "constrained reinforcement learning",
      "task-specific rewards",
      "reward weighting",
      "constrained state augmented reinforcement learning",
      "reward function",
      "constraint satisfaction",
      "soft Lagrangian relaxation",
      "mathematical reasoning tasks",
      "resource-constrained settings"
    ]
  },
  {
    "title": "UniVid: The Open-Source Unified Video Model",
    "summary": "Unified video modeling that combines generation and understanding capabilities is increasingly important but faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the limitations of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, a unified architecture that couples an MLLM with a diffusion decoder through a lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
    "authors": [
      "Jiabin Luo",
      "Junhui Lin",
      "Zeyu Zhang",
      "Biao Wu",
      "Meng Fang",
      "Ling Chen",
      "Hao Tang"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "flow-based generation",
      "text-visual token imbalance",
      "uniform cross-modal attention",
      "diffusion decoder",
      "Temperature Modality Alignment",
      "Pyramid Reflection",
      "VBench-Long",
      "MSVD-QA",
      "ActivityNet-QA"
    ]
  },
  {
    "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus",
    "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language Processing (NLP) have evolved rapidly, especially with the advent of Transformer-based Large Language Models (LLMs). This shift has transformed research goals and priorities, from Lexical and Semantic Resources to Language Modelling and Multimodality. In this study, we track the research trends of the Italian CL and NLP community through an analysis of the contributions to CLiC-it, arguably the leading Italian conference in the field. We compile the proceedings from the first 10 editions of the CLiC-it conference (from 2014 to 2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its metadata, including author provenance, gender, affiliations, and more, as well as the content of the papers themselves, which address various topics. Our goal is to provide the Italian and international research communities with valuable insights into emerging trends and key developments over time, supporting informed decisions and future directions in the field.",
    "authors": [
      "Chiara Alzetta",
      "Serena Auriemma",
      "Alessandro Bondielli",
      "Luca Dini",
      "Chiara Fazzone",
      "Alessio Miaschi",
      "Martina Miliani",
      "Marta Sartor"
    ],
    "date": "2025-09-23",
    "ai_keywords": [
      "Transformer-based Large Language Models",
      "Language Modelling",
      "Multimodality"
    ]
  },
  {
    "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's\n  Contribution to Model Performance",
    "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
    "authors": [
      "Nicolas Boizard",
      "Hippolyte Gisserot-Boukhlef",
      "Kevin El-Haddad",
      "C\u00e9line Hudelot",
      "Pierre Colombo"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Instruction Fine-Tuning",
      "reasoning models",
      "synthetic data distillation",
      "multiple-choice",
      "open-ended formats",
      "reasoning-intensive tasks"
    ]
  },
  {
    "title": "MMPB: It's Time for Multi-Modal Personalization",
    "summary": "Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
    "authors": [
      "Jaeik Kim",
      "Woojin Kim",
      "Woohyeon Park",
      "Jaeyoung Do"
    ],
    "date": "2025-09-26",
    "ai_keywords": [
      "Vision-Language Models",
      "VLMs",
      "personalization",
      "benchmark",
      "image-query pairs",
      "preference-grounded queries",
      "concept injection",
      "multi-turn dialogue",
      "personalized querying",
      "refusal behaviors",
      "long-context forgetting"
    ]
  },
  {
    "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless\n  Communications with Reinforcement Learning",
    "summary": "Large language models (LLMs) excel at general mathematical reasoning but fail catastrophically on specialized technical mathematics. In wireless communications, where problems require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations, even state-of-the-art models struggle to achieve competent performance. We present WirelessMathLM, demonstrating that compact models (0.5B-7B parameters) can match or exceed much larger models through domain-specific reinforcement learning with verifiable rewards. Our key insight is that wireless mathematics problems possess a unique property--verifiable correctness--that enables effective reinforcement learning without human feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027 problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with binary verification rewards, we train models directly from base checkpoints without supervised warm-start. Our 7B model achieves 39.5% accuracy on WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B +81%), with positive transfer to general mathematics benchmarks--our models gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without any training on these tasks.",
    "authors": [
      "Xin Li",
      "Mengbing Liu",
      "Yiyang Zhu",
      "Wenhe Zhang",
      "Li Wei",
      "Jiancheng An",
      "Chau Yuen"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Large language models",
      "wireless communications",
      "information-theoretic bounds",
      "optimization constraints",
      "signal processing",
      "domain-specific reinforcement learning",
      "verifiable rewards",
      "WirelessMathLM",
      "WirelessMathBench-XL",
      "Group Relative Policy Optimization",
      "GRPO",
      "binary verification rewards",
      "DeepSeek-R1",
      "MATH",
      "Minerva-Math",
      "OlympiadBench",
      "AMC",
      "AIME"
    ]
  },
  {
    "title": "Local Success Does Not Compose: Benchmarking Large Language Models for\n  Compositional Formal Verification",
    "summary": "We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification generation in Dafny. Unlike prior benchmarks that focus on single-function tasks, DafnyCOMP targets programs composed of multiple interacting functions with data dependencies, requiring reasoning across component boundaries. The benchmark consists of 300 automatically synthesized multi-function programs. We evaluate several state-of-the-art LLM families and find that, while they perform well on single-function verification, their performance drops sharply on compositional tasks. Analysis reveals systematic failures in cross-functional reasoning, including fragile specifications, misalignment between implementations and proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for measuring progress toward reliable, verifiable, and compositional code generation with LLMs.",
    "authors": [
      "Xu Xu",
      "Xin Li",
      "Xingwei Qu",
      "Jie Fu",
      "Binhang Yuan"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "large language models",
      "compositional specification generation",
      "Dafny",
      "multi-function programs",
      "cross-functional reasoning",
      "fragile specifications",
      "misalignment",
      "unstable reasoning"
    ]
  },
  {
    "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large\n  Language Models",
    "summary": "Wikipedia is the largest open knowledge corpus, widely used worldwide and serving as a key resource for training large language models (LLMs) and retrieval-augmented generation (RAG) systems. Ensuring its accuracy is therefore critical. But how accurate is Wikipedia, and how can we improve it?   We focus on inconsistencies, a specific type of factual inaccuracy, and introduce the task of corpus-level inconsistency detection. We present CLAIRE, an agentic system that combines LLM reasoning with retrieval to surface potentially inconsistent claims along with contextual evidence for human review. In a user study with experienced Wikipedia editors, 87.5% reported higher confidence when using CLAIRE, and participants identified 64.7% more inconsistencies in the same amount of time.   Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first benchmark of real Wikipedia inconsistencies. Using random sampling with CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset reveals substantial headroom: the best fully automated system achieves an AUROC of only 75.1%.   Our results show that contradictions are a measurable component of Wikipedia and that LLM-based systems like CLAIRE can provide a practical tool to help editors improve knowledge consistency at scale.",
    "authors": [
      "Sina J. Semnani",
      "Jirayu Burapacheep",
      "Arpandeep Khatua",
      "Thanawan Atchariyachanvanit",
      "Zheng Wang",
      "Monica S. Lam"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "large language models",
      "retrieval-augmented generation",
      "corpus-level inconsistency detection",
      "CLAIRE",
      "WIKICOLLIDE",
      "AUROC",
      "FEVEROUS",
      "AmbigQA"
    ]
  },
  {
    "title": "BPMN Assistant: An LLM-Based Approach to Business Process Modeling",
    "summary": "This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural language-based creation and editing of BPMN diagrams. A specialized JSON-based representation is introduced as a structured alternative to the direct handling of XML to enhance the accuracy of process modifications. Process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is evaluated with a binary success metric. Results show that JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates. We discuss key trade-offs, limitations, and future improvements. The implementation is available at https://github.com/jtlicardo/bpmn-assistant.",
    "authors": [
      "Josip Tomo Licardo",
      "Nikola Tankovic",
      "Darko Etinger"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "Large Language Models",
      "BPMN diagrams",
      "JSON-based representation",
      "XML",
      "Graph Edit Distance",
      "Relative Graph Edit Distance"
    ]
  },
  {
    "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
    "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.",
    "authors": [
      "Jingyi Yang",
      "Guanxu Chen",
      "Xuhao Hu",
      "Jing Shao"
    ],
    "date": "2025-09-28",
    "ai_keywords": [
      "Masked diffusion language models",
      "autoregressive models",
      "parallel decoding",
      "flexible generation orders",
      "block-wise decoding",
      "semi-AR decoding",
      "full diffusion-style decoding",
      "reinforcement learning",
      "non-causal decoding",
      "rollout trajectory",
      "optimization trajectory",
      "EOS Early Rejection",
      "Ascending Step-Size",
      "Consistency Trajectory Group Relative Policy Optimization",
      "LLaDA-8B-Instruct"
    ]
  },
  {
    "title": "SCI-Verifier: Scientific Verifier with Thinking",
    "summary": "As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.",
    "authors": [
      "Shenghe Zheng",
      "Chenyu Huang",
      "Fangchen Yu",
      "Junchi Yao",
      "Jingqi Ye",
      "Tao Chen",
      "Yun Luo",
      "Ning Ding",
      "LEI BAI",
      "Ganqu Cui",
      "Peng Ye"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "large language models",
      "scientific reasoning",
      "answer verification",
      "SCI-VerifyBench",
      "SCI-Verifier",
      "domain-specific equivalence transformations",
      "logical reasoning",
      "equivalence judgment"
    ]
  },
  {
    "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation",
    "summary": "Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.",
    "authors": [
      "Wei Zhou",
      "Guoliang Li",
      "Haoyu Wang",
      "Yuxing Han",
      "Xufei Wu",
      "Fan Wu",
      "Xuanhe Zhou"
    ],
    "date": "2025-09-27",
    "ai_keywords": [
      "Large language models",
      "Text-to-SQL",
      "Cross-System SQL Translation",
      "SQL-to-SQL",
      "database systems",
      "SQL dialects",
      "PARROT",
      "benchmark",
      "system-specific SQL",
      "public leaderboard"
    ]
  },
  {
    "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
    "summary": "The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\\% vs.\\ 33.5\\%).",
    "authors": [
      "Andr\u00e9s Fern\u00e1ndez Garc\u00eda",
      "Javier de la Rosa",
      "Julio Gonzalo",
      "Roser Morante",
      "Enrique Amig\u00f3",
      "Alejandro Benito-Santos",
      "Jorge Carrillo-de-Albornoz",
      "V\u00edctor Fresno",
      "Adrian Ghajari",
      "Guillermo Marco",
      "Laura Plaza",
      "Eva S\u00e1nchez Salido"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "BOE-XSUM",
      "Bolet\u00edn Oficial del Estado",
      "large language models",
      "LLMs",
      "fine-tuned models",
      "zero-shot setting",
      "BERTIN GPT-J 6B",
      "DeepSeek-R1"
    ]
  },
  {
    "title": "Scaling Generalist Data-Analytic Agents",
    "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.",
    "authors": [
      "Shuofei Qiao",
      "Yanqiu Zhao",
      "Zhisong Qiu",
      "Xiaobin Wang",
      "Jintian Zhang",
      "Zhao Bin",
      "Ningyu Zhang",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ],
    "date": "2025-09-29",
    "ai_keywords": [
      "fine-grained task taxonomy",
      "recursive easy-to-hard task composition",
      "knowledge-augmented trajectory sampling",
      "model-based filtering",
      "rule-based filtering",
      "dynamically adjustable training objective",
      "SFT",
      "RL losses",
      "memory-frugal",
      "code-based multi-turn rollout",
      "DataMind-12K",
      "DataMind-14B",
      "DataMind-7B"
    ]
  }
]